{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb036424",
   "metadata": {},
   "source": [
    "<h1>Tutorial: Hierarchical Refinement for Large-Scale Optimal Transport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17899ca5",
   "metadata": {},
   "source": [
    "This tutorial presents the implementation of the Hierarchical Refinement (HiRef) algorithm from the paper \"Hierarchical Refinement: Optimal Transport to Infinity and Beyond\" (Halmos et al., 2025). This algorithm solves the optimal transport (OT) problem for two large sets of points with linear memory complexity. The key idea is to exploit the fact that low-rank optimal transport solutions co-cluster each point with its image under the (bijective) Monge map. HiRef therefore recursively constructs a multi-scale partition of the data by solving a hierarchy of low-rank OT sub-problems, ultimately leading to a complete bijective coupling. Below we detail the implementation steps, highlighting methodological choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f99a4",
   "metadata": {},
   "source": [
    "<h2> Setup and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "945f7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union, Dict, Any\n",
    "import random\n",
    "import operator\n",
    "import functools\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ott.geometry import costs, pointcloud\n",
    "from ott.tools import sinkhorn_divergence, progot\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from ott.geometry import geometry\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cae2dc",
   "metadata": {},
   "source": [
    "# 1. Load images & model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c95bb47",
   "metadata": {},
   "source": [
    "The tutorial begins by loading a set of images (here a subset of 5000 images from COCO/ImageNet). Each image is transformed (resized to 224×224 and converted to a tensor) then passed through a pre-trained neural network (ResNet-50). The final classification layer (model.fc = Identity) is removed to obtain only the feature vectors (embeddings) of dimension 2048."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173d8ca",
   "metadata": {},
   "source": [
    "To save time during future executions, we save the embeddings with pickle. In the end, we obtain an embeddings tensor of shape (5000, 2048) containing the vector representations of our images. So you can also directly go to part 3 if you want to start directly from the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d50b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Liste tous les fichiers .jpg\n",
    "        self.image_paths = [os.path.join(root_dir, fname)\n",
    "                            for fname in os.listdir(root_dir)\n",
    "                            if fname.endswith('.jpg')]\n",
    "        # Si le nombre d'images est impair, enlever la dernière\n",
    "        if len(self.image_paths) % 2 != 0:\n",
    "            self.image_paths = self.image_paths[:-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Toujours convertir en RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # Pas d'étiquette ici, juste l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d362e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 images from ImageNet!\n"
     ]
    }
   ],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images for CNN input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load COCO dataset from extracted path\n",
    "imagenet_dataset = CustomImageDataset(root_dir='images', transform=transform)\n",
    "\n",
    "\n",
    "# Create DataLoader for batching\n",
    "imagenet_loader = DataLoader(imagenet_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Loaded {len(imagenet_dataset)} images from ImageNet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280953f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.expanduser(\"resnet50-0676ba61.pth\")\n",
    "\n",
    "# Load pretrained ResNet model\n",
    "model = models.resnet50()\n",
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "model.fc = torch.nn.Identity()  # Remove classification layer to extract features\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f1229",
   "metadata": {},
   "source": [
    "# 2. Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6001bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 157/157 [04:00<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_features(dataloader, model):\n",
    "    \"\"\"\n",
    "    Compute embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for idx, images in tqdm(enumerate(dataloader), desc=\"Extracting features\", total=len(dataloader)):\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            embeddings.append(jnp.array(features.detach().cpu().numpy()))  \n",
    "    return jnp.vstack(embeddings)  # Stack all embeddings\n",
    "\n",
    "print('extracting embeddings!')\n",
    "embeddings = extract_features(imagenet_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dbe4e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully to embeddings/embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "with open('embeddings/embeddings.pkl', \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(\"Embeddings saved successfully to embeddings/embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb07d8",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cde20a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully! Shape: (5000, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings from the pickle file\n",
    "with open('embeddings/embeddings.pkl', \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(f\"Embeddings loaded successfully! Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe5baf",
   "metadata": {},
   "source": [
    "In the paper, OT is formulated between two uniform distributions of the same size, where the problem is actually an assignment problem (Monge bijection). To reproduce this, we start with the embeddings extracted from 5000 images and do:  \n",
    "Random permutation: We create a random index vector of size 5000 (fixed with a seed for reproducibility).\n",
    "<b>Separation into two equal subsets:We take the first 2500 entries of this permutation to form set X and the next 2500 for Y. Thus, |X| = |Y| = 2500, with the same uniform measure a priori.\n",
    "These two sets X and Y of 2048-dim vectors will be the input points for the HiRef algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "155a4ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2500, 2048), Y shape: (2500, 2048)\n"
     ]
    }
   ],
   "source": [
    "num_samples = embeddings.shape[0]\n",
    "\n",
    "# Shuffle indices\n",
    "key = jax.random.PRNGKey(42)\n",
    "indices = jax.random.permutation(key, num_samples)\n",
    "\n",
    "# Split into two tensors\n",
    "X = embeddings[indices[:num_samples // 2]]\n",
    "Y = embeddings[indices[num_samples // 2:]]\n",
    "\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa4c1e",
   "metadata": {},
   "source": [
    "# 4. Sinkhorn with epsilon-schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d28c1",
   "metadata": {},
   "source": [
    "Entropic optimal transport with the Sinkhorn algorithm is the foundation on which we'll build. Here's an implementation of the log-stabilized Sinkhorn with the utility functions for optimal transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53e92a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ott_log_sinkhorn(grad,\n",
    "                     a,\n",
    "                     b,\n",
    "                     gamma_k,\n",
    "                     max_iter = 50,\n",
    "                     balanced = True,\n",
    "                     unbalanced = False,\n",
    "                     tau = None,\n",
    "                     tau2 = None):\n",
    "    \"\"\"\n",
    "    grad: cost matrix (n, m)\n",
    "    a: source histogram (n,)\n",
    "    b: target histogram (m,)\n",
    "    gamma_k: régularisation inverse (1/epsilon)\n",
    "    \"\"\"\n",
    "    epsilon = 1.0 / gamma_k\n",
    "\n",
    "    # Choix des tau pour marges\n",
    "    if balanced and not unbalanced:\n",
    "        tau_a, tau_b = 1.0, 1.0\n",
    "    elif not balanced and unbalanced:\n",
    "        tau_a = tau / (tau + epsilon)\n",
    "        tau_b = tau2 / (tau2 + epsilon) if tau2 is not None else tau_a\n",
    "    else:  # semi-relaxed\n",
    "        tau_a, tau_b = 1.0, tau / (tau + epsilon)\n",
    "\n",
    "    # Géométrie entropique sur la matrice de coût\n",
    "    geom = geometry.Geometry(cost_matrix=grad, epsilon=epsilon)\n",
    "\n",
    "    # Construction du problème linéaire\n",
    "    prob = linear_problem.LinearProblem(\n",
    "        geom,\n",
    "        a=a,\n",
    "        b=b,\n",
    "        tau_a=tau_a,\n",
    "        tau_b=tau_b\n",
    "    )\n",
    "\n",
    "    # Solveur Sinkhorn\n",
    "    solver = sinkhorn.Sinkhorn(max_iterations=max_iter)\n",
    "    out = solver(prob)\n",
    "\n",
    "    return out.matrix\n",
    "\n",
    "def utils__Delta(vark, varkm1, gamma_k):\n",
    "    return (gamma_k**-2) * (jnp.linalg.norm(vark[0] - varkm1[0]) + jnp.linalg.norm(vark[1] - varkm1[1]) + jnp.linalg.norm(vark[2] - varkm1[2]))\n",
    "\n",
    "def utils__random_simplex_sample(key, N, dtype = jnp.float64):\n",
    "    \"\"\"\n",
    "    Draws a random point from the (N-1)-simplex using normalized exponentiated Gaussian variates.\n",
    "\n",
    "    Args:\n",
    "        key: PRNGKey for random number generation.\n",
    "        N: Dimensionality of the simplex (vector length).\n",
    "        dtype: Desired floating-point type of the output.\n",
    "\n",
    "    Returns:\n",
    "        A 1D array of shape (N,) with non-negative entries summing to 1.\n",
    "    \"\"\"\n",
    "    # Sample N independent standard normals\n",
    "    z = jax.random.normal(key, shape=(N,), dtype=dtype)\n",
    "    # Exponentiate\n",
    "    e = jnp.exp(z)\n",
    "    # Normalize to sum to 1\n",
    "    return e / jnp.sum(e)\n",
    "\n",
    "def utils__initialize_couplings(a, b, gQ, gR, gamma, full_rank = True, key = jax.random.PRNGKey(0), dtype = float, rank2_random = False, max_iter = 50):\n",
    "    \"\"\"\n",
    "    Initialize coupling factors in JAX.\n",
    "    \"\"\"\n",
    "    N1 = a.shape[0]\n",
    "    N2 = b.shape[0]\n",
    "    r = gQ.shape[0]\n",
    "    r2 = gR.shape[0]\n",
    "\n",
    "    one_N1 = jnp.ones((N1,), dtype=dtype)\n",
    "    one_N2 = jnp.ones((N2,), dtype=dtype)\n",
    "\n",
    "    if full_rank:\n",
    "        # Full-rank initialization via log-Sinkhorn\n",
    "        key, subkey = jax.random.split(key)\n",
    "        C_random = jax.random.uniform(subkey, (N1, r), dtype=dtype)\n",
    "        Q = ott_log_sinkhorn(C_random, a, gQ, gamma,\n",
    "                                max_iter=max_iter,\n",
    "                                balanced=True)\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        C_random = jax.random.uniform(subkey, (N2, r2), dtype=dtype)\n",
    "        R = ott_log_sinkhorn(C_random, b, gR, gamma,\n",
    "                                max_iter=max_iter,\n",
    "                                balanced=True)\n",
    "\n",
    "        # Compute updated inner marginals\n",
    "        gR_new = R.T @ one_N2\n",
    "        gQ_new = Q.T @ one_N1\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        C_random = jax.random.uniform(subkey, (r, r2), dtype=dtype)\n",
    "        T = ott_log_sinkhorn(C_random, gQ_new, gR_new, gamma,\n",
    "                                max_iter=max_iter,\n",
    "                                balanced=True)\n",
    "\n",
    "        # Inner inverse coupling\n",
    "        if r == r2:\n",
    "            Lambda = jnp.linalg.inv(T)\n",
    "        else:\n",
    "            Lambda = jnp.diag(1.0 / gQ_new) @ T @ jnp.diag(1.0 / gR_new)\n",
    "\n",
    "    else:\n",
    "        # Rank-2 initialization (Scetbon et al. 2021)\n",
    "        if r != r2:\n",
    "            raise ValueError(\"Rank-2 init requires equal inner ranks.\")\n",
    "        g = gQ\n",
    "        lambd = jnp.minimum(jnp.min(a), jnp.min(b))\n",
    "        lambd = jnp.minimum(lambd, jnp.min(g)) / 2.0\n",
    "\n",
    "        # Sample or deterministic\n",
    "        if rank2_random:\n",
    "            key, *splits = random.split(key, 4)\n",
    "            a1 = utils__random_simplex_sample(N1, splits[0], dtype)\n",
    "            b1 = utils__random_simplex_sample(N2, splits[1], dtype)\n",
    "            g1 = utils__random_simplex_sample(r, splits[2], dtype)\n",
    "        else:\n",
    "            g1 = jnp.arange(1, r + 1, dtype=dtype)\n",
    "            g1 = g1 / jnp.sum(g1)\n",
    "            a1 = jnp.arange(1, N1 + 1, dtype=dtype)\n",
    "            a1 = a1 / jnp.sum(a1)\n",
    "            b1 = jnp.arange(1, N2 + 1, dtype=dtype)\n",
    "            b1 = b1 / jnp.sum(b1)\n",
    "\n",
    "        a2 = (a - lambd * a1) / (1 - lambd)\n",
    "        b2 = (b - lambd * b1) / (1 - lambd)\n",
    "        g2 = (g - lambd * g1) / (1 - lambd)\n",
    "\n",
    "        Q = lambd * jnp.outer(a1, g1) + (1 - lambd) * jnp.outer(a2, g2)\n",
    "        R = lambd * jnp.outer(b1, g1) + (1 - lambd) * jnp.outer(b2, g2)\n",
    "\n",
    "        gR_new = R.T @ one_N2\n",
    "        gQ_new = Q.T @ one_N1\n",
    "\n",
    "        T = (1 - lambd) * jnp.diag(g) + lambd * jnp.outer(gR_new, gQ_new)\n",
    "        Lambda = jnp.linalg.inv(T)\n",
    "\n",
    "    return Q, R, T, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905adddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd__Wasserstein_Grad(C_or_Cfactors, Q, R, Lambda, full_grad=True, low_rank=False):\n",
    "    if low_rank:\n",
    "        C1, C2 = C_or_Cfactors\n",
    "        gradQ = C1 @ ((C2 @ R) @ Lambda.T)\n",
    "    else:\n",
    "        C = C_or_Cfactors\n",
    "        gradQ = (C @ R) @ Lambda.T\n",
    "\n",
    "    if full_grad:\n",
    "        N1 = Q.shape[0]\n",
    "        one_N1 = jnp.ones((N1,), dtype=Q.dtype)\n",
    "        gQ = Q.T @ one_N1\n",
    "        w1 = jnp.diag((gradQ.T @ Q) @ jnp.diag(1.0 / gQ))\n",
    "        gradQ = gradQ - jnp.outer(one_N1, w1)\n",
    "\n",
    "    if low_rank:\n",
    "        gradR = C2.T @ ((C1.T @ Q) @ Lambda)\n",
    "    else:\n",
    "        gradR = (C.T @ Q) @ Lambda\n",
    "\n",
    "    if full_grad:\n",
    "        N2 = R.shape[0]\n",
    "        one_N2 = jnp.ones((N2,), dtype=R.dtype)\n",
    "        gR = R.T @ one_N2\n",
    "        w2 = jnp.diag(jnp.diag(1.0 / gR) @ (R.T @ gradR))\n",
    "        gradR = gradR - jnp.outer(one_N2, w2)\n",
    "\n",
    "    return gradQ, gradR\n",
    "\n",
    "def gd__compute_grad_A(C, Q, R, Lambda, gamma,\n",
    "                       semiRelaxedLeft, semiRelaxedRight,\n",
    "                       Wasserstein=True, FGW=False,\n",
    "                       A=None, B=None,\n",
    "                       alpha=0.0,\n",
    "                       unbalanced=False,\n",
    "                       full_grad=True,\n",
    "                       low_rank=False,\n",
    "                       C_factors=None, A_factors=None, B_factors=None):\n",
    "    \"\"\"\n",
    "    JAX version of gradient computation for Wasserstein, GW and FGW.\n",
    "    If `low_rank` is True, it uses low-rank factorized cost matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    r = Lambda.shape[0]\n",
    "    one_r = jnp.ones((r,))\n",
    "    One_rr = jnp.outer(one_r, one_r)\n",
    "\n",
    "    if low_rank:\n",
    "        # Assume: A_factors = (A1, A2), B_factors = (B1, B2), C_factors = (C1, C2)\n",
    "        A1, A2 = A_factors\n",
    "        B1, B2 = B_factors\n",
    "\n",
    "        gradQ = -4 * (A1 @ (A2 @ (Q @ Lambda @ ((R.T @ B1) @ (B2 @ R)) @ Lambda.T)))\n",
    "        gradR = -4 * (B1 @ (B2 @ (R @ (Lambda.T @ ((Q.T @ A1) @ (A2 @ Q)) @ Lambda))))\n",
    "\n",
    "        if full_grad:\n",
    "            N1, N2 = Q.shape[0], R.shape[0]\n",
    "            one_N1 = jnp.ones((N1,))\n",
    "            one_N2 = jnp.ones((N2,))\n",
    "            gQ = Q.T @ one_N1\n",
    "            gR = R.T @ one_N2\n",
    "\n",
    "            MR = Lambda.T @ ((Q.T @ A1) @ (A2 @ Q)) @ Lambda @ ((R.T @ B1) @ (B2 @ R)) @ jnp.diag(1. / gR)\n",
    "            MQ = Lambda @ ((R.T @ B1) @ (B2 @ R)) @ Lambda.T @ ((Q.T @ A1) @ (A2 @ Q)) @ jnp.diag(1. / gQ)\n",
    "            gradQ += 4 * jnp.outer(one_N1, jnp.diag(MQ))\n",
    "            gradR += 4 * jnp.outer(one_N2, jnp.diag(MR))\n",
    "\n",
    "        # Wasserstein gradients in low-rank form\n",
    "        gradQW, gradRW = gd__Wasserstein_Grad(C_factors, Q, R, Lambda, full_grad=full_grad, low_rank=True)\n",
    "\n",
    "        gradQ = (1 - alpha) * gradQW + (alpha / 2) * gradQ\n",
    "        gradR = (1 - alpha) * gradRW + (alpha / 2) * gradR\n",
    "\n",
    "    else:\n",
    "        if Wasserstein:\n",
    "            gradQ, gradR = gd__Wasserstein_Grad(C, Q, R, Lambda, full_grad=full_grad)\n",
    "        elif A is not None and B is not None:\n",
    "            if not semiRelaxedLeft and not semiRelaxedRight and not unbalanced:\n",
    "                gradQ = -4 * (A @ Q) @ Lambda @ (R.T @ B @ R) @ Lambda.T\n",
    "                gradR = -4 * (B @ R @ Lambda.T) @ (Q.T @ A @ Q) @ Lambda\n",
    "            elif semiRelaxedRight:\n",
    "                gradQ = -4 * (A @ Q) @ Lambda @ (R.T @ B @ R) @ Lambda.T\n",
    "                gradR = 2 * (B @ B) @ R @ One_rr - 4 * (B @ R @ Lambda.T) @ (Q.T @ A @ Q) @ Lambda\n",
    "            elif semiRelaxedLeft:\n",
    "                gradQ = 2 * (A @ A) @ Q @ One_rr - 4 * (A @ Q) @ Lambda @ (R.T @ B @ R) @ Lambda.T\n",
    "                gradR = -4 * (B @ R @ Lambda.T) @ (Q.T @ A @ Q) @ Lambda\n",
    "            elif unbalanced:\n",
    "                gradQ = 2 * (A @ A) @ Q @ One_rr - 4 * (A @ Q) @ Lambda @ (R.T @ B @ R) @ Lambda.T\n",
    "                gradR = 2 * (B @ B) @ R @ One_rr - 4 * (B @ R @ Lambda.T) @ (Q.T @ A @ Q) @ Lambda\n",
    "\n",
    "            if full_grad:\n",
    "                N1, N2 = Q.shape[0], R.shape[0]\n",
    "                one_N1 = jnp.ones((N1,))\n",
    "                one_N2 = jnp.ones((N2,))\n",
    "                gQ = Q.T @ one_N1\n",
    "                gR = R.T @ one_N2\n",
    "                F = Q @ Lambda @ R.T\n",
    "                MR = Lambda.T @ Q.T @ A @ F @ B @ R @ jnp.diag(1. / gR)\n",
    "                MQ = Lambda @ R.T @ B @ F.T @ A @ Q @ jnp.diag(1. / gQ)\n",
    "                gradQ += 4 * jnp.outer(one_N1, jnp.diag(MQ))\n",
    "                gradR += 4 * jnp.outer(one_N2, jnp.diag(MR))\n",
    "\n",
    "            if FGW:\n",
    "                gradQW, gradRW = gd__Wasserstein_Grad(C, Q, R, Lambda, full_grad=full_grad)\n",
    "                gradQ = (1 - alpha) * gradQW + alpha * gradQ\n",
    "                gradR = (1 - alpha) * gradRW + alpha * gradR\n",
    "        else:\n",
    "            raise ValueError(\"Provide either Wasserstein=True or distance matrices A and B for GW problem.\")\n",
    "\n",
    "    normalizer = jnp.max(jnp.array([jnp.max(jnp.abs(gradQ)), jnp.max(jnp.abs(gradR))]))\n",
    "    gamma_k = gamma / normalizer\n",
    "\n",
    "    return gradQ, gradR, gamma_k\n",
    "\n",
    "\n",
    "\n",
    "def gd__compute_grad_B(C=None, Q=None, R=None, Lambda=None, gQ=None, gR=None, gamma=1.0,\n",
    "                       Wasserstein=True, FGW=False, A=None, B=None, alpha=0.0,\n",
    "                       low_rank=False, C_factors=None, A_factors=None, B_factors=None):\n",
    "    '''\n",
    "    JAX version of the Wasserstein / GW / FGW gradient w.r.t. the transport plan T.\n",
    "    Supports both full and low-rank computation.\n",
    "    '''\n",
    "    if low_rank:\n",
    "        # Gradient using low-rank approximation\n",
    "        C1, C2 = C_factors\n",
    "        gradLambda = (1 - alpha) * ((Q.T @ C1) @ (C2 @ R))\n",
    "\n",
    "        if A_factors is not None and B_factors is not None:\n",
    "            A1, A2 = A_factors\n",
    "            B1, B2 = B_factors\n",
    "            grad_GW = -4 * ((Q.T @ A1) @ (A2 @ Q)) @ Lambda @ ((R.T @ B1) @ (B2 @ R))\n",
    "            gradLambda += (alpha / 2.0) * grad_GW\n",
    "    else:\n",
    "        if Wasserstein:\n",
    "            gradLambda = Q.T @ C @ R\n",
    "        else:\n",
    "            gradLambda = -4 * Q.T @ A @ Q @ Lambda @ R.T @ B @ R\n",
    "            if FGW:\n",
    "                gradLambda = (1 - alpha) * (Q.T @ C @ R) + alpha * gradLambda\n",
    "\n",
    "    # Final gradient\n",
    "    gradT = jnp.diag(1.0 / gQ) @ gradLambda @ jnp.diag(1.0 / gR)\n",
    "    gamma_T = gamma / jnp.max(jnp.abs(gradT))\n",
    "    return gradT, gamma_T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55dc51f",
   "metadata": {},
   "source": [
    "<h2> Implementation of the FRLC Solver (Low-Rank Optimal Transport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e68607",
   "metadata": {},
   "source": [
    "The paper relies on a low-rank optimal transport solver called FRLC (Factor Relaxation with Latent Coupling). Here's its implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FRLC_opt(C, A=None, B=None, C_factors=None, A_factors=None, B_factors=None,\n",
    "             a=None, b=None, tau_in=50, tau_out=50, gamma=90, r=10, r2=None,\n",
    "             max_iter=200, Wasserstein=True, returnFull=False, FGW=False, alpha=0.0,\n",
    "             initialization='Full', init_args=None, full_grad=True,\n",
    "             convergence_criterion=True, tol=1e-5, min_iter=25,\n",
    "             max_inneriters_balanced=300, max_inneriters_relaxed=50,\n",
    "             diagonalize_return=False, low_rank=False):\n",
    "    \"\"\"\n",
    "    FRLC Optimal Transport solver. Supports dense and low-rank cost formulations.\n",
    "    \"\"\"\n",
    "    if r2 is None:\n",
    "        r2 = r\n",
    "\n",
    "    n, m = (C.shape[0], C.shape[1]) if C is not None else (C_factors[0].shape[0], C_factors[1].shape[0])\n",
    "\n",
    "    if a is None:\n",
    "        a = jnp.ones(n) / n\n",
    "    if b is None:\n",
    "        b = jnp.ones(m) / m\n",
    "\n",
    "    # Initialize coupling decomposition Q, R, Lambda\n",
    "    if initialization == 'Full':\n",
    "        T0 = ott_log_sinkhorn(C, a, b, max_inneriters_balanced)\n",
    "    elif initialization == 'Identity':\n",
    "        T0 = jnp.outer(a, b)\n",
    "    elif initialization == 'Rank-1':\n",
    "        T0 = jnp.outer(a, b)\n",
    "    elif initialization == 'Random':\n",
    "        T0 = jax.random.uniform(jax.random.PRNGKey(0), shape=(n, m))\n",
    "        T0 = T0 / T0.sum()\n",
    "    elif initialization == 'Given' and init_args is not None:\n",
    "        T0 = init_args\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported initialization method\")\n",
    "\n",
    "    U, s, Vt = jnp.linalg.svd(T0, full_matrices=False)\n",
    "    Q = U[:, :r]\n",
    "    R = Vt[:r, :].T\n",
    "    Lambda = jnp.diag(s[:r])\n",
    "\n",
    "    err = jnp.inf\n",
    "    it = 0\n",
    "    converged = False\n",
    "\n",
    "    while not converged and it < max_iter:\n",
    "        # Gradient via factorized formulation or dense formulation\n",
    "        gradQ, gradR, gamma_k = gd__compute_grad_A(\n",
    "            C_factors, A_factors, B_factors, Q, R, Lambda, gamma, alpha=alpha, full_grad=full_grad, low_rank=low_rank\n",
    "        )\n",
    "\n",
    "        # Update Q\n",
    "        Q -= gamma_k * gradQ\n",
    "        Q = Q / jnp.linalg.norm(Q, axis=0, keepdims=True)\n",
    "\n",
    "        T_mid = Q @ Lambda @ R.T\n",
    "\n",
    "        # Solve subproblem for R\n",
    "        T_R = ott_log_sinkhorn(C if not low_rank else None, a, b, max_inneriters_relaxed, init=T_mid)\n",
    "        U, s, Vt = jnp.linalg.svd(T_R, full_matrices=False)\n",
    "        R = Vt[:r2, :].T\n",
    "        Lambda = jnp.diag(s[:r2])\n",
    "\n",
    "        gradQ, gradR, gamma_k = gd__compute_grad_B(\n",
    "            C, A, B, Q, R, Lambda, gamma, alpha=alpha, full_grad=full_grad, low_rank==low_rank, C_factors=C_factors, A_factors=A_factors, B_factors=B_factors\n",
    "        )\n",
    "\n",
    "        # Update R\n",
    "        R -= gamma_k * gradR\n",
    "        R = R / jnp.linalg.norm(R, axis=0, keepdims=True)\n",
    "\n",
    "        T_mid = Q @ Lambda @ R.T\n",
    "\n",
    "        # Solve subproblem for Q\n",
    "        T_Q = ott_log_sinkhorn(C if not low_rank else None, a, b, max_inneriters_relaxed, init=T_mid)\n",
    "        U, s, Vt = jnp.linalg.svd(T_Q, full_matrices=False)\n",
    "        Q = U[:, :r]\n",
    "        Lambda = jnp.diag(s[:r])\n",
    "\n",
    "        # Convergence check\n",
    "        if convergence_criterion:\n",
    "            err = jnp.linalg.norm(T_Q - T_mid, ord='fro') / jnp.linalg.norm(T_Q, ord='fro')\n",
    "            if it >= min_iter and err < tol:\n",
    "                converged = True\n",
    "\n",
    "        it += 1\n",
    "\n",
    "    if returnFull:\n",
    "        return Q @ Lambda @ R.T\n",
    "    elif diagonalize_return:\n",
    "        return Q, Lambda, R.T\n",
    "    else:\n",
    "        return Q, R, Lambda\n",
    "\n",
    "    \n",
    "def FRLC_compute_OT_cost(X, Y, C = None, Monge_clusters = None, sq_Euclidean = True):\n",
    "    \"\"\"\n",
    "    Compute the optimal transport cost in linear space and time (without coupling), in JAX.\n",
    "    Supports squared Euclidean cost via OTT cost object.\n",
    "    \"\"\"\n",
    "    if Monge_clusters is None or len(Monge_clusters) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    def compute_pair_cost(pair):\n",
    "        idx1, idx2 = pair\n",
    "        if C is not None:\n",
    "            return C[idx1, idx2]\n",
    "        else:\n",
    "            diff = X[idx1] - Y[idx2]\n",
    "            if sq_Euclidean:\n",
    "                return jnp.sum(diff**2)\n",
    "            else:\n",
    "                return jnp.linalg.norm(diff)\n",
    "\n",
    "    pair_costs = jax.vmap(compute_pair_cost)(jnp.array(Monge_clusters))\n",
    "    total_cost = jnp.sum(pair_costs)\n",
    "    return total_cost / len(Monge_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 25\n",
      "Iteration: 50\n",
      "Iteration: 75\n",
      "FRLC cost: 23.961475372314453\n"
     ]
    }
   ],
   "source": [
    "# 1. Calcul de la matrice des coûts (distance euclidienne)\n",
    "def cdist_jax(X, Y):\n",
    "    # ||x - y||^2 = ||x||^2 + ||y||^2 - 2<x, y>\n",
    "    X_norm = jnp.sum(X ** 2, axis=1)[:, None]\n",
    "    Y_norm = jnp.sum(Y ** 2, axis=1)[None, :]\n",
    "    C = jnp.sqrt(jnp.maximum(X_norm + Y_norm - 2 * jnp.dot(X, Y.T), 0.0))\n",
    "    return C\n",
    "\n",
    "C = cdist_jax(X, Y)\n",
    "\n",
    "try:\n",
    "    # 2. Appel à FRLC_opt (on passe dtype=jnp.float32)\n",
    "    Q, R, T, errs = FRLC_opt(\n",
    "        C=C,\n",
    "        gamma=30,\n",
    "        r=40,\n",
    "        max_iter=100,\n",
    "        tau_in=100000,\n",
    "        low_rank=False\n",
    "    )\n",
    "\n",
    "    # 3. Calcul de la matrice de couplage P complète\n",
    "    inv_sum_Q = 1.0 / jnp.sum(Q, axis=0)  # shape (r,)\n",
    "    inv_sum_R = 1.0 / jnp.sum(R, axis=0)  # shape (r,)\n",
    "    P = (Q\n",
    "         @ jnp.diag(inv_sum_Q)\n",
    "         @ T\n",
    "         @ jnp.diag(inv_sum_R)\n",
    "         @ R.T)  # shape (n_X, n_Y)\n",
    "\n",
    "    # 4. Extraction des paires (i,j) où P[i,j] > 0\n",
    "    ij = jnp.argwhere(P > 0)  # shape (num_pairs, 2)\n",
    "    Monge_clusters = [(int(i), int(j)) for i, j in ij]\n",
    "\n",
    "    # 5. Calcul du coût OT via la fonction JAXisée\n",
    "    cost_frlc = FRLC_compute_OT_cost(\n",
    "        X, Y,\n",
    "        C=C,\n",
    "        Monge_clusters=Monge_clusters,\n",
    "        sq_Euclidean=True\n",
    "    )\n",
    "\n",
    "    print(f'FRLC cost: {cost_frlc}')\n",
    "\n",
    "    # 6. Approximation du couplage pour l'extraction de correspondances\n",
    "    P_approx = Q @ T @ R.T  # shape (n_X, n_Y)\n",
    "    matches_Y = jnp.argmax(P_approx, axis=1)  # shape (n_X,)\n",
    "\n",
    "    # 7. Construction de la liste F de paires indices\n",
    "    F = [\n",
    "        (jnp.array([i], dtype=jnp.int32),\n",
    "         jnp.array([int(matches_Y[i])], dtype=jnp.int32))\n",
    "        for i in range(X.shape[0])\n",
    "    ]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'FRLC failed for sample size {X.shape[0]}: {e}')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1158f",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------\n",
    "Code for gradients assuming low-rank distance matrices C, A, B\n",
    "--------------\n",
    "'''\n",
    "\n",
    "def gd__compute_grad_A_LR(C_factors, A_factors, B_factors, Q, R, Lambda, gamma, alpha=0.0, full_grad=False):\n",
    "    \n",
    "    N1, N2 = C_factors[0].shape[0], C_factors[1].shape[1]\n",
    "\n",
    "    if A_factors is not None and B_factors is not None:\n",
    "        A1, A2 = A_factors\n",
    "        B1, B2 = B_factors\n",
    "\n",
    "        # GW gradients\n",
    "        gradQ = -4 * (A1 @ (A2 @ (Q @ Lambda @ ((R.T @ B1) @ (B2 @ R)) @ Lambda.T)))\n",
    "        gradR = -4 * (B1 @ (B2 @ (R @ (Lambda.T @ ((Q.T @ A1) @ (A2 @ Q)) @ Lambda))))\n",
    "\n",
    "        one_N1 = jnp.ones((N1,), dtype=Q.dtype)\n",
    "        one_N2 = jnp.ones((N2,), dtype=R.dtype)\n",
    "\n",
    "        if full_grad:\n",
    "            gQ = Q.T @ one_N1\n",
    "            gR = R.T @ one_N2\n",
    "\n",
    "            MR = (Lambda.T @ ((Q.T @ A1) @ (A2 @ Q)) @ Lambda\n",
    "                  @ ((R.T @ B1) @ (B2 @ R)) @ jnp.diag(1.0 / gR))\n",
    "            MQ = (Lambda @ ((R.T @ B1) @ (B2 @ R)) @ Lambda.T\n",
    "                  @ ((Q.T @ A1) @ (A2 @ Q)) @ jnp.diag(1.0 / gQ))\n",
    "\n",
    "            gradQ += 4 * jnp.outer(one_N1, jnp.diag(MQ))\n",
    "            gradR += 4 * jnp.outer(one_N2, jnp.diag(MR))\n",
    "    else:\n",
    "        gradQ = jnp.zeros_like(Q)\n",
    "        gradR = jnp.zeros_like(R)\n",
    "\n",
    "    # Appel à une version jaxifiée de gd__Wasserstein_Grad_LR\n",
    "    gradQW, gradRW = gd__Wasserstein_Grad_LR(C_factors, Q, R, Lambda, full_grad=full_grad)\n",
    "\n",
    "    gradQ = (1 - alpha) * gradQW + (alpha / 2.0) * gradQ\n",
    "    gradR = (1 - alpha) * gradRW + (alpha / 2.0) * gradR\n",
    "\n",
    "    normalizer = jnp.maximum(jnp.max(jnp.abs(gradQ)), jnp.max(jnp.abs(gradR)))\n",
    "    gamma_k = gamma / normalizer\n",
    "\n",
    "    return gradQ, gradR, gamma_k\n",
    "\n",
    "def gd__compute_grad_B_LR(C_factors, A_factors, B_factors, Q, R, Lambda, gQ, gR, gamma, alpha=0.0):\n",
    "    \"\"\"\n",
    "    Low-rank gradient computation in JAX for Wasserstein / Gromov-Wasserstein.\n",
    "    \"\"\"\n",
    "    C1, C2 = C_factors  # (N1, rC), (rC, N2)\n",
    "    gradLambda = 0.0\n",
    "\n",
    "    if A_factors is not None and B_factors is not None:\n",
    "        A1, A2 = A_factors  # (N1, rA), (rA, N1)\n",
    "        B1, B2 = B_factors  # (N2, rB), (rB, N2)\n",
    "        term_A = (Q.T @ A1) @ (A2 @ Q)         # shape: (r, r)\n",
    "        term_B = (R.T @ B1) @ (B2 @ R)         # shape: (r, r)\n",
    "        gradLambda = -4.0 * term_A @ Lambda @ term_B\n",
    "\n",
    "    term_C = (Q.T @ C1) @ (C2 @ R)             # shape: (r, r)\n",
    "    gradLambda = (1 - alpha) * term_C + (alpha / 2.0) * gradLambda\n",
    "\n",
    "    gradT = jnp.diag(1.0 / gQ) @ gradLambda @ jnp.diag(1.0 / gR)\n",
    "    gamma_T = gamma / jnp.max(jnp.abs(gradT))\n",
    "    return gradT, gamma_T\n",
    "\n",
    "def gd__Wasserstein_Grad_LR(C_factors, Q, R, Lambda, full_grad=True):\n",
    "    \"\"\"\n",
    "    JAX version of Wasserstein gradient with low-rank cost approximation:\n",
    "    C ≈ C1 @ C2.T\n",
    "    \"\"\"\n",
    "    C1, C2 = C_factors\n",
    "\n",
    "    gradQ = C1 @ ((C2 @ R) @ Lambda.T)\n",
    "    if full_grad:\n",
    "        N1 = Q.shape[0]\n",
    "        one_N1 = jnp.ones((N1,), dtype=Q.dtype)\n",
    "        gQ = Q.T @ one_N1\n",
    "        w1 = jnp.diag((gradQ.T @ Q) @ jnp.diag(1.0 / gQ))\n",
    "        gradQ = gradQ - jnp.outer(one_N1, w1)\n",
    "\n",
    "    gradR = C2.T @ ((C1.T @ Q) @ Lambda)\n",
    "    if full_grad:\n",
    "        N2 = R.shape[0]\n",
    "        one_N2 = jnp.ones((N2,), dtype=R.dtype)\n",
    "        gR = R.T @ one_N2\n",
    "        w2 = jnp.diag(jnp.diag(1.0 / gR) @ (R.T @ gradR))\n",
    "        gradR = gradR - jnp.outer(one_N2, w2)\n",
    "\n",
    "    return gradQ, gradR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe4033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_annealing__factors(n):\n",
    "    \"\"\"\n",
    "    Return list of all factors of an integer\n",
    "    \"\"\"\n",
    "    n = int(n)  # Conversion pour compatibilité avec jnp.arange\n",
    "    candidates = jnp.arange(1, jnp.floor(jnp.sqrt(n)) + 1).astype(int)\n",
    "    divisible = (n % candidates) == 0\n",
    "    factors1 = candidates[divisible]\n",
    "    factors2 = n // factors1\n",
    "    all_factors = jnp.concatenate([factors1, factors2])\n",
    "    unique_factors = jnp.unique(all_factors)\n",
    "    return unique_factors\n",
    "\n",
    "def rank_annealing__max_factor_lX(n, max_X):\n",
    "    \"\"\"\n",
    "    Find max factor of n , such that max_factor \\leq max_X\n",
    "    \"\"\"\n",
    "    factor_lst = rank_annealing__factors(n)\n",
    "    factors_leq_max = factor_lst[factor_lst <= max_X]\n",
    "    return jnp.max(factors_leq_max)\n",
    "\n",
    "def rank_annealing__min_sum_partial_products_with_factors(n, k, C):\n",
    "    \"\"\"\n",
    "    Dynamic program to compute the rank-schedule, subject to a constraint of intermediates being \\leq C\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        The dataset size to be factored into a rank-scheduler. Assumed to be non-prime.\n",
    "    k: int\n",
    "        The depth of the hierarchy.\n",
    "    C: int\n",
    "        A constraint on the maximal intermediate rank across the hierarchy.\n",
    "    \n",
    "    \"\"\"\n",
    "    INF = 1e10  # Large constant instead of float('inf') for JAX compatibility\n",
    "\n",
    "    dp = jnp.full((n+1, k+1), INF)\n",
    "    choice = jnp.full((n+1, k+1), -1)\n",
    "\n",
    "    def init_base_case(dp, choice):\n",
    "        d = jnp.arange(1, n+1)\n",
    "        mask = d <= C\n",
    "        dp = dp.at[d[mask], 1].set(d[mask])\n",
    "        choice = choice.at[d[mask], 1].set(d[mask])\n",
    "        return dp, choice\n",
    "\n",
    "    dp, choice = init_base_case(dp, choice)\n",
    "\n",
    "    for t in range(2, k+1):\n",
    "        for d in range(1, n+1):\n",
    "            if dp[d, t-1] >= INF:\n",
    "                continue\n",
    "            for r in range(1, min(C, d)+1):\n",
    "                if d % r == 0:\n",
    "                    candidate = r + r * dp[d // r, t-1]\n",
    "                    if candidate < dp[d, t]:\n",
    "                        dp = dp.at[d, t].set(candidate)\n",
    "                        choice = choice.at[d, t].set(r)\n",
    "\n",
    "    if dp[n, k] >= INF:\n",
    "        return None, []\n",
    "\n",
    "    # Backtracking\n",
    "    factors = []\n",
    "    d_cur, t_cur = n, k\n",
    "    while t_cur > 0:\n",
    "        r_cur = int(choice[d_cur, t_cur])\n",
    "        factors.append(r_cur)\n",
    "        d_cur //= r_cur\n",
    "        t_cur -= 1\n",
    "\n",
    "    return dp[n, k], factors\n",
    "\n",
    "def rank_annealing__optimal_rank_schedule(n, hierarchy_depth=6, max_Q=int(2**10), max_rank=16):\n",
    "    \"\"\"\n",
    "    A function to compute the optimal rank-scheduler of refinement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        Size of the input dataset -- cannot be a prime number\n",
    "    hierarchy_depth: int\n",
    "        Maximal permissible depth of the multi-scale hierarchy\n",
    "    max_Q: int\n",
    "        Maximal rank at terminal base case (before reducing the \\leq max_Q rank coupling to a 1-1 alignment)\n",
    "    max_rank: int\n",
    "        Maximal rank at the intermediate steps of the rank-schedule\n",
    "        \n",
    "    \"\"\"\n",
    "    Q = int(rank_annealing__max_factor_lX(n, max_Q))\n",
    "    ndivQ = int(n // Q)\n",
    "\n",
    "    _, rank_schedule = rank_annealing__min_sum_partial_products_with_factors(ndivQ, hierarchy_depth, max_rank)\n",
    "    rank_schedule = sorted(rank_schedule)\n",
    "    rank_schedule.append(Q)\n",
    "    rank_schedule = [x for x in rank_schedule if x != 1]\n",
    "\n",
    "    print(f'Optimized rank-annealing schedule: {rank_schedule}')\n",
    "\n",
    "    assert functools.reduce(operator.mul, rank_schedule, 1) == n, \"Error! Rank-schedule does not factorize n!\"\n",
    "\n",
    "    return rank_schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43693af1",
   "metadata": {},
   "source": [
    "This function allows representing the squared Euclidean cost matrix in factorized form to avoid storing the full matrix, which is crucial for scaling the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lr_sqeuclidean_matrix(X_s,\n",
    "                                  X_t,\n",
    "                                  rescale_cost: bool = False):\n",
    "    \"\"\"\n",
    "    Adapted to JAX from the low-rank squared Euclidean cost decomposition,\n",
    "    as in Scetbon, Cuturi & Peyré (2021), Section 3.5, Proposition 1.\n",
    "    \"\"\"\n",
    "\n",
    "    ns, dim = X_s.shape\n",
    "    nt, _ = X_t.shape\n",
    "\n",
    "    # First low-rank term (source-side)\n",
    "    sum_Xs_sq = jnp.sum(X_s ** 2, axis=1, keepdims=True)         # (ns, 1)\n",
    "    ones_ns = jnp.ones((ns, 1), dtype=X_s.dtype)                 # (ns, 1)\n",
    "    neg_two_Xs = -2.0 * X_s                                      # (ns, d)\n",
    "    M1 = jnp.concatenate([sum_Xs_sq, ones_ns, neg_two_Xs], axis=1)  # (ns, d+2)\n",
    "\n",
    "    # Second low-rank term (target-side)\n",
    "    ones_nt = jnp.ones((nt, 1), dtype=X_t.dtype)                 # (nt, 1)\n",
    "    sum_Xt_sq = jnp.sum(X_t ** 2, axis=1, keepdims=True)         # (nt, 1)\n",
    "    M2 = jnp.concatenate([ones_nt, sum_Xt_sq, X_t], axis=1)      # (nt, d+2)\n",
    "\n",
    "    if rescale_cost:\n",
    "        # Use jnp.max over entire arrays\n",
    "        max_M1 = jnp.max(jnp.abs(M1))\n",
    "        max_M2 = jnp.max(jnp.abs(M2))\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if max_M1 > 0:\n",
    "            M1 = M1 / jnp.sqrt(max_M1)\n",
    "        if max_M2 > 0:\n",
    "            M2 = M2 / jnp.sqrt(max_M2)\n",
    "\n",
    "    return M1, M2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115e17a",
   "metadata": {},
   "source": [
    "<h2> Implementation of Hierarchical Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13668d46",
   "metadata": {},
   "source": [
    "The core of the paper is the \"Hierarchical Refinement\" algorithm that uses low-rank decompositions recursively to build a full transport plan between two datasets. \\\n",
    "The key insight is that optimal factors of low-rank optimal transport co-cluster points with their image under the Monge map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalRefinementOT:\n",
    "    \"\"\"\n",
    "    A class to perform Hierarchical OT refinement with optional (CPU) parallelization.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    C : torch.tensor\n",
    "        The cost matrix of shape (N, N), currently assumed square for hierarchical OT.\n",
    "        Can represent general user-defined costs.\n",
    "    rank_schedule : list\n",
    "        The list of ranks for each hierarchical level -- i.e. the rank-annealing schedule.\n",
    "    solver : callable\n",
    "        A low-rank OT solver that takes a cost submatrix and returns Q, R, diagG, errs.\n",
    "    solver_params: Dict[str, Any], optional\n",
    "        Additional parameters for the low-rank solver. If None, default values are used.\n",
    "    device : str\n",
    "        The device ('cpu' or 'cuda') to be used for computations.\n",
    "    base_rank : int\n",
    "        Base-case rank at which to stop subdividing clusters.\n",
    "    clustering_type : str\n",
    "        'soft' or 'hard'. Determines how cluster assignments are computed after each OT solve.\n",
    "    plot_clusterings : bool\n",
    "        Whether to plot the Q and R matrices at each step for debugging.\n",
    "    parallel : bool\n",
    "        Whether to execute each subproblem at a level in parallel.\n",
    "    num_processes : int or None\n",
    "        Number of worker processes to spawn (if `parallel=True`). Defaults to `None` which uses `mp.cpu_count()`.\n",
    "    X, Y : torch.tensor\n",
    "        The point-clouds for the first dataset (X) and the second dataset (Y)\n",
    "    N : int\n",
    "        The size of the dataset.\n",
    "    Monge_clusters : list (tuples of type torch.float)\n",
    "        A list containing the Monge-map pairings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                C: torch.Tensor,\n",
    "                 rank_schedule: List[int],\n",
    "                 solver: Callable = FRLC_opt,\n",
    "                 solver_params: Union[Dict[str, Any] , None] = None,\n",
    "                 device: str = 'cpu',\n",
    "                 base_rank: int = 1,\n",
    "                 clustering_type: str = 'soft',\n",
    "                 plot_clusterings: bool = False,\n",
    "                 parallel: bool = False,\n",
    "                 num_processes: Union[int, None] = None\n",
    "                ):\n",
    "    \n",
    "        self.C = C.to(device)\n",
    "        self.rank_schedule = rank_schedule\n",
    "        self.solver = solver\n",
    "        self.device = device\n",
    "        self.base_rank = base_rank\n",
    "        self.clustering_type = clustering_type\n",
    "        self.plot_clusterings =  plot_clusterings\n",
    "        self.parallel = parallel\n",
    "        self.num_processes = num_processes\n",
    "        \n",
    "        # Point clouds optional attributes\n",
    "        self.X, self.Y = None, None\n",
    "        self.N = C.shape[0]\n",
    "        self.Monge_clusters = None\n",
    "        # This is a dummy line -- this init doesn't compute C or its factorization\n",
    "        self.sq_Euclidean = False\n",
    "\n",
    "        # Setting parameters to use with the FRLC solver\n",
    "        default_solver_params = {\n",
    "            'gamma' : 30,\n",
    "            'max_iter' : 60,\n",
    "            'min_iter' : 25,\n",
    "            'max_inneriters_balanced' : 100,\n",
    "            'max_inneriters_relaxed' : 40,\n",
    "            'printCost' : False,\n",
    "            'tau_in' : 100000\n",
    "        }\n",
    "        if solver_params is not None:\n",
    "            default_solver_params.update(solver_params)\n",
    "        self.solver_params = default_solver_params\n",
    "        \n",
    "        assert C.shape[0] == C.shape[1], \"Currently assume square costs so that |X| = |Y| = N\"\n",
    "    \n",
    "    @classmethod\n",
    "    def init_from_point_clouds(cls,\n",
    "                            X: torch.Tensor,\n",
    "                            Y: torch.Tensor,\n",
    "                            rank_schedule: List[int],\n",
    "                            distance_rank_schedule: Union[List[int], None] = None,\n",
    "                            solver: Callable = lambda *args: FRLC_opt(*args, low_rank=True),\n",
    "                            solver_params: Union[Dict[str, Any] , None] = None,\n",
    "                            device: str = 'cpu',\n",
    "                            base_rank: int = 1,\n",
    "                            clustering_type: str = 'soft',\n",
    "                            plot_clusterings: bool = False,\n",
    "                            parallel: bool = False,\n",
    "                            num_processes: Union[int, None] = None,\n",
    "                              sq_Euclidean = False):\n",
    "        r\"\"\"\n",
    "        Constructor for initializing from point clouds.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        X : torch.tensor\n",
    "            The point-cloud of shape N for measure \\mu\n",
    "        Y: torch.tensor\n",
    "            Point cloud of shape N for measure \\nu\n",
    "        distance_rank_schedule: List[int]\n",
    "            A separate rank-schedule for the low-rank distance matrix being factorized.\n",
    "        sq_Euclidean : bool\n",
    "            If True, assumes squared Euclidean cost. Otherwise, defaults to Euclidean.\n",
    "            Needed for the point-cloud variant, in order to define a distance metric\n",
    "            to use for the low-rank approximation of C.\n",
    "        \"\"\"\n",
    "        \n",
    "        obj = cls.__new__(cls)\n",
    "        \n",
    "        obj.X = X\n",
    "        obj.Y = Y\n",
    "        obj.rank_schedule = rank_schedule\n",
    "        \n",
    "        if distance_rank_schedule is None:\n",
    "            # Default: assume distance rank schedule is identical to rank schedule for coupling.\n",
    "            obj.distance_rank_schedule = rank_schedule\n",
    "        else:\n",
    "            obj.distance_rank_schedule = distance_rank_schedule\n",
    "        \n",
    "        obj.solver = solver\n",
    "        obj.device = device\n",
    "        obj.base_rank = base_rank\n",
    "        obj.clustering_type = clustering_type\n",
    "        obj.plot_clusterings =  plot_clusterings\n",
    "        obj.parallel = parallel\n",
    "        obj.num_processes = num_processes\n",
    "        obj.N = X.shape[0]\n",
    "        \n",
    "        # Cost-mat an optional attribute\n",
    "        obj.C = None\n",
    "        obj.Monge_clusters = None\n",
    "        obj.sq_Euclidean = sq_Euclidean\n",
    "\n",
    "        # Setting parameters to use with the FRLC solver\n",
    "        default_solver_params = {\n",
    "            'gamma' : 30,\n",
    "            'max_iter' : 60,\n",
    "            'min_iter' : 25,\n",
    "            'max_inneriters_balanced' : 100,\n",
    "            'max_inneriters_relaxed' : 40,\n",
    "            'printCost' : False,\n",
    "            'tau_in' : 100000\n",
    "        }\n",
    "        if solver_params is not None:\n",
    "            default_solver_params.update(solver_params)\n",
    "        obj.solver_params = default_solver_params\n",
    "        \n",
    "        assert X.shape[0] == Y.shape[0], \"Currently assume square costs so that |X| = |Y| = N\"\n",
    "        \n",
    "        return obj\n",
    "\n",
    "    def run(self, return_as_coupling: bool = False):\n",
    "        \"\"\"\n",
    "        Routine to run hierarchical refinement.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        return_as_coupling : bool\n",
    "            Whether to return a full coupling matrix (size NxN) \n",
    "            or a list of (idxX, idxY) co-clusters / assignments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of (idxX, idxY) pairs OR torch.tensor\n",
    "            If return_as_coupling=False: returns a list of tuples (idxX, idxY) for each co-cluster.\n",
    "            If return_as_coupling=True: returns a dense coupling matrix of shape (N, N).\n",
    "        \"\"\"\n",
    "        if self.parallel:\n",
    "            \"\"\" WARNING: not currently implemented in this class! See HR_OT_parallelized instead! \"\"\"\n",
    "            return self._hierarchical_refinement_parallelized(return_as_coupling = return_as_coupling)\n",
    "        else:\n",
    "            return self._hierarchical_refinement(return_as_coupling = return_as_coupling)\n",
    "\n",
    "    def _hierarchical_refinement(self, return_as_coupling: bool = False):\n",
    "        \"\"\"\n",
    "        Single-process (serial) Hierarchical Refinement\n",
    "        \"\"\"\n",
    "\n",
    "        # Define partitions\n",
    "        F_t = [(torch.arange( self.N , device=self.device), \n",
    "                torch.arange( self.N , device=self.device))]\n",
    "        \n",
    "        for i, rank_level in enumerate(self.rank_schedule):\n",
    "            # Iterate over ranks in the scheduler\n",
    "            F_tp1 = []\n",
    "\n",
    "            if i == len(self.rank_schedule)-1:\n",
    "                fin_iters = int(self.N / rank_level)\n",
    "                print(f'Last level, rank chunk-size {rank_level} with {fin_iters} iterations to completion.')\n",
    "                j = 0\n",
    "            \n",
    "            for (idxX, idxY) in F_t:\n",
    "\n",
    "                if i == len(self.rank_schedule)-1:\n",
    "                    print(f'{j}/{fin_iters} of final-level iterations to completion')\n",
    "                    j += 1\n",
    "                \n",
    "                if len(idxX) <=self.base_rank or len(idxY) <= self.base_rank:\n",
    "                    # Return tuple of base-rank sized index sets (e.g. (x,T(x)) for base_rank=1)\n",
    "                    F_tp1.append( ( idxX, idxY ) )\n",
    "                    continue\n",
    "                \n",
    "                if self.C is not None:\n",
    "                    Q,R = self._solve_prob( idxX, idxY, rank_level)\n",
    "                else:\n",
    "                    rank_D = self.distance_rank_schedule[i]\n",
    "                    Q,R = self._solve_LR_prob( idxX, idxY, rank_level, rank_D )\n",
    "                \n",
    "                if self.plot_clusterings:\n",
    "                    # If visualizing the Q - R clustering matrices.\n",
    "                    \n",
    "                    plt.figure(figsize=(12, 5))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.imshow(Q.detach().cpu().numpy(), aspect='auto', cmap='viridis')\n",
    "                    plt.title(f\"Q Clustering Level {i+1}\")\n",
    "                    plt.colorbar()\n",
    "    \n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.imshow(R.detach().cpu().numpy(), aspect='auto', cmap='viridis')\n",
    "                    plt.title(f\"R Clustering Level {i+1}\")\n",
    "                    plt.colorbar()\n",
    "                    plt.show()\n",
    "                \n",
    "                # Next level cluster capacity\n",
    "                capacity = int(self.N) / int(torch.prod(torch.Tensor(self.rank_schedule[0:i+1])))\n",
    "                capacity = int(capacity)\n",
    "                \n",
    "                idx_seenX, idx_seenY = torch.arange(Q.shape[0], device=self.device), \\\n",
    "                                                    torch.arange(R.shape[0], device=self.device)\n",
    "                \n",
    "                # Split by hard or soft-clustering\n",
    "                if self.clustering_type == 'soft':\n",
    "                    # If using a solver which returns \"soft\" clusterings, must strictly fill partitions to capacities.\n",
    "                    \n",
    "                    for z in range(rank_level):\n",
    "                        \n",
    "                        topk_values, topk_indices_X = torch.topk( Q[idx_seenX][:,z], k=capacity )\n",
    "                        idxX_z = idxX[idx_seenX[topk_indices_X]]\n",
    "                        topk_values, topk_indices_Y = torch.topk( R[idx_seenY][:,z], k=capacity )\n",
    "                        idxY_z = idxY[idx_seenY[topk_indices_Y]]\n",
    "                        \n",
    "                        F_tp1.append(( idxX_z, idxY_z ))\n",
    "                        \n",
    "                        idx_seenX = idx_seenX[~torch.isin(idx_seenX, idx_seenX[topk_indices_X])]\n",
    "                        idx_seenY = idx_seenY[~torch.isin(idx_seenY, idx_seenY[topk_indices_Y])]\n",
    "                \n",
    "                elif self.clustering_type == 'hard':\n",
    "                    # If using a solver which returns \"hard\" clusterings, can exactly take argmax.\n",
    "                    \n",
    "                    zX = torch.argmax(Q, axis=1) # X-assignments\n",
    "                    zY = torch.argmax(R, axis=1) # Y-assignments\n",
    "                    \n",
    "                    for z in range(rank_level):\n",
    "                        \n",
    "                        idxX_z = idxX[zX == z]\n",
    "                        idxY_z = idxY[zY == z]\n",
    "    \n",
    "                        assert len(idxX_z) == len(idxY_z) == capacity, \\\n",
    "                                            \"Assertion failed! Not a hard-clustering function, or point sets of unequal size!\"\n",
    "                        \n",
    "                        F_tp1.append((idxX_z, idxY_z))\n",
    "                        \n",
    "            F_t = F_tp1\n",
    "        \n",
    "        self.Monge_clusters = F_t\n",
    "\n",
    "        \n",
    "        if return_as_coupling is False:\n",
    "            return self.Monge_clusters\n",
    "        else:\n",
    "            return self._compute_coupling_from_Ft()\n",
    "\n",
    "    def _solve_LR_prob(self, idxX, idxY, rank_level, rankD, eps=0.04):\n",
    "        \n",
    "        \"\"\"\n",
    "        Solve problem for low-rank coupling under a low-rank factorization of distance matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        _x0, _x1 = torch.index_select(self.X, 0, idxX), torch.index_select(self.Y, 0, idxY)\n",
    "        \n",
    "        if rankD < _x0.shape[0]:\n",
    "            \n",
    "            C_factors, A_factors, B_factors = self.get_dist_mats(_x0, _x1, \n",
    "                                                                 rankD, eps, \n",
    "                                                                 self.sq_Euclidean )\n",
    "            \n",
    "            # Solve a low-rank OT sub-problem with black-box solver\n",
    "            Q, R, diagG, errs = self.solver(C_factors, A_factors, B_factors,\n",
    "                                       gamma = self.solver_params['gamma'],\n",
    "                                       r = rank_level,\n",
    "                                       max_iter = self.solver_params['max_iter'],\n",
    "                                       device=self.device,\n",
    "                                       min_iter = self.solver_params['min_iter'],\n",
    "                                       max_inneriters_balanced = self.solver_params['max_inneriters_balanced'],\n",
    "                                       max_inneriters_relaxed = self.solver_params['max_inneriters_relaxed'],\n",
    "                                       diagonalize_return = True,\n",
    "                                       printCost = False, tau_in = self.solver_params['tau_in'],\n",
    "                                        dtype = _x0.dtype)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Final base instance -- cost within-cluster costs explicitly\n",
    "            if self.sq_Euclidean:\n",
    "                C_XY = torch.cdist(_x0, _x1)**2\n",
    "                \n",
    "            else:\n",
    "                # normal Euclidean distance otherwise\n",
    "                C_XY = torch.cdist(_x0, _x1)\n",
    "            \n",
    "            Q, R, diagG, errs = FRLC_opt(C_XY,\n",
    "                                   gamma = self.solver_params['gamma'],\n",
    "                                   r = rank_level,\n",
    "                                   max_iter = self.solver_params['max_iter'],\n",
    "                                   device = self.device,\n",
    "                                   min_iter = self.solver_params['min_iter'],\n",
    "                                   max_inneriters_balanced = self.solver_params['max_inneriters_balanced'],\n",
    "                                   max_inneriters_relaxed = self.solver_params['max_inneriters_relaxed'],\n",
    "                                   diagonalize_return=True,\n",
    "                                   printCost=False, tau_in = self.solver_params['tau_in'],\n",
    "                                       dtype = C_XY.dtype)\n",
    "            \n",
    "        return Q, R\n",
    "        \n",
    "    def _solve_prob(self, idxX, idxY, rank_level):\n",
    "        \"\"\"\n",
    "        Solve problem for low-rank coupling assuming cost sub-matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Index into sub-cost\n",
    "        submat = torch.index_select(self.C, 0, idxX)\n",
    "        C_XY = torch.index_select(submat, 1, idxY)\n",
    "        \n",
    "        # Solve a low-rank OT sub-problem with black-box solver\n",
    "        Q, R, diagG, errs = self.solver(C_XY,\n",
    "                                   gamma = self.solver_params['gamma'],\n",
    "                                   r = rank_level,\n",
    "                                   max_iter = self.solver_params['max_iter'],\n",
    "                                   device=self.device,\n",
    "                                   min_iter = self.solver_params['min_iter'],\n",
    "                                   max_inneriters_balanced = self.solver_params['max_inneriters_balanced'],\n",
    "                                   max_inneriters_relaxed = self.solver_params['max_inneriters_relaxed'],\n",
    "                                   diagonalize_return=True,\n",
    "                                   printCost=False, tau_in = self.solver_params['tau_in'],\n",
    "                                       dtype = C_XY.dtype)\n",
    "        return Q, R\n",
    "    \n",
    "    def _compute_coupling_from_Ft(self):\n",
    "        \"\"\"\n",
    "        Returns coupling as a full-rank matrix rather than as a set of (x, T(x)) pairs.\n",
    "        \"\"\"\n",
    "        size = (self.N, self.N)\n",
    "        P = torch.zeros(size)\n",
    "        # Fill sparse coupling with entries\n",
    "        for pair in self.Monge_clusters:\n",
    "            idx1, idx2 = pair\n",
    "            P[idx1, idx2] = 1\n",
    "        # Return, trivially normalized to satisfy standard OT constraints\n",
    "        return P / self.N\n",
    "    \n",
    "    def compute_OT_cost(self):\n",
    "        \"\"\"\n",
    "        Compute the optimal transport in linear space and time (w/o coupling).\n",
    "        \"\"\"\n",
    "        \n",
    "        cost = 0\n",
    "        for clus in self.Monge_clusters:\n",
    "            idx1, idx2 = clus\n",
    "            if self.C is not None:\n",
    "                # If C saved, index into general cost directly\n",
    "                cost += self.C[idx1, idx2]\n",
    "            else:\n",
    "                # In case point-cloud init used, must directly compute distances between point pairs in X, Y.\n",
    "                if self.sq_Euclidean:\n",
    "                    # squared Euclidean case\n",
    "                    cost += torch.norm(self.X[idx1,:] - self.Y[idx2,:])**2\n",
    "                else:\n",
    "                    # normal Euclidean cost\n",
    "                    cost += torch.norm(self.X[idx1,:] - self.Y[idx2,:])\n",
    "        # Appropriately normalize the cost\n",
    "        cost = cost / self.N\n",
    "        return cost\n",
    "\n",
    "    \n",
    "    def get_dist_mats(self, _x0, _x1, rankD, eps , sq_Euclidean ):\n",
    "        \n",
    "        # Wasserstein-only, setting A and B factors to be NoneType\n",
    "        A_factors = None\n",
    "        B_factors = None\n",
    "        \n",
    "        if sq_Euclidean:\n",
    "            # Sq Euclidean\n",
    "            C_factors = compute_lr_sqeuclidean_matrix(_x0, _x1, True)\n",
    "        else:\n",
    "            # Standard Euclidean dist\n",
    "            C_factors = self.ret_normalized_cost(_x0, _x1, rankD, eps)\n",
    "        \n",
    "        return C_factors, A_factors, B_factors\n",
    "    \n",
    "    def ret_normalized_cost(self, X, Y, rankD, eps):\n",
    "        \n",
    "        C1, C2 = utils__low_rank_distance_factorization(X,\n",
    "                                                      Y,\n",
    "                                                      r=rankD,\n",
    "                                                      eps=eps,\n",
    "                                                      device=self.device)\n",
    "        # Normalize appropriately\n",
    "        c = ( C1.max()**1/2 ) * ( C2.max()**1/2 )\n",
    "        C1, C2 = C1/c, C2/c\n",
    "        C_factors = (C1.to(X.dtype), C2.to(X.dtype))\n",
    "        \n",
    "        return C_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized rank-annealing schedule: [2, 1250]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     hrot_lr \u001b[38;5;241m=\u001b[39m \u001b[43mHierarchicalRefinementOT\u001b[49m\u001b[38;5;241m.\u001b[39minit_from_point_clouds(X, Y, rank_schedule, base_rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m X, Y\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HierarchicalRefinementOT' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHR-OT cost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcost_hrot_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHROT-LR failed for sample size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "rank_schedule = rank_annealing__optimal_rank_schedule( X.shape[0] , hierarchy_depth = 6, max_Q = int(2**11), max_rank = 64 )\n",
    "\n",
    "try:\n",
    "    hrot_lr = HierarchicalRefinementOT.init_from_point_clouds(X, Y, rank_schedule, base_rank=1, device=device)\n",
    "    del X, Y\n",
    "    F = hrot_lr.run(return_as_coupling=False)\n",
    "    cost_hrot_lr = hrot_lr.compute_OT_cost()\n",
    "    print(f'HR-OT cost: {cost_hrot_lr}')\n",
    "except Exception as e:\n",
    "    print(f'HROT-LR failed for sample size {n}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48514026",
   "metadata": {},
   "source": [
    "<h2> Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc4821",
   "metadata": {},
   "source": [
    "In this tutorial, we explored the Hierarchical Refinement (HiRef) algorithm for large-scale optimal transport. This innovative algorithm allows computing bijective correspondences between large datasets with linear space complexity, thus overcoming one of the main limitations of traditional optimal transport methods.\n",
    "We implemented the key components of the algorithm, including:\n",
    "\n",
    "The low-rank optimal transport solver (FRLC)\n",
    "The calculation of the optimal rank annealing schedule\n",
    "The main hierarchical refinement algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e6069",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
