{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945f7bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\courn\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Callable, Union, Dict, Any\n",
    "import random\n",
    "import operator\n",
    "import functools\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ott.geometry import costs, pointcloud\n",
    "from ott.tools import sinkhorn_divergence, progot\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from ott.geometry import geometry\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cae2dc",
   "metadata": {},
   "source": [
    "# 1. Load images & model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173d8ca",
   "metadata": {},
   "source": [
    "You can also directly go to part 3 if you want to start directly from the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d50b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Liste tous les fichiers .jpg\n",
    "        self.image_paths = [os.path.join(root_dir, fname)\n",
    "                            for fname in os.listdir(root_dir)\n",
    "                            if fname.endswith('.jpg')]\n",
    "        # Si le nombre d'images est impair, enlever la dernière\n",
    "        if len(self.image_paths) % 2 != 0:\n",
    "            self.image_paths = self.image_paths[:-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Toujours convertir en RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # Pas d'étiquette ici, juste l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d362e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 images from ImageNet!\n"
     ]
    }
   ],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images for CNN input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load COCO dataset from extracted path\n",
    "imagenet_dataset = CustomImageDataset(root_dir='images', transform=transform)\n",
    "\n",
    "\n",
    "# Create DataLoader for batching\n",
    "imagenet_loader = DataLoader(imagenet_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Loaded {len(imagenet_dataset)} images from ImageNet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280953f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.expanduser(\"resnet50-0676ba61.pth\")\n",
    "\n",
    "# Load pretrained ResNet model\n",
    "model = models.resnet50()\n",
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "model.fc = torch.nn.Identity()  # Remove classification layer to extract features\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f1229",
   "metadata": {},
   "source": [
    "# 2. Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6001bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 157/157 [04:00<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_features(dataloader, model):\n",
    "    \"\"\"\n",
    "    Compute embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for idx, images in tqdm(enumerate(dataloader), desc=\"Extracting features\", total=len(dataloader)):\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            embeddings.append(jnp.array(features.detach().cpu().numpy()))  \n",
    "    return jnp.vstack(embeddings)  # Stack all embeddings\n",
    "\n",
    "print('extracting embeddings!')\n",
    "embeddings = extract_features(imagenet_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dbe4e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully to embeddings/embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "with open('embeddings/embeddings.pkl', \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(\"Embeddings saved successfully to embeddings/embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb07d8",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde20a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully! Shape: (5000, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings from the pickle file\n",
    "with open('embeddings/embeddings.pkl', \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(f\"Embeddings loaded successfully! Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155a4ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2500, 2048), Y shape: (2500, 2048)\n"
     ]
    }
   ],
   "source": [
    "num_samples = embeddings.shape[0]\n",
    "\n",
    "# Shuffle indices\n",
    "key = jax.random.PRNGKey(42)\n",
    "indices = jax.random.permutation(key, num_samples)\n",
    "\n",
    "# Split into two tensors\n",
    "X = embeddings[indices[:num_samples // 2]]\n",
    "Y = embeddings[indices[num_samples // 2:]]\n",
    "\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa4c1e",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e92a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ott_log_sinkhorn(grad,\n",
    "                     a,\n",
    "                     b,\n",
    "                     gamma_k,\n",
    "                     max_iter = 50,\n",
    "                     balanced = True,\n",
    "                     unbalanced = False,\n",
    "                     tau = None,\n",
    "                     tau2 = None):\n",
    "    \"\"\"\n",
    "    grad: cost matrix (n, m)\n",
    "    a: source histogram (n,)\n",
    "    b: target histogram (m,)\n",
    "    gamma_k: régularisation inverse (1/epsilon)\n",
    "    \"\"\"\n",
    "    epsilon = 1.0 / gamma_k\n",
    "\n",
    "    # Choix des tau pour marges\n",
    "    if balanced and not unbalanced:\n",
    "        tau_a, tau_b = 1.0, 1.0\n",
    "    elif not balanced and unbalanced:\n",
    "        tau_a = tau / (tau + epsilon)\n",
    "        tau_b = tau2 / (tau2 + epsilon) if tau2 is not None else tau_a\n",
    "    else:  # semi-relaxed\n",
    "        tau_a, tau_b = 1.0, tau / (tau + epsilon)\n",
    "\n",
    "    # Géométrie entropique sur la matrice de coût\n",
    "    geom = geometry.Geometry(cost_matrix=grad, epsilon=epsilon)\n",
    "\n",
    "    # Construction du problème linéaire\n",
    "    prob = linear_problem.LinearProblem(\n",
    "        geom,\n",
    "        a=a,\n",
    "        b=b,\n",
    "        tau_a=tau_a,\n",
    "        tau_b=tau_b\n",
    "    )\n",
    "\n",
    "    # Solveur Sinkhorn\n",
    "    solver = sinkhorn.Sinkhorn(max_iterations=max_iter)\n",
    "    out = solver(prob)\n",
    "\n",
    "    return out.matrix\n",
    "\n",
    "def utils__Delta(vark, varkm1, gamma_k):\n",
    "    return (gamma_k**-2) * (jnp.linalg.norm(vark[0] - varkm1[0]) + jnp.linalg.norm(vark[1] - varkm1[1]) + jnp.linalg.norm(vark[2] - varkm1[2]))\n",
    "\n",
    "def utils__random_simplex_sample(key, N, dtype = jnp.float64):\n",
    "    \"\"\"\n",
    "    Draws a random point from the (N-1)-simplex using normalized exponentiated Gaussian variates.\n",
    "\n",
    "    Args:\n",
    "        key: PRNGKey for random number generation.\n",
    "        N: Dimensionality of the simplex (vector length).\n",
    "        dtype: Desired floating-point type of the output.\n",
    "\n",
    "    Returns:\n",
    "        A 1D array of shape (N,) with non-negative entries summing to 1.\n",
    "    \"\"\"\n",
    "    # Sample N independent standard normals\n",
    "    z = jax.random.normal(key, shape=(N,), dtype=dtype)\n",
    "    # Exponentiate\n",
    "    e = jnp.exp(z)\n",
    "    # Normalize to sum to 1\n",
    "    return e / jnp.sum(e)\n",
    "\n",
    "def utils__initialize_couplings(a, b, gQ, gR, gamma, full_rank = True, key = jax.random.PRNGKey(0), dtype = float, rank2_random = False, max_iter = 50):\n",
    "    \"\"\"\n",
    "    Initialize coupling factors in JAX.\n",
    "    \"\"\"\n",
    "    N1 = a.shape[0]\n",
    "    N2 = b.shape[0]\n",
    "    r = gQ.shape[0]\n",
    "    r2 = gR.shape[0]\n",
    "\n",
    "    one_N1 = jnp.ones((N1,), dtype=dtype)\n",
    "    one_N2 = jnp.ones((N2,), dtype=dtype)\n",
    "\n",
    "    if full_rank:\n",
    "        # Full-rank initialization via log-Sinkhorn\n",
    "        key, subkey = jax.random.split(key)\n",
    "        C_random = jax.random.uniform(subkey, (N1, r), dtype=dtype)\n",
    "        Q = ott_log_sinkhorn(C_random, a, gQ, gamma,\n",
    "                                max_iter=max_iter,\n",
    "                                balanced=True)\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        C_random = jax.random.uniform(subkey, (N2, r2), dtype=dtype)\n",
    "        R = ott_log_sinkhorn(C_random, b, gR, gamma,\n",
    "                                max_iter=max_iter,\n",
    "                                balanced=True)\n",
    "\n",
    "        # Compute updated inner marginals\n",
    "        gR_new = R.T @ one_N2\n",
    "        gQ_new = Q.T @ one_N1\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        C_random = jax.random.uniform(subkey, (r, r2), dtype=dtype)\n",
    "        T = ott_log_sinkhorn(C_random, gQ_new, gR_new, gamma,\n",
    "                                max_iter=max_iter,\n",
    "                                balanced=True)\n",
    "\n",
    "        # Inner inverse coupling\n",
    "        if r == r2:\n",
    "            Lambda = jnp.linalg.inv(T)\n",
    "        else:\n",
    "            Lambda = jnp.diag(1.0 / gQ_new) @ T @ jnp.diag(1.0 / gR_new)\n",
    "\n",
    "    else:\n",
    "        # Rank-2 initialization (Scetbon et al. 2021)\n",
    "        if r != r2:\n",
    "            raise ValueError(\"Rank-2 init requires equal inner ranks.\")\n",
    "        g = gQ\n",
    "        lambd = jnp.minimum(jnp.min(a), jnp.min(b))\n",
    "        lambd = jnp.minimum(lambd, jnp.min(g)) / 2.0\n",
    "\n",
    "        # Sample or deterministic\n",
    "        if rank2_random:\n",
    "            key, *splits = random.split(key, 4)\n",
    "            a1 = utils__random_simplex_sample(N1, splits[0], dtype)\n",
    "            b1 = utils__random_simplex_sample(N2, splits[1], dtype)\n",
    "            g1 = utils__random_simplex_sample(r, splits[2], dtype)\n",
    "        else:\n",
    "            g1 = jnp.arange(1, r + 1, dtype=dtype)\n",
    "            g1 = g1 / jnp.sum(g1)\n",
    "            a1 = jnp.arange(1, N1 + 1, dtype=dtype)\n",
    "            a1 = a1 / jnp.sum(a1)\n",
    "            b1 = jnp.arange(1, N2 + 1, dtype=dtype)\n",
    "            b1 = b1 / jnp.sum(b1)\n",
    "\n",
    "        a2 = (a - lambd * a1) / (1 - lambd)\n",
    "        b2 = (b - lambd * b1) / (1 - lambd)\n",
    "        g2 = (g - lambd * g1) / (1 - lambd)\n",
    "\n",
    "        Q = lambd * jnp.outer(a1, g1) + (1 - lambd) * jnp.outer(a2, g2)\n",
    "        R = lambd * jnp.outer(b1, g1) + (1 - lambd) * jnp.outer(b2, g2)\n",
    "\n",
    "        gR_new = R.T @ one_N2\n",
    "        gQ_new = Q.T @ one_N1\n",
    "\n",
    "        T = (1 - lambd) * jnp.diag(g) + lambd * jnp.outer(gR_new, gQ_new)\n",
    "        Lambda = jnp.linalg.inv(T)\n",
    "\n",
    "    return Q, R, T, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905adddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd__compute_grad_A(C, Q, R, Lambda, gamma,\n",
    "                       semiRelaxedLeft, semiRelaxedRight,\n",
    "                       Wasserstein = True, FGW = False,\n",
    "                       A  = None,\n",
    "                       B = None,\n",
    "                       alpha = 0.0,\n",
    "                       unbalanced = False,\n",
    "                       full_grad = True):\n",
    "    \"\"\"\n",
    "    JAX version of gradient computation for Wasserstein, GW and FGW.\n",
    "    \"\"\"\n",
    "\n",
    "    r = Lambda.shape[0]\n",
    "    one_r = jnp.ones((r,))\n",
    "    One_rr = jnp.outer(one_r, one_r)\n",
    "\n",
    "    if Wasserstein:\n",
    "        gradQ, gradR = gd__Wasserstein_Grad(C, Q, R, Lambda, full_grad=full_grad)  # À adapter avec OTT-JAX\n",
    "    elif A is not None and B is not None:\n",
    "        if not semiRelaxedLeft and not semiRelaxedRight and not unbalanced:\n",
    "            gradQ = -4 * (A @ Q) @ Lambda @ (R.T @ B @ R) @ Lambda.T\n",
    "            gradR = -4 * (B @ R @ Lambda.T) @ (Q.T @ A @ Q) @ Lambda\n",
    "        elif semiRelaxedRight:\n",
    "            gradQ = -4 * (A @ Q) @ Lambda @ (R.T @ B @ R) @ Lambda.T\n",
    "            gradR = 2 * (B @ B) @ R @ One_rr - 4 * (B @ R @ Lambda.T) @ (Q.T @ A @ Q) @ Lambda\n",
    "        elif semiRelaxedLeft:\n",
    "            gradQ = 2 * (A @ A) @ Q @ One_rr - 4 * (A @ Q) @ Lambda @ (R.T @ B @ R) @ Lambda.T\n",
    "            gradR = -4 * (B @ R @ Lambda.T) @ (Q.T @ A @ Q) @ Lambda\n",
    "        elif unbalanced:\n",
    "            gradQ = 2 * (A @ A) @ Q @ One_rr - 4 * (A @ Q) @ Lambda @ (R.T @ B @ R) @ Lambda.T\n",
    "            gradR = 2 * (B @ B) @ R @ One_rr - 4 * (B @ R @ Lambda.T) @ (Q.T @ A @ Q) @ Lambda\n",
    "\n",
    "        if full_grad:\n",
    "            N1, N2 = Q.shape[0], R.shape[0]\n",
    "            one_N1 = jnp.ones((N1,))\n",
    "            one_N2 = jnp.ones((N2,))\n",
    "            gQ = Q.T @ one_N1\n",
    "            gR = R.T @ one_N2\n",
    "            F = Q @ Lambda @ R.T\n",
    "            MR = Lambda.T @ Q.T @ A @ F @ B @ R @ jnp.diag(1. / gR)\n",
    "            MQ = Lambda @ R.T @ B @ F.T @ A @ Q @ jnp.diag(1. / gQ)\n",
    "            gradQ += 4 * jnp.outer(one_N1, jnp.diag(MQ))\n",
    "            gradR += 4 * jnp.outer(one_N2, jnp.diag(MR))\n",
    "\n",
    "        if FGW:\n",
    "            gradQW, gradRW = gd__Wasserstein_Grad(C, Q, R, Lambda, full_grad=full_grad)  # À adapter\n",
    "            gradQ = (1 - alpha) * gradQW + alpha * gradQ\n",
    "            gradR = (1 - alpha) * gradRW + alpha * gradR\n",
    "    else:\n",
    "        raise ValueError(\"Provide either Wasserstein=True or distance matrices A and B for GW problem.\")\n",
    "\n",
    "    normalizer = jnp.max(jnp.array([jnp.max(jnp.abs(gradQ)), jnp.max(jnp.abs(gradR))]))\n",
    "    gamma_k = gamma / normalizer\n",
    "\n",
    "    return gradQ, gradR, gamma_k\n",
    "\n",
    "\n",
    "def gd__compute_grad_B(C, Q, R, Lambda, gQ, gR, gamma, Wasserstein=True,\n",
    "                       FGW=False, A=None, B=None, alpha=0.0):\n",
    "    '''\n",
    "    JAX version of the Wasserstein / GW / FGW gradient w.r.t. the transport plan T.\n",
    "    '''\n",
    "    if Wasserstein:\n",
    "        gradLambda = Q.T @ C @ R\n",
    "    else:\n",
    "        gradLambda = -4 * Q.T @ A @ Q @ Lambda @ R.T @ B @ R\n",
    "        if FGW:\n",
    "            gradLambda = (1 - alpha) * (Q.T @ C @ R) + alpha * gradLambda\n",
    "\n",
    "    gradT = jnp.diag(1.0 / gQ) @ gradLambda @ jnp.diag(1.0 / gR)\n",
    "    gamma_T = gamma / jnp.max(jnp.abs(gradT))\n",
    "    return gradT, gamma_T\n",
    "\n",
    "def gd__Wasserstein_Grad(C, Q, R, Lambda, full_grad=True):\n",
    "    gradQ = (C @ R) @ Lambda.T\n",
    "    if full_grad:\n",
    "        N1 = Q.shape[0]\n",
    "        one_N1 = jnp.ones((N1,), dtype=Q.dtype)\n",
    "        gQ = Q.T @ one_N1\n",
    "        w1 = jnp.diag((gradQ.T @ Q) @ jnp.diag(1.0 / gQ))\n",
    "        gradQ = gradQ - jnp.outer(one_N1, w1)\n",
    "\n",
    "    gradR = (C.T @ Q) @ Lambda\n",
    "    if full_grad:\n",
    "        N2 = R.shape[0]\n",
    "        one_N2 = jnp.ones((N2,), dtype=R.dtype)\n",
    "        gR = R.T @ one_N2\n",
    "        w2 = jnp.diag(jnp.diag(1.0 / gR) @ (R.T @ gradR))\n",
    "        gradR = gradR - jnp.outer(one_N2, w2)\n",
    "\n",
    "    return gradQ, gradR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FRLC_opt(C, a=None, b=None, A=None, B=None, tau_in=50, tau_out=50,\n",
    "             gamma=90, r=10, r2=None, max_iter=200,\n",
    "             semiRelaxedLeft=False, semiRelaxedRight=False, Wasserstein=True,\n",
    "             returnFull=False, FGW=False, alpha=0.0, unbalanced=False,\n",
    "             initialization='Full', init_args=None, full_grad=True,\n",
    "             convergence_criterion=True, tol=1e-5, min_iter=25,\n",
    "             min_iterGW=500, max_iterGW=1000,\n",
    "             max_inneriters_balanced=300, max_inneriters_relaxed=50,\n",
    "             diagonalize_return=False):\n",
    "\n",
    "    N1, N2 = C.shape\n",
    "    k = 0\n",
    "\n",
    "    one_N1 = jnp.ones((N1,))\n",
    "    one_N2 = jnp.ones((N2,))\n",
    "\n",
    "    if a is None:\n",
    "        a = one_N1 / N1\n",
    "    if b is None:\n",
    "        b = one_N2 / N2\n",
    "    if r2 is None:\n",
    "        r2 = r\n",
    "\n",
    "    one_r = jnp.ones((r,))\n",
    "    one_r2 = jnp.ones((r2,))\n",
    "\n",
    "    gQ = one_r / r\n",
    "    gR = one_r2 / r2\n",
    "\n",
    "    full_rank = initialization == 'Full' or initialization not in ['Full', 'Rank-2']\n",
    "\n",
    "    if init_args is None:\n",
    "        Q, R, T, Lambda = utils__initialize_couplings(a, b, gQ, gR,\n",
    "                                                      gamma, full_rank=full_rank,\n",
    "                                                      max_iter=max_inneriters_balanced)\n",
    "    else:\n",
    "        Q, R, T = init_args\n",
    "        Lambda = jnp.diag(1 / (Q.T @ one_N1)) @ T @ jnp.diag(1 / (R.T @ one_N2))\n",
    "\n",
    "    if not Wasserstein:\n",
    "        min_iter = min_iterGW\n",
    "        max_iter = max_iterGW\n",
    "\n",
    "    errs = []\n",
    "    gamma_k = gamma\n",
    "    Q_prev, R_prev, T_prev = None, None, None\n",
    "\n",
    "    def not_converged(k, Q, R, T, Q_prev, R_prev, T_prev, gamma_k):\n",
    "        if not convergence_criterion:\n",
    "            return True\n",
    "        if k < min_iter:\n",
    "            return True\n",
    "        delta = utils__Delta((Q, R, T), (Q_prev, R_prev, T_prev), gamma_k)\n",
    "        return delta > tol\n",
    "\n",
    "    while (k < max_iter and not_converged(k, Q, R, T, Q_prev, R_prev, T_prev, gamma_k)):\n",
    "        if convergence_criterion:\n",
    "            Q_prev, R_prev, T_prev = Q, R, T\n",
    "\n",
    "        if k % 25 == 0:\n",
    "            print(f'Iteration: {k}')\n",
    "\n",
    "        gradQ, gradR, gamma_k = gd__compute_grad_A(C, Q, R, Lambda, gamma,\n",
    "                                                   semiRelaxedLeft, semiRelaxedRight,\n",
    "                                                   Wasserstein=Wasserstein, A=A, B=B,\n",
    "                                                   FGW=FGW, alpha=alpha,\n",
    "                                                   unbalanced=unbalanced, full_grad=full_grad)\n",
    "\n",
    "        logQ = jnp.log(Q)\n",
    "        logR = jnp.log(R)\n",
    "\n",
    "        # Gestion explicite des modes de relaxation\n",
    "        if semiRelaxedLeft:\n",
    "            balanced_Q, unbalanced_Q = False, True\n",
    "            balanced_R, unbalanced_R = False, False\n",
    "        elif semiRelaxedRight:\n",
    "            balanced_Q, unbalanced_Q = False, False\n",
    "            balanced_R, unbalanced_R = False, True\n",
    "        elif unbalanced:\n",
    "            balanced_Q, unbalanced_Q = False, True\n",
    "            balanced_R, unbalanced_R = False, True\n",
    "        else:\n",
    "            balanced_Q, unbalanced_Q = True, False\n",
    "            balanced_R, unbalanced_R = True, False\n",
    "\n",
    "        Q = ott_log_sinkhorn(gradQ - (1 / gamma_k) * logQ, a, gQ, gamma_k,\n",
    "                             max_iter=max_inneriters_relaxed,\n",
    "                             balanced=balanced_Q, unbalanced=unbalanced_Q,\n",
    "                             tau=tau_out, tau2=tau_in)\n",
    "\n",
    "        R = ott_log_sinkhorn(gradR - (1 / gamma_k) * logR, b, gR, gamma_k,\n",
    "                             max_iter=max_inneriters_relaxed,\n",
    "                             balanced=balanced_R, unbalanced=unbalanced_R,\n",
    "                             tau=tau_out, tau2=tau_in)\n",
    "\n",
    "        gQ = Q.T @ one_N1\n",
    "        gR = R.T @ one_N2\n",
    "\n",
    "        gradT, gamma_T = gd__compute_grad_B(C, Q, R, Lambda, gQ, gR,\n",
    "                                            gamma, Wasserstein=Wasserstein,\n",
    "                                            A=A, B=B, FGW=FGW, alpha=alpha)\n",
    "\n",
    "        T = ott_log_sinkhorn(gradT - (1 / gamma_T) * jnp.log(T), gQ, gR, gamma_T,\n",
    "                             max_iter=max_inneriters_balanced,\n",
    "                             balanced=True, unbalanced=False)\n",
    "\n",
    "        Lambda = jnp.diag(1 / gQ) @ T @ jnp.diag(1 / gR)\n",
    "        k += 1\n",
    "\n",
    "    if returnFull:\n",
    "        P = Q @ Lambda @ R.T\n",
    "        return P, errs\n",
    "    else:\n",
    "        if diagonalize_return:\n",
    "            Q = Q @ jnp.diag(1 / gQ) @ T\n",
    "            gR = R.T @ one_N2\n",
    "            T = jnp.diag(gR)\n",
    "        return Q, R, T, errs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def FRLC_compute_OT_cost(X, Y, C = None, Monge_clusters = None, sq_Euclidean = True):\n",
    "    \"\"\"\n",
    "    Compute the optimal transport cost in linear space and time (without coupling), in JAX.\n",
    "    Supports squared Euclidean cost via OTT cost object.\n",
    "    \"\"\"\n",
    "    if Monge_clusters is None or len(Monge_clusters) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    def compute_pair_cost(pair):\n",
    "        idx1, idx2 = pair\n",
    "        if C is not None:\n",
    "            return C[idx1, idx2]\n",
    "        else:\n",
    "            diff = X[idx1] - Y[idx2]\n",
    "            if sq_Euclidean:\n",
    "                return jnp.sum(diff**2)\n",
    "            else:\n",
    "                return jnp.linalg.norm(diff)\n",
    "\n",
    "    pair_costs = jax.vmap(compute_pair_cost)(jnp.array(Monge_clusters))\n",
    "    total_cost = jnp.sum(pair_costs)\n",
    "    return total_cost / len(Monge_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 25\n",
      "Iteration: 50\n",
      "Iteration: 75\n"
     ]
    }
   ],
   "source": [
    "# 1. Calcul de la matrice des coûts (distance euclidienne)\n",
    "def cdist_jax(X, Y):\n",
    "    # ||x - y||^2 = ||x||^2 + ||y||^2 - 2<x, y>\n",
    "    X_norm = jnp.sum(X ** 2, axis=1)[:, None]\n",
    "    Y_norm = jnp.sum(Y ** 2, axis=1)[None, :]\n",
    "    C = jnp.sqrt(jnp.maximum(X_norm + Y_norm - 2 * jnp.dot(X, Y.T), 0.0))\n",
    "    return C\n",
    "\n",
    "C = cdist_jax(X, Y)\n",
    "\n",
    "try:\n",
    "    # 2. Appel à FRLC_opt (on passe dtype=jnp.float32)\n",
    "    Q, R, T, errs = FRLC_opt(\n",
    "        C=C,\n",
    "        gamma=30,\n",
    "        r=40,\n",
    "        max_iter=100,\n",
    "        tau_in=100000\n",
    "    )\n",
    "\n",
    "    # 3. Calcul de la matrice de couplage P complète\n",
    "    inv_sum_Q = 1.0 / jnp.sum(Q, axis=0)  # shape (r,)\n",
    "    inv_sum_R = 1.0 / jnp.sum(R, axis=0)  # shape (r,)\n",
    "    P = (Q\n",
    "         @ jnp.diag(inv_sum_Q)\n",
    "         @ T\n",
    "         @ jnp.diag(inv_sum_R)\n",
    "         @ R.T)  # shape (n_X, n_Y)\n",
    "\n",
    "    # 4. Extraction des paires (i,j) où P[i,j] > 0\n",
    "    ij = jnp.argwhere(P > 0)  # shape (num_pairs, 2)\n",
    "    Monge_clusters = [(int(i), int(j)) for i, j in ij]\n",
    "\n",
    "    # 5. Calcul du coût OT via la fonction JAXisée\n",
    "    cost_frlc = FRLC_compute_OT_cost(\n",
    "        X, Y,\n",
    "        C=C,\n",
    "        Monge_clusters=Monge_clusters,\n",
    "        sq_Euclidean=True\n",
    "    )\n",
    "\n",
    "    print(f'FRLC cost: {cost_frlc}')\n",
    "\n",
    "    # 6. Approximation du couplage pour l'extraction de correspondances\n",
    "    P_approx = Q @ T @ R.T  # shape (n_X, n_Y)\n",
    "    matches_Y = jnp.argmax(P_approx, axis=1)  # shape (n_X,)\n",
    "\n",
    "    # 7. Construction de la liste F de paires indices\n",
    "    F = [\n",
    "        (jnp.array([i], dtype=jnp.int32),\n",
    "         jnp.array([int(matches_Y[i])], dtype=jnp.int32))\n",
    "        for i in range(X.shape[0])\n",
    "    ]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'FRLC failed for sample size {X.shape[0]}: {e}')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1158f",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4f4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------\n",
    "Code for gradients assuming low-rank distance matrices C, A, B\n",
    "--------------\n",
    "'''\n",
    "\n",
    "def gd__compute_grad_A_LR(C_factors, A_factors, B_factors, Q, R, Lambda, gamma, alpha=0.0, full_grad=False):\n",
    "    \n",
    "    N1, N2 = C_factors[0].shape[0], C_factors[1].shape[1]\n",
    "\n",
    "    if A_factors is not None and B_factors is not None:\n",
    "        A1, A2 = A_factors\n",
    "        B1, B2 = B_factors\n",
    "\n",
    "        # GW gradients\n",
    "        gradQ = -4 * (A1 @ (A2 @ (Q @ Lambda @ ((R.T @ B1) @ (B2 @ R)) @ Lambda.T)))\n",
    "        gradR = -4 * (B1 @ (B2 @ (R @ (Lambda.T @ ((Q.T @ A1) @ (A2 @ Q)) @ Lambda))))\n",
    "\n",
    "        one_N1 = jnp.ones((N1,), dtype=Q.dtype)\n",
    "        one_N2 = jnp.ones((N2,), dtype=R.dtype)\n",
    "\n",
    "        if full_grad:\n",
    "            gQ = Q.T @ one_N1\n",
    "            gR = R.T @ one_N2\n",
    "\n",
    "            MR = (Lambda.T @ ((Q.T @ A1) @ (A2 @ Q)) @ Lambda\n",
    "                  @ ((R.T @ B1) @ (B2 @ R)) @ jnp.diag(1.0 / gR))\n",
    "            MQ = (Lambda @ ((R.T @ B1) @ (B2 @ R)) @ Lambda.T\n",
    "                  @ ((Q.T @ A1) @ (A2 @ Q)) @ jnp.diag(1.0 / gQ))\n",
    "\n",
    "            gradQ += 4 * jnp.outer(one_N1, jnp.diag(MQ))\n",
    "            gradR += 4 * jnp.outer(one_N2, jnp.diag(MR))\n",
    "    else:\n",
    "        gradQ = jnp.zeros_like(Q)\n",
    "        gradR = jnp.zeros_like(R)\n",
    "\n",
    "    # Appel à une version jaxifiée de gd__Wasserstein_Grad_LR\n",
    "    gradQW, gradRW = gd__Wasserstein_Grad_LR(C_factors, Q, R, Lambda, full_grad=full_grad)\n",
    "\n",
    "    gradQ = (1 - alpha) * gradQW + (alpha / 2.0) * gradQ\n",
    "    gradR = (1 - alpha) * gradRW + (alpha / 2.0) * gradR\n",
    "\n",
    "    normalizer = jnp.maximum(jnp.max(jnp.abs(gradQ)), jnp.max(jnp.abs(gradR)))\n",
    "    gamma_k = gamma / normalizer\n",
    "\n",
    "    return gradQ, gradR, gamma_k\n",
    "\n",
    "def gd__compute_grad_B_LR(C_factors, A_factors, B_factors, Q, R, Lambda, gQ, gR, gamma, alpha=0.0):\n",
    "    \"\"\"\n",
    "    Low-rank gradient computation in JAX for Wasserstein / Gromov-Wasserstein.\n",
    "    \"\"\"\n",
    "    C1, C2 = C_factors  # (N1, rC), (rC, N2)\n",
    "    gradLambda = 0.0\n",
    "\n",
    "    if A_factors is not None and B_factors is not None:\n",
    "        A1, A2 = A_factors  # (N1, rA), (rA, N1)\n",
    "        B1, B2 = B_factors  # (N2, rB), (rB, N2)\n",
    "        term_A = (Q.T @ A1) @ (A2 @ Q)         # shape: (r, r)\n",
    "        term_B = (R.T @ B1) @ (B2 @ R)         # shape: (r, r)\n",
    "        gradLambda = -4.0 * term_A @ Lambda @ term_B\n",
    "\n",
    "    term_C = (Q.T @ C1) @ (C2 @ R)             # shape: (r, r)\n",
    "    gradLambda = (1 - alpha) * term_C + (alpha / 2.0) * gradLambda\n",
    "\n",
    "    gradT = jnp.diag(1.0 / gQ) @ gradLambda @ jnp.diag(1.0 / gR)\n",
    "    gamma_T = gamma / jnp.max(jnp.abs(gradT))\n",
    "    return gradT, gamma_T\n",
    "\n",
    "def gd__Wasserstein_Grad_LR(C_factors, Q, R, Lambda, full_grad=True):\n",
    "    \"\"\"\n",
    "    JAX version of Wasserstein gradient with low-rank cost approximation:\n",
    "    C ≈ C1 @ C2.T\n",
    "    \"\"\"\n",
    "    C1, C2 = C_factors\n",
    "\n",
    "    gradQ = C1 @ ((C2 @ R) @ Lambda.T)\n",
    "    if full_grad:\n",
    "        N1 = Q.shape[0]\n",
    "        one_N1 = jnp.ones((N1,), dtype=Q.dtype)\n",
    "        gQ = Q.T @ one_N1\n",
    "        w1 = jnp.diag((gradQ.T @ Q) @ jnp.diag(1.0 / gQ))\n",
    "        gradQ = gradQ - jnp.outer(one_N1, w1)\n",
    "\n",
    "    gradR = C2.T @ ((C1.T @ Q) @ Lambda)\n",
    "    if full_grad:\n",
    "        N2 = R.shape[0]\n",
    "        one_N2 = jnp.ones((N2,), dtype=R.dtype)\n",
    "        gR = R.T @ one_N2\n",
    "        w2 = jnp.diag(jnp.diag(1.0 / gR) @ (R.T @ gradR))\n",
    "        gradR = gradR - jnp.outer(one_N2, w2)\n",
    "\n",
    "    return gradQ, gradR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe4033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_annealing__factors(n):\n",
    "    \"\"\"\n",
    "    Return list of all factors of an integer\n",
    "    \"\"\"\n",
    "    n = int(n)  # Conversion pour compatibilité avec jnp.arange\n",
    "    candidates = jnp.arange(1, jnp.floor(jnp.sqrt(n)) + 1).astype(int)\n",
    "    divisible = (n % candidates) == 0\n",
    "    factors1 = candidates[divisible]\n",
    "    factors2 = n // factors1\n",
    "    all_factors = jnp.concatenate([factors1, factors2])\n",
    "    unique_factors = jnp.unique(all_factors)\n",
    "    return unique_factors\n",
    "\n",
    "def rank_annealing__max_factor_lX(n, max_X):\n",
    "    \"\"\"\n",
    "    Find max factor of n , such that max_factor \\leq max_X\n",
    "    \"\"\"\n",
    "    factor_lst = rank_annealing__factors(n)\n",
    "    factors_leq_max = factor_lst[factor_lst <= max_X]\n",
    "    return jnp.max(factors_leq_max)\n",
    "\n",
    "def rank_annealing__min_sum_partial_products_with_factors(n, k, C):\n",
    "    \"\"\"\n",
    "    Dynamic program to compute the rank-schedule, subject to a constraint of intermediates being \\leq C\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        The dataset size to be factored into a rank-scheduler. Assumed to be non-prime.\n",
    "    k: int\n",
    "        The depth of the hierarchy.\n",
    "    C: int\n",
    "        A constraint on the maximal intermediate rank across the hierarchy.\n",
    "    \n",
    "    \"\"\"\n",
    "    INF = 1e10  # Large constant instead of float('inf') for JAX compatibility\n",
    "\n",
    "    dp = jnp.full((n+1, k+1), INF)\n",
    "    choice = jnp.full((n+1, k+1), -1)\n",
    "\n",
    "    def init_base_case(dp, choice):\n",
    "        d = jnp.arange(1, n+1)\n",
    "        mask = d <= C\n",
    "        dp = dp.at[d[mask], 1].set(d[mask])\n",
    "        choice = choice.at[d[mask], 1].set(d[mask])\n",
    "        return dp, choice\n",
    "\n",
    "    dp, choice = init_base_case(dp, choice)\n",
    "\n",
    "    for t in range(2, k+1):\n",
    "        for d in range(1, n+1):\n",
    "            if dp[d, t-1] >= INF:\n",
    "                continue\n",
    "            for r in range(1, min(C, d)+1):\n",
    "                if d % r == 0:\n",
    "                    candidate = r + r * dp[d // r, t-1]\n",
    "                    if candidate < dp[d, t]:\n",
    "                        dp = dp.at[d, t].set(candidate)\n",
    "                        choice = choice.at[d, t].set(r)\n",
    "\n",
    "    if dp[n, k] >= INF:\n",
    "        return None, []\n",
    "\n",
    "    # Backtracking\n",
    "    factors = []\n",
    "    d_cur, t_cur = n, k\n",
    "    while t_cur > 0:\n",
    "        r_cur = int(choice[d_cur, t_cur])\n",
    "        factors.append(r_cur)\n",
    "        d_cur //= r_cur\n",
    "        t_cur -= 1\n",
    "\n",
    "    return dp[n, k], factors\n",
    "\n",
    "def rank_annealing__optimal_rank_schedule(n, hierarchy_depth=6, max_Q=int(2**10), max_rank=16):\n",
    "    \"\"\"\n",
    "    A function to compute the optimal rank-scheduler of refinement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        Size of the input dataset -- cannot be a prime number\n",
    "    hierarchy_depth: int\n",
    "        Maximal permissible depth of the multi-scale hierarchy\n",
    "    max_Q: int\n",
    "        Maximal rank at terminal base case (before reducing the \\leq max_Q rank coupling to a 1-1 alignment)\n",
    "    max_rank: int\n",
    "        Maximal rank at the intermediate steps of the rank-schedule\n",
    "        \n",
    "    \"\"\"\n",
    "    Q = int(rank_annealing__max_factor_lX(n, max_Q))\n",
    "    ndivQ = int(n // Q)\n",
    "\n",
    "    _, rank_schedule = rank_annealing__min_sum_partial_products_with_factors(ndivQ, hierarchy_depth, max_rank)\n",
    "    rank_schedule = sorted(rank_schedule)\n",
    "    rank_schedule.append(Q)\n",
    "    rank_schedule = [x for x in rank_schedule if x != 1]\n",
    "\n",
    "    print(f'Optimized rank-annealing schedule: {rank_schedule}')\n",
    "\n",
    "    assert functools.reduce(operator.mul, rank_schedule, 1) == n, \"Error! Rank-schedule does not factorize n!\"\n",
    "\n",
    "    return rank_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9fe3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils__low_rank_distance_factorization(X1, X2, r, eps, device='cpu', dtype=jnp.float64):\n",
    "    n = X1.shape[0]\n",
    "    m = X2.shape[0]\n",
    "    t = int(r / eps)  # La taille d'échantillon t\n",
    "\n",
    "    # Conversion en tableau JAX\n",
    "    X1_jax = jnp.array(X1)\n",
    "    X2_jax = jnp.array(X2)\n",
    "\n",
    "    # Étape 1 : Calcul des distances (matrice de coût)\n",
    "    cost_matrix = jnp.sum((X1_jax[:, None, :] - X2_jax[None, :, :]) ** 2, axis=-1)\n",
    "\n",
    "    # Étape 2 : Résolution du problème de transport optimal avec Sinkhorn\n",
    "    reg_param = 0.1  # Paramètre de régularisation, ajustez si nécessaire\n",
    "    ot_solution = sinkhorn(cost_matrix, reg_param, niter=1000)\n",
    "\n",
    "    # Étape 3 : Calcul des probabilités d'échantillonnage pour X1\n",
    "    # Pondération selon la solution du transport optimal\n",
    "    p = jnp.sum(ot_solution, axis=1)**2  # Probabilités pour X1\n",
    "    p_dist = p / jnp.sum(p)\n",
    "\n",
    "    # Échantillonnage des indices pour X1\n",
    "    indices_p = np.random.choice(n, size=t, p=p_dist)\n",
    "    X1_t = X1_jax[indices_p]\n",
    "\n",
    "    # Étape 4 : Calcul de P_t (ponctions d'échantillonnage)\n",
    "    P_t = jnp.sqrt(p[indices_p] * t)\n",
    "\n",
    "    # Calcul de la matrice S\n",
    "    S = jax.scipy.spatial.distance.cdist(X1_t, X2_jax) / P_t[:, None]  # t x m\n",
    "\n",
    "    # Étape 5 : Calcul des probabilités d'échantillonnage pour X2\n",
    "    q = jnp.norm(S, axis=0)**2 / jnp.norm(S)**2  # Probabilités pour X2\n",
    "    q_dist = q / jnp.sum(q)\n",
    "\n",
    "    # Échantillonnage des indices pour X2\n",
    "    indices_q = np.random.choice(m, size=t, p=q_dist)\n",
    "    S_t = S[:, indices_q]  # t x t\n",
    "\n",
    "    # Calcul de Q_t\n",
    "    Q_t = jnp.sqrt(q[indices_q] * t)\n",
    "\n",
    "    # Calcul de la matrice W\n",
    "    W = S_t / Q_t[None, :]\n",
    "\n",
    "    # Étape 6 : Décomposition SVD de W\n",
    "    U, Sig, Vh = jax.linalg.svd(W)\n",
    "\n",
    "    # Filtrage des premiers r vecteurs singuliers\n",
    "    F = U[:, :r]\n",
    "\n",
    "    # Calcul de U_t pour le retour\n",
    "    U_t = jnp.dot(S.T, F) / jnp.linalg.norm(jnp.dot(W.T, F))\n",
    "\n",
    "    # Étape 7 : Utilisation de U_t pour calculer V\n",
    "    indices = np.random.choice(m, size=t)\n",
    "    X2_t = X2_jax[indices]\n",
    "\n",
    "    D_t = jax.scipy.spatial.distance.cdist(X1_jax, X2_t) / jnp.sqrt(t)\n",
    "    Q = jnp.dot(U_t.T, U_t)  # Matrice carrée r x r\n",
    "    U, Sig, Vh = jax.linalg.svd(Q)\n",
    "    U = U / Sig  # Normalisation\n",
    "\n",
    "    U_tSub = U_t[indices, :].T  # t x r\n",
    "    B = jnp.dot(U.T, U_tSub) / jnp.sqrt(t)\n",
    "\n",
    "    # Calcul de A et de Z\n",
    "    A = jnp.linalg.inv(jnp.dot(B, B.T))\n",
    "    Z = jnp.dot(jnp.dot(A, B), D_t.T)  # r x t * t x n\n",
    "\n",
    "    # Calcul de V\n",
    "    V = jnp.dot(Z.T, U)\n",
    "\n",
    "    return V, U_t.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b79b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils__hadamard_square_lr(A1, A2):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        A1: jax.numpy array, low-rank subcoupling of shape (n, r)\n",
    "        A2: jax.numpy array, low-rank subcoupling of shape (n, r)\n",
    "                ( such that A ≈ A1 @ A2.T )\n",
    "\n",
    "    Output\n",
    "        A1_tilde: jax.numpy array, low-rank subcoupling of shape (n, r**2)\n",
    "        A2_tilde: jax.numpy array, low-rank subcoupling of shape (n, r**2)\n",
    "               ( such that A * A ≈ A1_tilde @ A2_tilde.T )\n",
    "    \"\"\"\n",
    "    n, r = A1.shape\n",
    "    A1_tilde = jnp.einsum(\"ij,ik->ijk\", A1, A1).reshape(n, r * r)\n",
    "    A2_tilde = jnp.einsum(\"ij,ik->ijk\", A2, A2).reshape(n, r * r)\n",
    "\n",
    "    return A1_tilde, A2_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6235124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FRLC_LR_opt(C, a, b, r, tau, gamma, delta, epsilon, max_iter=100):\n",
    "    # Initialisation identique (utilise utils__initialize_couplings défini ailleurs)\n",
    "    Q, R, gQ, gR = utils__initialize_couplings(a, b, r)\n",
    "    for it in range(max_iter):\n",
    "        # Calcul du couplage latent A = Q * C * R^T (en JAX on peut utiliser l’opérateur @)\n",
    "        A = Q @ (C @ R.T)  # R.T transposé implicite JAX\n",
    "        # Évitement des zéros dans le log (on remplace les ≤0 par inf avant log)\n",
    "        A = jnp.log(jnp.where(A <= 0, jnp.inf, A))\n",
    "        # Sinkhorn logarithmique pour mettre à jour T en utilisant marges gQ, gR\n",
    "        T = ott_log_sinkhorn(A, gQ, gR, tau=tau, delta=delta)\n",
    "        # Mise à jour des marges depuis le couplage T\n",
    "        gQ = jnp.sum(T, axis=1)\n",
    "        gR = jnp.sum(T, axis=0)\n",
    "        # Calcul des gradients pour Q et R (fonction externe inchangée)\n",
    "        grad_Q, grad_R = gd__compute_grad_A_LR(A, C, gQ, gR)\n",
    "        # Pas de gradient descent (mirrored descent, etc.) – logique identique\n",
    "        Q = Q - gamma * grad_Q\n",
    "        R = R - gamma * grad_R\n",
    "        # Projection ou normalisation éventuelle (exemple : normaliser selon les marges)\n",
    "        # (On montre ci-dessous des exemples génériques de normalisation en JAX)\n",
    "        Q = (Q.T * (1.0 / gQ)).T   # normaliser chaque ligne de Q à gQ\n",
    "        R = (R * (1.0 / gR)).T    # normaliser chaque colonne de R à gR\n",
    "        # Critère d’arrêt éventuellement utilisant la trace\n",
    "        cost = jnp.trace(Q @ (C @ R.T))\n",
    "        if cost < epsilon:\n",
    "            break\n",
    "    return Q, R, gQ, gR, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4e86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalRefinementOT:\n",
    "    \"\"\"\n",
    "    Classe pour effectuer le raffinage hiérarchique OT en JAX.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 C: jnp.ndarray,\n",
    "                 rank_schedule: List[int],\n",
    "                 solver: Callable = FRLC_opt,\n",
    "                 solver_params: Union[Dict[str, Any], None] = None,\n",
    "                 device: str = 'cpu',\n",
    "                 base_rank: int = 1,\n",
    "                 clustering_type: str = 'soft',\n",
    "                 plot_clusterings: bool = False,\n",
    "                 parallel: bool = False,\n",
    "                 num_processes: Union[int, None] = None):\n",
    "        # Matrice de coût et paramètres initiaux\n",
    "        self.C = jnp.array(C)                     # Coût en tant que jnp.array\n",
    "        self.rank_schedule = rank_schedule\n",
    "        self.solver = solver\n",
    "        self.device = device\n",
    "        self.base_rank = base_rank\n",
    "        self.clustering_type = clustering_type\n",
    "        self.plot_clusterings = plot_clusterings\n",
    "        self.parallel = parallel\n",
    "        self.num_processes = num_processes\n",
    "\n",
    "        self.X, self.Y = None, None\n",
    "        self.N = self.C.shape[0]\n",
    "        self.Monge_clusters = None\n",
    "\n",
    "        # Paramètres par défaut du solveur FRLC\n",
    "        default_solver_params = {\n",
    "            'gamma': 30,\n",
    "            'max_iter': 60,\n",
    "            'min_iter': 25,\n",
    "            'max_inneriters_balanced': 100,\n",
    "            'max_inneriters_relaxed': 40,\n",
    "            'printCost': False,\n",
    "            'tau_in': 100000\n",
    "        }\n",
    "        if solver_params is not None:\n",
    "            default_solver_params.update(solver_params)\n",
    "        self.solver_params = default_solver_params\n",
    "\n",
    "        assert self.C.shape[0] == self.C.shape[1], \\\n",
    "            \"La matrice de coût doit être carrée (|X| = |Y| = N)\"\n",
    "\n",
    "    @classmethod\n",
    "    def init_from_point_clouds(cls,\n",
    "                               X: jnp.ndarray,\n",
    "                               Y: jnp.ndarray,\n",
    "                               rank_schedule: List[int],\n",
    "                               distance_rank_schedule: Union[List[int], None] = None,\n",
    "                               solver: Callable = FRLC_LR_opt,\n",
    "                               solver_params: Union[Dict[str, Any], None] = None,\n",
    "                               device: str = 'cpu',\n",
    "                               base_rank: int = 1,\n",
    "                               clustering_type: str = 'soft',\n",
    "                               plot_clusterings: bool = False,\n",
    "                               parallel: bool = False,\n",
    "                               num_processes: Union[int, None] = None,\n",
    "                               sq_Euclidean: bool = False):\n",
    "        \"\"\"\n",
    "        Constructeur à partir de nuages de points X, Y.\n",
    "        \"\"\"\n",
    "        obj = cls.__new__(cls)\n",
    "        obj.X = jnp.array(X)\n",
    "        obj.Y = jnp.array(Y)\n",
    "        obj.rank_schedule = rank_schedule\n",
    "        obj.distance_rank_schedule = rank_schedule if distance_rank_schedule is None else distance_rank_schedule\n",
    "        obj.solver = solver\n",
    "        obj.device = device\n",
    "        obj.base_rank = base_rank\n",
    "        obj.clustering_type = clustering_type\n",
    "        obj.plot_clusterings = plot_clusterings\n",
    "        obj.parallel = parallel\n",
    "        obj.num_processes = num_processes\n",
    "        obj.N = X.shape[0]\n",
    "        obj.C = None\n",
    "        obj.Monge_clusters = None\n",
    "        obj.sq_Euclidean = sq_Euclidean\n",
    "\n",
    "        default_solver_params = {\n",
    "            'gamma': 30,\n",
    "            'max_iter': 60,\n",
    "            'min_iter': 25,\n",
    "            'max_inneriters_balanced': 100,\n",
    "            'max_inneriters_relaxed': 40,\n",
    "            'printCost': False,\n",
    "            'tau_in': 100000\n",
    "        }\n",
    "        if solver_params is not None:\n",
    "            default_solver_params.update(solver_params)\n",
    "        obj.solver_params = default_solver_params\n",
    "\n",
    "        assert X.shape[0] == Y.shape[0], \"Assume |X| = |Y| = N\"\n",
    "        return obj\n",
    "\n",
    "    def run(self, return_as_coupling: bool = False):\n",
    "        \"\"\"\n",
    "        Lance le raffinage hiérarchique (série ou parallèle).\n",
    "        \"\"\"\n",
    "        if self.parallel:\n",
    "            return self._hierarchical_refinement_parallelized(return_as_coupling=return_as_coupling)\n",
    "        else:\n",
    "            return self._hierarchical_refinement(return_as_coupling=return_as_coupling)\n",
    "\n",
    "    def _hierarchical_refinement(self, return_as_coupling: bool = False):\n",
    "        \"\"\"\n",
    "        Raffinement hiérarchique mono-processus.\n",
    "        \"\"\"\n",
    "        # Partition initiale (tous les points dans un seul cluster)\n",
    "        F_t = [(jnp.arange(self.N), jnp.arange(self.N))]\n",
    "        for i, rank_level in enumerate(self.rank_schedule):\n",
    "            F_tp1 = []\n",
    "            is_last = (i == len(self.rank_schedule) - 1)\n",
    "            if is_last:\n",
    "                fin_iters = int(self.N) // int(jnp.prod(jnp.array(self.rank_schedule[:i+1])))\n",
    "                print(f'Last level, chunk-size {rank_level}, {fin_iters} itérations.')\n",
    "\n",
    "            j = 0\n",
    "            for (idxX, idxY) in F_t:\n",
    "                if is_last:\n",
    "                    print(f'{j}/{fin_iters} itérations finalistes')\n",
    "                    j += 1\n",
    "\n",
    "                # Si cluster arrivé à la taille base_rank, on le conserve tel quel\n",
    "                if len(idxX) <= self.base_rank or len(idxY) <= self.base_rank:\n",
    "                    F_tp1.append((idxX, idxY))\n",
    "                    continue\n",
    "\n",
    "                # Résoudre sous-problème d'OT (coût explicite ou low-rank sur distance)\n",
    "                if self.C is not None:\n",
    "                    Q, R = self._solve_prob(idxX, idxY, rank_level)\n",
    "                else:\n",
    "                    rank_D = self.distance_rank_schedule[i]\n",
    "                    Q, R = self._solve_LR_prob(idxX, idxY, rank_level, rank_D)\n",
    "\n",
    "                # Calcul de la nouvelle taille de cluster (capacités)\n",
    "                capacity = int(self.N) // int(jnp.prod(jnp.array(self.rank_schedule[:i+1])))\n",
    "                idx_seenX = jnp.arange(Q.shape[0])\n",
    "                idx_seenY = jnp.arange(R.shape[0])\n",
    "\n",
    "                # Clustering \"soft\" ou \"hard\"\n",
    "                if self.clustering_type == 'soft':\n",
    "                    for z in range(rank_level):\n",
    "                        # top-k sur les colonnes de Q et R\n",
    "                        _, topk_X = lax.top_k(Q[idx_seenX, z], capacity)\n",
    "                        idxX_z = idxX[idx_seenX[topk_X]]\n",
    "                        _, topk_Y = lax.top_k(R[idx_seenY, z], capacity)\n",
    "                        idxY_z = idxY[idx_seenY[topk_Y]]\n",
    "                        F_tp1.append((idxX_z, idxY_z))\n",
    "                        # Retirer ces indices pour la suite\n",
    "                        idx_seenX = idx_seenX[~jnp.isin(idx_seenX, idx_seenX[topk_X])]\n",
    "                        idx_seenY = idx_seenY[~jnp.isin(idx_seenY, idx_seenY[topk_Y])]\n",
    "\n",
    "                elif self.clustering_type == 'hard':\n",
    "                    # Assignations par argmax\n",
    "                    zX = jnp.argmax(Q, axis=1)\n",
    "                    zY = jnp.argmax(R, axis=1)\n",
    "                    for z in range(rank_level):\n",
    "                        idxX_z = idxX[zX == z]\n",
    "                        idxY_z = idxY[zY == z]\n",
    "                        # On s’assure que c’est effectivement un hard-clustering\n",
    "                        assert len(idxX_z) == len(idxY_z) == capacity, \\\n",
    "                            \"Cluster déséquilibre ou pas du hard-clustering!\"\n",
    "                        F_tp1.append((idxX_z, idxY_z))\n",
    "\n",
    "            F_t = F_tp1\n",
    "\n",
    "        self.Monge_clusters = F_t\n",
    "        if return_as_coupling:\n",
    "            return self._compute_coupling_from_Ft()\n",
    "        else:\n",
    "            return self.Monge_clusters\n",
    "\n",
    "    def _hierarchical_refinement_parallelized(self, return_as_coupling: bool = False):\n",
    "        # Non implémenté dans cette classe\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _solve_LR_prob(self, idxX, idxY, rank_level, rankD, eps=0.04):\n",
    "        \"\"\"\n",
    "        Solve sub-problème OT en utilisant la factorisation low-rank de la distance.\n",
    "        \"\"\"\n",
    "        _x0 = self.X[idxX]    # Extrait les points X\n",
    "        _x1 = self.Y[idxY]    # Extrait les points Y\n",
    "        if rankD < _x0.shape[0]:\n",
    "            # Calcul des facteurs de la matrice de distance\n",
    "            C_factors, A_factors, B_factors = self.get_dist_mats(_x0, _x1, rankD, eps, self.sq_Euclidean)\n",
    "            Q, R, _, _ = self.solver(\n",
    "                C_factors, A_factors, B_factors,\n",
    "                gamma=self.solver_params['gamma'],\n",
    "                r=rank_level,\n",
    "                max_iter=self.solver_params['max_iter'],\n",
    "                device=self.device,\n",
    "                min_iter=self.solver_params['min_iter'],\n",
    "                max_inneriters_balanced=self.solver_params['max_inneriters_balanced'],\n",
    "                max_inneriters_relaxed=self.solver_params['max_inneriters_relaxed'],\n",
    "                diagonalize_return=True,\n",
    "                printCost=False,\n",
    "                tau_in=self.solver_params['tau_in'],\n",
    "                dtype=_x0.dtype\n",
    "            )\n",
    "        else:\n",
    "            # Cas final (sous-cluster) : calcul direct des distances\n",
    "            if self.sq_Euclidean:\n",
    "                # Distance euclidienne au carré\n",
    "                C_XY = jnp.linalg.norm(_x0[:, None, :] - _x1[None, :, :], axis=2)**2\n",
    "            else:\n",
    "                # Distance euclidienne normale\n",
    "                C_XY = jnp.linalg.norm(_x0[:, None, :] - _x1[None, :, :], axis=2)\n",
    "            Q, R, _, _ = FRLC_opt(\n",
    "                C_XY,\n",
    "                gamma=self.solver_params['gamma'],\n",
    "                r=rank_level,\n",
    "                max_iter=self.solver_params['max_iter'],\n",
    "                device=self.device,\n",
    "                min_iter=self.solver_params['min_iter'],\n",
    "                max_inneriters_balanced=self.solver_params['max_inneriters_balanced'],\n",
    "                max_inneriters_relaxed=self.solver_params['max_inneriters_relaxed'],\n",
    "                diagonalize_return=True,\n",
    "                printCost=False,\n",
    "                tau_in=self.solver_params['tau_in'],\n",
    "                dtype=C_XY.dtype\n",
    "            )\n",
    "        return Q, R\n",
    "\n",
    "    def _solve_prob(self, idxX, idxY, rank_level):\n",
    "        \"\"\"\n",
    "        Solve sous-problème OT en indexant la sous-matrice de coût explicite.\n",
    "        \"\"\"\n",
    "        C_sub = self.C[idxX][:, idxY]\n",
    "        Q, R, _, _ = self.solver(\n",
    "            C_sub,\n",
    "            gamma=self.solver_params['gamma'],\n",
    "            r=rank_level,\n",
    "            max_iter=self.solver_params['max_iter'],\n",
    "            device=self.device,\n",
    "            min_iter=self.solver_params['min_iter'],\n",
    "            max_inneriters_balanced=self.solver_params['max_inneriters_balanced'],\n",
    "            max_inneriters_relaxed=self.solver_params['max_inneriters_relaxed'],\n",
    "            diagonalize_return=True,\n",
    "            printCost=False,\n",
    "            tau_in=self.solver_params['tau_in'],\n",
    "            dtype=C_sub.dtype\n",
    "        )\n",
    "        return Q, R\n",
    "\n",
    "    def _compute_coupling_from_Ft(self):\n",
    "        \"\"\"\n",
    "        Construit la matrice de couplage dense à partir des clusters Monge map.\n",
    "        \"\"\"\n",
    "        size = (self.N, self.N)\n",
    "        P = jnp.zeros(size)\n",
    "        for idx1, idx2 in self.Monge_clusters:\n",
    "            for i in range(idx1.shape[0]):\n",
    "                P = P.at[idx1[i], idx2[i]].set(1)\n",
    "        # On normalise par N (masse uniformément répartie)\n",
    "        return P / self.N\n",
    "\n",
    "    def compute_OT_cost(self):\n",
    "        \"\"\"\n",
    "        Calcule le coût OT associé aux clusters Monge (sans construire explicitement le couplage).\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for idx1, idx2 in self.Monge_clusters:\n",
    "            if self.C is not None:\n",
    "                cost += jnp.sum(self.C[idx1, idx2])\n",
    "            else:\n",
    "                if self.sq_Euclidean:\n",
    "                    cost += jnp.sum((self.X[idx1] - self.Y[idx2])**2)\n",
    "                else:\n",
    "                    cost += jnp.linalg.norm(self.X[idx1] - self.Y[idx2])\n",
    "        return cost / self.N\n",
    "\n",
    "    def get_dist_mats(self, _x0, _x1, rankD, eps, sq_Euclidean):\n",
    "        \"\"\"\n",
    "        Retourne les facteurs de la matrice de distance low-rank.\n",
    "        \"\"\"\n",
    "        if sq_Euclidean:\n",
    "            # Facteurs pour une distance euclidienne au carré\n",
    "            C_factors = compute_lr_sqeuclidean_matrix(_x0, _x1, True)\n",
    "            A_factors = None\n",
    "            B_factors = None\n",
    "        else:\n",
    "            # Low-rank factorisation pour distance standard\n",
    "            C_factors = self.ret_normalized_cost(_x0, _x1, rankD, eps)\n",
    "            A_factors = None\n",
    "            B_factors = None\n",
    "        return C_factors, A_factors, B_factors\n",
    "\n",
    "    def ret_normalized_cost(self, X, Y, rankD, eps):\n",
    "        \"\"\"\n",
    "        Facteur de coût normalisé via factorisation low-rank de distance.\n",
    "        \"\"\"\n",
    "        C1, C2 = utils__low_rank_distance_factorization(\n",
    "            X, Y, r=rankD, eps=eps, device=self.device\n",
    "        )\n",
    "        c = (jnp.max(C1)**0.5) * (jnp.max(C2)**0.5)\n",
    "        C1 = (C1 / c).astype(X.dtype)\n",
    "        C2 = (C2 / c).astype(X.dtype)\n",
    "        return C1, C2\n",
    "\n",
    "def compute_lr_sqeuclidean_matrix(X_s, X_t, rescale_cost, device=None, dtype=None):\n",
    "    \"\"\"\n",
    "    Calcul de la factorisation low-rank de la distance euclidienne au carré.\n",
    "    \"\"\"\n",
    "    dtype = X_s.dtype\n",
    "    ns, _ = X_s.shape\n",
    "    nt, _ = X_t.shape\n",
    "\n",
    "    sum_Xs_sq = jnp.sum(X_s ** 2, axis=1).reshape(ns, 1)\n",
    "    ones_ns = jnp.ones((ns, 1), dtype=dtype)\n",
    "    neg_two_Xs = -2 * X_s\n",
    "    M1 = jnp.concatenate([sum_Xs_sq, ones_ns, neg_two_Xs], axis=1)\n",
    "\n",
    "    ones_nt = jnp.ones((nt, 1), dtype=dtype)\n",
    "    sum_Xt_sq = jnp.sum(X_t ** 2, axis=1).reshape(nt, 1)\n",
    "    M2 = jnp.concatenate([ones_nt, sum_Xt_sq, X_t], axis=1)\n",
    "\n",
    "    if rescale_cost:\n",
    "        max_M1 = jnp.max(M1)\n",
    "        max_M2 = jnp.max(M2)\n",
    "        if max_M1 > 0:\n",
    "            M1 = M1 / jnp.sqrt(max_M1)\n",
    "        if max_M2 > 0:\n",
    "            M2 = M2 / jnp.sqrt(max_M2)\n",
    "\n",
    "    return (M1, M2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized rank-annealing schedule: [2, 1250]\n"
     ]
    }
   ],
   "source": [
    "rank_schedule = rank_annealing__optimal_rank_schedule( X.shape[0] , hierarchy_depth = 6, max_Q = int(2**11), max_rank = 64 )\n",
    "\n",
    "try:\n",
    "    hrot_lr = HierarchicalRefinementOT.init_from_point_clouds(X, Y, rank_schedule, base_rank=1)\n",
    "    del X, Y\n",
    "    F = hrot_lr.run(return_as_coupling=False)\n",
    "    cost_hrot_lr = hrot_lr.compute_OT_cost()\n",
    "    print(f'HR-OT cost: {cost_hrot_lr}')\n",
    "except Exception as e:\n",
    "    print(f'HROT-LR failed for sample size: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
