{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfd5b5-ef7d-4ed7-b168-5d76ac0ddc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\courn\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Callable, Union, Dict, Any\n",
    "import random\n",
    "import operator\n",
    "import functools\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ott.geometry import costs, pointcloud\n",
    "from ott.tools import sinkhorn_divergence, progot\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41386d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Package for computing an optimal rank-annealing schedule for Hierarchical Refinement\n",
    "\"\"\"\n",
    "\n",
    "def rank_annealing__optimal_rank_schedule(n, hierarchy_depth=6, max_Q=int(2**10), max_rank=16):\n",
    "    \"\"\"\n",
    "    \n",
    "    A function to compute the optimal rank-scheduler of refinement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        Size of the input dataset -- cannot be a prime number\n",
    "    hierarchy_depth: int\n",
    "        Maximal permissible depth of the multi-scale hierarchy\n",
    "    max_Q: int\n",
    "        Maximal rank at terminal base case (before reducing the \\leq max_Q rank coupling to a 1-1 alignment)\n",
    "    max_rank: int\n",
    "        Maximal rank at the intermediate steps of the rank-schedule\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Factoring out the max factor\n",
    "    Q = rank_annealing__max_factor_lX(n, max_Q)\n",
    "    ndivQ = int(n / Q)\n",
    "\n",
    "    # Compute partial rank schedule up to Q\n",
    "    min_value, rank_schedule = rank_annealing__min_sum_partial_products_with_factors(ndivQ, hierarchy_depth, max_rank)\n",
    "    rank_schedule.sort()\n",
    "    rank_schedule.append(Q)\n",
    "    rank_schedule = [x for x in rank_schedule if x != 1]\n",
    "    \n",
    "    print(f'Optimized rank-annealing schedule: { rank_schedule }')\n",
    "    \n",
    "    assert functools.reduce(operator.mul, rank_schedule) == n, \"Error! Rank-schedule does not factorize n!\"\n",
    "    \n",
    "    return rank_schedule\n",
    "\n",
    "def rank_annealing__factors(n):\n",
    "    # Return list of all factors of an integer\n",
    "    return set(reduce(\n",
    "        list.__add__,\n",
    "        ([i, n//i] for i in range(1, int(n**0.5) + 1) if n % i == 0)))\n",
    "\n",
    "def rank_annealing__max_factor_lX(n, max_X):\n",
    "    # Find max factor of n , such that max_factor \\leq max_X\n",
    "    factor_lst = rank_annealing__factors(n)\n",
    "    max_factor = 0\n",
    "    for factor in factor_lst:\n",
    "        if factor > max_factor and factor < max_X:\n",
    "            max_factor = factor\n",
    "    return max_factor\n",
    "\n",
    "def rank_annealing__min_sum_partial_products_with_factors(n, k, C):\n",
    "    \"\"\"\n",
    "    Dynamic program to compute the rank-schedule, subject to a constraint of intermediates being \\leq C\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        The dataset size to be factored into a rank-scheduler. Assumed to be non-prime.\n",
    "    k: int\n",
    "        The depth of the hierarchy.\n",
    "    C: int\n",
    "        A constraint on the maximal intermediate rank across the hierarchy.\n",
    "    \n",
    "    \"\"\"\n",
    "    INF = float('inf')\n",
    "    \n",
    "    dp = [[INF]*(k+1) for _ in range(n+1)]\n",
    "    choice = [[-1]*(k+1) for _ in range(n+1)]\n",
    "    \n",
    "    for d in range(1, n+1):\n",
    "        if d <= C:\n",
    "            dp[d][1] = d\n",
    "            choice[d][1] = d\n",
    "    \n",
    "    for t in range(2, k+1):\n",
    "        for d in range(1, n+1):\n",
    "            if dp[d][t-1] == INF and t > 1:\n",
    "                pass\n",
    "            \n",
    "            for r in range(1, min(C,d)+1):\n",
    "                if d % r == 0:\n",
    "                    candidate = r + r * dp[d // r][t-1]\n",
    "                    if candidate < dp[d][t]:\n",
    "                        dp[d][t] = candidate\n",
    "                        choice[d][t] = r\n",
    "    \n",
    "    \n",
    "    if dp[n][k] == INF:\n",
    "        return None, []\n",
    "    \n",
    "    factors = []\n",
    "    d_cur, t_cur = n, k\n",
    "    \n",
    "    while t_cur > 0:\n",
    "        r_cur = choice[d_cur][t_cur]\n",
    "        factors.append(r_cur)\n",
    "        d_cur //= r_cur\n",
    "        t_cur -= 1\n",
    "    \n",
    "    return dp[n][k], factors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils__initialize_couplings(a, b, gQ, gR, gamma, \\\n",
    "                         full_rank=True, device='cpu', \\\n",
    "                         dtype=torch.float64, rank2_random=False, \\\n",
    "                        max_iter=50):\n",
    "    '''\n",
    "    ------Parameters------\n",
    "    a: torch tensor\n",
    "        Left outer marginal, should be positive and sum to 1.0\n",
    "    b: torch tensor\n",
    "        Right outer marginal, should be positive and sum to 1.0\n",
    "    gQ: torch tensor\n",
    "        Left inner marginal, should be positive and sum to 1.0\n",
    "    gR: torch tensor\n",
    "        Right inner marginal, should be positive and sum to 1.0\n",
    "    gamma: float\n",
    "        Step-size of the coordinate MD\n",
    "    full_rank: bool\n",
    "        If True, initialize a full-rank set of sub-couplings.\n",
    "        Else if False, initialize with a rank-2 initialization.\n",
    "    device: str\n",
    "        'cpu' if running on CPU, else 'cuda' for GPU\n",
    "    dtype: torch dtype\n",
    "        Defaults to float64\n",
    "    rank2_random: bool\n",
    "        If False, use deterministic rank 2 initialization of Scetbon '21\n",
    "        Else, use an initialization with randomly sampled vector on simplex.\n",
    "    max_iter: int\n",
    "        The maximum number of Sinkhorn iterations for initialized sub-couplings.\n",
    "    '''\n",
    "    N1, N2 = a.size(dim=0), b.size(dim=0)\n",
    "    r, r2 = gQ.size(dim=0), gR.size(dim=0)\n",
    "    one_N1 = torch.ones((N1), device=device, dtype=dtype)\n",
    "    one_N2 = torch.ones((N2), device=device, dtype=dtype)\n",
    "    \n",
    "    if full_rank:\n",
    "        '''\n",
    "        A means of initializing full-rank sub-coupling matrices using randomly sampled matrices\n",
    "        and Sinkhorn projection onto the polytope of feasible couplings.\n",
    "\n",
    "        Only non-diagonal initialization for the LC-factorization and handles the case of unequal\n",
    "        inner left and right ranks (non-square latent couplings).\n",
    "        '''\n",
    "        # 1. Q-generation\n",
    "        # Generate a random (full-rank) matrix as our coupling initialization\n",
    "        C_random = torch.rand((N1,r), device=device, dtype=dtype)\n",
    "        '''\n",
    "        # Generate a random Kernel\n",
    "        xi_random = torch.exp( -C_random )\n",
    "        # Generate a random coupling\n",
    "        u, v = Sinkhorn(xi_random, a, gQ, N1, r, gamma, device=device, max_iter=max_iter, dtype=dtype)\n",
    "        Q = torch.diag(u) @ xi_random @ torch.diag(v)\n",
    "        '''\n",
    "        Q = utils__logSinkhorn(C_random, a, gQ, gamma, max_iter = max_iter, \\\n",
    "                         device=device, dtype=dtype, balanced=True, unbalanced=False)\n",
    "        \n",
    "        # 2. R-generation\n",
    "        C_random = torch.rand((N2,r2), device=device, dtype=dtype)\n",
    "        '''\n",
    "        xi_random = torch.exp( -C_random )\n",
    "        u, v = Sinkhorn(xi_random, b, gR, N2, r2, gamma, device=device, max_iter=max_iter, dtype=dtype)\n",
    "        R = torch.diag(u) @ xi_random @ torch.diag(v)'''\n",
    "        R = utils__logSinkhorn(C_random, b, gR, gamma, max_iter = max_iter, \\\n",
    "                         device=device, dtype=dtype, balanced=True, unbalanced=False)\n",
    "        \n",
    "        # 3. T-generation\n",
    "        gR, gQ = R.T @ one_N2, Q.T @ one_N1\n",
    "        C_random = torch.rand((r,r2), device=device, dtype=dtype)\n",
    "        '''\n",
    "        xi_random = torch.exp( -C_random )\n",
    "        u, v = Sinkhorn(xi_random, gQ, gR, r, r2, gamma, device=device, max_iter=max_iter, dtype=dtype)\n",
    "        T = torch.diag(u) @ xi_random @ torch.diag(v)\n",
    "        '''\n",
    "        T = utils__logSinkhorn(C_random, gQ, gR, gamma, max_iter = max_iter, \\\n",
    "                         device=device, dtype=dtype, balanced=True, unbalanced=False)\n",
    "        \n",
    "        # Use this to form the inner inverse coupling\n",
    "        if r == r2:\n",
    "            Lambda = torch.linalg.inv(T)\n",
    "        else:\n",
    "            Lambda = torch.diag(1/gQ) @ T @ torch.diag(1/gR)\n",
    "            #also, could do: torch.diag(1/gQ) @ T @ torch.diag(1/gR)\n",
    "    elif r == r2:\n",
    "        '''\n",
    "        Rank-2 initialization which requires equal inner ranks and gQ = gR = g.\n",
    "        This is adapted from \"Low-Rank Sinkhorn Factorization\" at https://arxiv.org/pdf/2103.04737\n",
    "        We advise setting full_rank = True and using the first initialization.\n",
    "        '''\n",
    "        g = gQ\n",
    "        lambd = torch.min(torch.tensor([torch.min(a), torch.min(b), torch.min(g)])) / 2\n",
    "\n",
    "        if rank2_random:\n",
    "            # Take random sample from probability simplex\n",
    "            a1 = utils__random_simplex_sample(N1, device=device, dtype=dtype)\n",
    "            b1 = utils__random_simplex_sample(N2, device=device, dtype=dtype)\n",
    "            g1 = utils__random_simplex_sample(r, device=device, dtype=dtype)\n",
    "        else:\n",
    "            # or initialize exactly as in scetbon 21' ott-jax repo\n",
    "            g1 = torch.arange(1, r + 1, device=device, dtype=dtype)\n",
    "            g1 /= g1.sum()\n",
    "            a1 = torch.arange(1, N1 + 1, device=device, dtype=dtype)\n",
    "            a1 /= a1.sum()\n",
    "            b1 = torch.arange(1, N2 + 1, device=device, dtype=dtype)\n",
    "            b1 /= b1.sum()\n",
    "        \n",
    "        a2 = (a - lambd*a1)/(1 - lambd)\n",
    "        b2 = (b - lambd*b1)/(1 - lambd)\n",
    "        g2 = (g - lambd*g1)/(1 - lambd)\n",
    "        \n",
    "        # Generate Rank-2 Couplings\n",
    "        Q = lambd*torch.outer(a1, g1).to(device) + (1 - lambd)*torch.outer(a2, g2).to(device)\n",
    "        R = lambd*torch.outer(b1, g1).to(device) + (1 - lambd)*torch.outer(b2, g2).to(device)\n",
    "        \n",
    "        # This is already determined as g (but recomputed anyway)\n",
    "        gR, gQ = R.T @ one_N2, Q.T @ one_N1\n",
    "        \n",
    "        # Last term adds very tiny off-diagonal component for the non-diagonal LC-factorization (o/w the matrix stays fully diagonal)\n",
    "        T = (1-lambd)*torch.diag(g) + lambd*torch.outer(gR, gQ).to(device)\n",
    "        Lambda = torch.linalg.inv(T)\n",
    "    \n",
    "    return Q, R, T, Lambda\n",
    "\n",
    "\n",
    "def utils__k_means_initialization(x0, x1, r1, r2=None, \\\n",
    "                           a=None, b=None, gQ=None, gR=None, \\\n",
    "                           eps = 1e-3, device = 'cpu', \\\n",
    "                           dtype=torch.float64):\n",
    "    '''\n",
    "    An initialization relying on a pair of k-means clusterings on the first and second dataset.\n",
    "    ------Parameters------\n",
    "    x0: torch.tensor\n",
    "        First N1 x d dataset for d the data-dimension\n",
    "    x1: torch.tensor\n",
    "        Second N2 x d dataset for d the data-dimension\n",
    "    r1: int\n",
    "        Latent source rank\n",
    "    r2: int\n",
    "        Latent target rank\n",
    "    a: torch tensor\n",
    "        Left outer marginal, should be positive and sum to 1.0\n",
    "    b: torch tensor\n",
    "        Right outer marginal, should be positive and sum to 1.0\n",
    "    gQ: torch tensor\n",
    "        Left inner marginal, should be positive and sum to 1.0\n",
    "    gR: torch tensor\n",
    "        Right inner marginal, should be positive and sum to 1.0\n",
    "    eps: float\n",
    "        Epsilon used for Sinkhorn to generate the sub-couplings.\n",
    "    device: str\n",
    "        'cpu' if running on CPU, else 'cuda' for GPU\n",
    "    dtype: torch dtype\n",
    "        Defaults to float64\n",
    "    '''\n",
    "    n, m =  x0.size(dim=0), x1.size(dim=0)\n",
    "    # Initialize outer marginals\n",
    "    if a is None:\n",
    "        one_n = torch.ones((n), device=device, dtype=dtype)\n",
    "        a = one_n / n\n",
    "    if b is None:\n",
    "        one_m = torch.ones((m), device=device, dtype=dtype)\n",
    "        b = one_m / m\n",
    "    # Set ranks equal if second rank not given  \n",
    "    if r2 is None:\n",
    "        r2 = r1\n",
    "    \n",
    "    if gQ is None:\n",
    "        one_r1 = torch.ones((r1), device=device, dtype=dtype)\n",
    "        gQ = one_r1 / r1\n",
    "    if gR is None:\n",
    "        one_r2 = torch.ones((r2), device=device, dtype=dtype)\n",
    "        gR = one_r2 / r2\n",
    "    _x0, _x1 = x0.cpu().numpy(), x1.cpu().numpy()\n",
    "    # Compute optimal clustering to initialize OT alignment\n",
    "    y0 = KMeans(n_clusters=r1, n_init=\"auto\").fit(_x0).cluster_centers_\n",
    "    y1 = KMeans(n_clusters=r2, n_init=\"auto\").fit(_x1).cluster_centers_\n",
    "    # Move back to tensor\n",
    "    x0,x1=x0.double(),x1.double()\n",
    "    y0,y1 = torch.from_numpy(y0).to(device).double(),torch.from_numpy(y1).to(device).double()\n",
    "    # Compute distance matrices\n",
    "    CQ,CT,CR = torch.cdist(x0, y0), torch.cdist(y0, y1), torch.cdist(x1,y1)\n",
    "    # Generate Kernel\n",
    "    xiQ, xiR, xiT = torch.exp( -CQ / eps ), torch.exp( -CR / eps ), torch.exp( -CT / eps )\n",
    "    \n",
    "    # Generate couplings\n",
    "    u, v = utils__Sinkhorn(xiQ, a, gQ, n, r1, eps, device=device)\n",
    "    Q = torch.diag(u) @ xiQ @ torch.diag(v)\n",
    "    u, v = utils__Sinkhorn(xiR, b, gR, m, r2, eps, device=device)\n",
    "    R = torch.diag(u) @ xiR @ torch.diag(v)\n",
    "    u, v = utils__Sinkhorn(xiT, gQ, gR, r1, r2, eps, device=device)\n",
    "    T = torch.diag(u) @ xiT @ torch.diag(v)\n",
    "    \n",
    "    return (Q,R,T)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def utils__random_simplex_sample(N, device='cpu', dtype=torch.float64):\n",
    "    # Samples a random N-dimensional vector from the simplex\n",
    "    d = torch.exp(torch.randn(N, device=device, dtype=dtype))\n",
    "    return d / torch.sum(d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def utils__semi_project_Left(xi1, a, g, N1, r, gamma_k, tau, max_iter = 50, \\\n",
    "                      delta = 1e-9, device='cpu', dtype=torch.float64):\n",
    "    '''\n",
    "    Semi-relaxed Sinkhorn with tight left marginal.\n",
    "    '''\n",
    "    u = torch.ones((N1), device=device, dtype=dtype)\n",
    "    v = torch.ones((r), device=device, dtype=dtype)\n",
    "    u_tild = u\n",
    "    v_tild = v\n",
    "    i = 0\n",
    "    while i == 0 or (i < max_iter and \n",
    "                     gamma_k**-1 * torch.max(torch.tensor([torch.max(torch.log(u/u_tild)),torch.max(torch.log(v/v_tild))])) > delta ):\n",
    "        u_tild = u\n",
    "        v_tild = v\n",
    "        u = (a / (xi1 @ v))**(tau/(tau + gamma_k**-1 ))\n",
    "        v = (g / (xi1.T @ u))\n",
    "        i+=1\n",
    "    \n",
    "    return u, v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def utils__semi_project_Right(xi2, b, g, N2, r, gamma_k, tau, max_iter = 50, \\\n",
    "                       delta = 1e-9, device='cpu', dtype=torch.float64):\n",
    "    '''\n",
    "    Semi-relaxed Sinkhorn with tight right marginal.\n",
    "    '''\n",
    "    u = torch.ones((N2), device=device, dtype=dtype)\n",
    "    v = torch.ones((r), device=device, dtype=dtype)\n",
    "    u_tild = u\n",
    "    v_tild = v\n",
    "    i = 0\n",
    "    while i == 0 or (i < max_iter and \n",
    "                     gamma_k**-1 * torch.max(torch.tensor([torch.max(torch.log(u/u_tild)),torch.max(torch.log(v/v_tild))])) > delta ):\n",
    "        u_tild = u\n",
    "        v_tild = v\n",
    "        u = (b / (xi2 @ v))**(tau/(tau + gamma_k**-1 ))\n",
    "        v = (g / (xi2.T @ u))\n",
    "        i+=1\n",
    "    \n",
    "    return u, v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def utils__semi_project_Balanced(xi1, a, g, N1, r, gamma_k, tau, max_iter = 50, \\\n",
    "                          delta = 1e-9, device='cpu', dtype=torch.float64):\n",
    "    # Lax-inner marginal\n",
    "    u = torch.ones((N1), device=device, dtype=dtype)\n",
    "    v = torch.ones((r), device=device, dtype=dtype)\n",
    "    u_tild = u\n",
    "    v_tild = v\n",
    "    i = 0\n",
    "    while i == 0 or (i < max_iter and \n",
    "                     gamma_k**-1 * torch.max(torch.tensor([torch.max(torch.log(u/u_tild)),torch.max(torch.log(v/v_tild))])) > delta ):\n",
    "        u_tild = u\n",
    "        v_tild = v\n",
    "        v = (g / (xi1.T @ u))**(tau/(tau + gamma_k**-1 ))\n",
    "        u = (a / (xi1 @ v))\n",
    "        i+=1\n",
    "    \n",
    "    return u, v\n",
    "\n",
    "\n",
    "def utils__project_Unbalanced(xi1, a, g, N1, r, gamma_k, tau, max_iter = 50, \\\n",
    "                       delta = 1e-9, device='cpu', dtype=torch.float64):\n",
    "    '''\n",
    "    Fully-relaxed Sinkhorn with relaxed left and right marginals.\n",
    "    '''\n",
    "    # Unbalanced\n",
    "    u = torch.ones((N1), device=device, dtype=dtype)\n",
    "    v = torch.ones((r), device=device, dtype=dtype)\n",
    "    u_tild = u\n",
    "    v_tild = v\n",
    "    i = 0\n",
    "    while i == 0 or (i < max_iter and \n",
    "                     gamma_k**-1 * torch.max(torch.tensor([torch.max(torch.log(u/u_tild)),torch.max(torch.log(v/v_tild))])) > delta ):\n",
    "        u_tild = u\n",
    "        v_tild = v\n",
    "        v = (g / (xi1.T @ u))**(tau/(tau + gamma_k**-1 ))\n",
    "        u = (a / (xi1 @ v))**(tau/(tau + gamma_k**-1 ))\n",
    "        i+=1\n",
    "    \n",
    "    return u, v\n",
    "\n",
    "def utils__logSinkhorn(grad, a, b, gamma_k, max_iter = 50, \\\n",
    "             device='cpu', dtype=torch.float64, balanced=True, unbalanced=False, tau=None, tau2=None):\n",
    "    \n",
    "    log_a = torch.log(a)\n",
    "    log_b = torch.log(b)\n",
    "\n",
    "    n, m = a.size(0), b.size(0)\n",
    "    \n",
    "    f_k = torch.zeros((n), device=device)\n",
    "    g_k = torch.zeros((m), device=device)\n",
    "\n",
    "    epsilon = gamma_k**-1\n",
    "    \n",
    "    if not balanced:\n",
    "        ubc = (tau/(tau + epsilon ))\n",
    "        if tau2 is not None:\n",
    "            ubc2 = (tau2/(tau2 + epsilon ))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        if balanced and not unbalanced:\n",
    "            # Balanced\n",
    "            f_k = f_k + epsilon*(log_a - torch.logsumexp(utils__Cost(f_k, g_k, grad, epsilon, device=device), axis=1))\n",
    "            g_k = g_k + epsilon*(log_b - torch.logsumexp(utils__Cost(f_k, g_k, grad, epsilon, device=device), axis=0))\n",
    "        elif not balanced and unbalanced:\n",
    "            # Unbalanced\n",
    "            f_k = ubc*(f_k + epsilon*(log_a - torch.logsumexp(utils__Cost(f_k, g_k, grad, epsilon, device=device), axis=1)) )\n",
    "            g_k = ubc2*(g_k + epsilon*(log_b - torch.logsumexp(utils__Cost(f_k, g_k, grad, epsilon, device=device), axis=0)) )\n",
    "        else:\n",
    "            # Semi-relaxed\n",
    "            f_k = (f_k + epsilon*(log_a - torch.logsumexp(utils__Cost(f_k, g_k, grad, epsilon, device=device), axis=1)) )\n",
    "            g_k = ubc*(g_k + epsilon*(log_b - torch.logsumexp(utils__Cost(f_k, g_k, grad, epsilon, device=device), axis=0)) )\n",
    "\n",
    "    P = torch.exp(utils__Cost(f_k, g_k, grad, epsilon, device=device))\n",
    "    \n",
    "    return P\n",
    "\n",
    "def utils__Sinkhorn(xi, a, b, N1, r, gamma_k, max_iter = 50, \\\n",
    "             delta = 1e-9, device='cpu', dtype=torch.float64):\n",
    "    '''\n",
    "    A lightweight impl of Sinkhorn.\n",
    "    ------Parameters------\n",
    "    xi: torch tensor\n",
    "        An n x m matrix of the exponentiated positive Sinkhorn kernel.\n",
    "    a: torch tensor\n",
    "        Left outer marginal, should be positive and sum to 1.0\n",
    "    b: torch tensor\n",
    "        Right outer marginal, should be positive and sum to 1.0\n",
    "    N1: int\n",
    "        Dimension 1\n",
    "    r: int\n",
    "        Dimension 2\n",
    "    gamma_k: float\n",
    "        Step-size used for scaling convergence criterion.\n",
    "    max_iter: int\n",
    "        Maximum number of iterations for Sinkhorn loop\n",
    "    delta: float\n",
    "        Used for determining convergence to marginals\n",
    "    device: str\n",
    "        'cpu' if running on CPU, else 'cuda' for GPU\n",
    "    dtype: torch dtype\n",
    "        Defaults to float64\n",
    "    '''\n",
    "    u = torch.ones((N1), device=device, dtype=dtype)\n",
    "    v = torch.ones((r), device=device, dtype=dtype)\n",
    "    u_tild = u\n",
    "    v_tild = v\n",
    "    i = 0\n",
    "    \n",
    "    while i == 0 or (i < max_iter and \n",
    "                     gamma_k**-1 * torch.max(torch.tensor([torch.max(torch.log(u/u_tild)),torch.max(torch.log(v/v_tild))])) > delta ):\n",
    "        \n",
    "        u_tild = u\n",
    "        v_tild = v\n",
    "        u = (a / (xi @ v))\n",
    "        v = (b / (xi.T @ u))\n",
    "        i+=1\n",
    "        \n",
    "    return u, v\n",
    "\n",
    "def utils__Cost(f, g, Grad, epsilon, device='cpu', dtype=torch.float64):\n",
    "    '''\n",
    "    A matrix which is using for the broadcasted log-domain log-sum-exp trick-based updates.\n",
    "    ------Parameters------\n",
    "    f: torch.tensor (N1)\n",
    "        First dual variable of semi-unbalanced Sinkhorn\n",
    "    g: torch.tensor (N2)\n",
    "        Second dual variable of semi-unbalanced Sinkhorn\n",
    "    Grad: torch.tensor (N1 x N2)\n",
    "        A collection of terms in our gradient for the update\n",
    "    epsilon: float\n",
    "        Entropic regularization for Sinkhorn\n",
    "    device: 'str'\n",
    "        Device tensors placed on\n",
    "    '''\n",
    "    return -( Grad - torch.outer(f, torch.ones(Grad.size(dim=1), device=device)) - torch.outer(torch.ones(Grad.size(dim=0), device=device), g) ) / epsilon\n",
    "\n",
    "\n",
    "\n",
    "def utils__Delta(vark, varkm1, gamma_k):\n",
    "    '''\n",
    "    Convergence criterion for FRLC.\n",
    "    ------Parameters------\n",
    "    vark: tuple of 3-tensors\n",
    "        Tuple of coordinate MD block variables (Q,R,T) at current iter\n",
    "    varkm1:  tuple of 3-tensors\n",
    "        Tuple of coordinate MD block variables (Q,R,T) at previous iter\n",
    "    gamma_k: float\n",
    "        Coordinate MD step-size\n",
    "    '''\n",
    "    Q, R, T = vark\n",
    "    Q_prev, R_prev, T_prev = varkm1\n",
    "    error = (gamma_k**-2)*(torch.norm(Q - Q_prev) + torch.norm(R - R_prev) + torch.norm(T - T_prev))\n",
    "    return error\n",
    "\n",
    "\n",
    "def utils__low_rank_distance_factorization(X1, X2, r, eps, device='cpu', dtype=jnp.float64):\n",
    "    n = X1.shape[0]\n",
    "    m = X2.shape[0]\n",
    "    t = int(r / eps)  # La taille d'échantillon t\n",
    "\n",
    "    # Conversion en tableau JAX\n",
    "    X1_jax = jnp.array(X1)\n",
    "    X2_jax = jnp.array(X2)\n",
    "\n",
    "    # Étape 1 : Calcul des distances (matrice de coût)\n",
    "    cost_matrix = jnp.sum((X1_jax[:, None, :] - X2_jax[None, :, :]) ** 2, axis=-1)\n",
    "\n",
    "    # Étape 2 : Résolution du problème de transport optimal avec Sinkhorn\n",
    "    reg_param = 0.1  # Paramètre de régularisation, ajustez si nécessaire\n",
    "    ot_solution = sinkhorn(cost_matrix, reg_param, niter=1000)\n",
    "\n",
    "    # Étape 3 : Calcul des probabilités d'échantillonnage pour X1\n",
    "    # Pondération selon la solution du transport optimal\n",
    "    p = jnp.sum(ot_solution, axis=1)**2  # Probabilités pour X1\n",
    "    p_dist = p / jnp.sum(p)\n",
    "\n",
    "    # Échantillonnage des indices pour X1\n",
    "    indices_p = np.random.choice(n, size=t, p=p_dist)\n",
    "    X1_t = X1_jax[indices_p]\n",
    "\n",
    "    # Étape 4 : Calcul de P_t (ponctions d'échantillonnage)\n",
    "    P_t = jnp.sqrt(p[indices_p] * t)\n",
    "\n",
    "    # Calcul de la matrice S\n",
    "    S = jax.scipy.spatial.distance.cdist(X1_t, X2_jax) / P_t[:, None]  # t x m\n",
    "\n",
    "    # Étape 5 : Calcul des probabilités d'échantillonnage pour X2\n",
    "    q = jnp.norm(S, axis=0)**2 / jnp.norm(S)**2  # Probabilités pour X2\n",
    "    q_dist = q / jnp.sum(q)\n",
    "\n",
    "    # Échantillonnage des indices pour X2\n",
    "    indices_q = np.random.choice(m, size=t, p=q_dist)\n",
    "    S_t = S[:, indices_q]  # t x t\n",
    "\n",
    "    # Calcul de Q_t\n",
    "    Q_t = jnp.sqrt(q[indices_q] * t)\n",
    "\n",
    "    # Calcul de la matrice W\n",
    "    W = S_t / Q_t[None, :]\n",
    "\n",
    "    # Étape 6 : Décomposition SVD de W\n",
    "    U, Sig, Vh = jax.linalg.svd(W)\n",
    "\n",
    "    # Filtrage des premiers r vecteurs singuliers\n",
    "    F = U[:, :r]\n",
    "\n",
    "    # Calcul de U_t pour le retour\n",
    "    U_t = jnp.dot(S.T, F) / jnp.linalg.norm(jnp.dot(W.T, F))\n",
    "\n",
    "    # Étape 7 : Utilisation de U_t pour calculer V\n",
    "    indices = np.random.choice(m, size=t)\n",
    "    X2_t = X2_jax[indices]\n",
    "\n",
    "    D_t = jax.scipy.spatial.distance.cdist(X1_jax, X2_t) / jnp.sqrt(t)\n",
    "    Q = jnp.dot(U_t.T, U_t)  # Matrice carrée r x r\n",
    "    U, Sig, Vh = jax.linalg.svd(Q)\n",
    "    U = U / Sig  # Normalisation\n",
    "\n",
    "    U_tSub = U_t[indices, :].T  # t x r\n",
    "    B = jnp.dot(U.T, U_tSub) / jnp.sqrt(t)\n",
    "\n",
    "    # Calcul de A et de Z\n",
    "    A = jnp.linalg.inv(jnp.dot(B, B.T))\n",
    "    Z = jnp.dot(jnp.dot(A, B), D_t.T)  # r x t * t x n\n",
    "\n",
    "    # Calcul de V\n",
    "    V = jnp.dot(Z.T, U)\n",
    "\n",
    "    return V, U_t.T\n",
    "\n",
    "def utils__hadamard_square_lr(A1, A2, device='cpu'):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        A1: torch.tensor, low-rank subcoupling of shape (n, r)\n",
    "        A2: torch.tensor, low-rank subcoupling of shape (n, r)\n",
    "                ( such that A \\approx A1 @ A2.T )\n",
    "    \n",
    "    Output\n",
    "        A1_tilde: torch.tensor, low-rank subcoupling of shape (n, r**2)\n",
    "        A2_tilde: torch.tensor, low-rank subcoupling of shape (n, r**2)\n",
    "               ( such that A * A \\approx A1_tilde @ A2_tilde.T )\n",
    "    \"\"\"\n",
    "    \n",
    "    A1 = A1.to(device)\n",
    "    A2 = A2.to(device)\n",
    "    n, r = A1.shape\n",
    "    A1_tilde = torch.einsum(\"ij,ik->ijk\", A1, A1).reshape(n, r * r)\n",
    "    A2_tilde = torch.einsum(\"ij,ik->ijk\", A2, A2).reshape(n, r * r)\n",
    "    \n",
    "    return A1_tilde, A2_tilde\n",
    "\n",
    "\n",
    "def utils__hadamard_lr(A1, A2, B1, B2, device='cpu'):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        A1: torch.tensor, low-rank subcoupling of shape (n, r)\n",
    "        A2: torch.tensor, low-rank subcoupling of shape (n, r)\n",
    "                ( such that A \\approx A1 @ A2.T )\n",
    "        \n",
    "        B1: torch.tensor, low-rank subcoupling of shape (n, r)\n",
    "        B2: torch.tensor, low-rank subcoupling of shape (n, r)\n",
    "                ( such that B \\approx B1 @ B2.T )\n",
    "    \n",
    "    Output\n",
    "        M1_tilde: torch.tensor, low-rank subcoupling of shape (n, r**2)\n",
    "        M2_tilde: torch.tensor, low-rank subcoupling of shape (n, r**2)\n",
    "               ( such that A * B \\approx M1_tilde @ M2_tilde.T given low-rank factorizations for A & B)\n",
    "    \"\"\"\n",
    "    A1 = A1.to(device)\n",
    "    A2 = A2.to(device)\n",
    "    B1 = B1.to(device)\n",
    "    B2 = B2.to(device)\n",
    "    n, r = A1.shape\n",
    "\n",
    "    M1_tilde = torch.einsum(\"ij,ik->ijk\", A1, B1).reshape(n, r * r)\n",
    "    M2_tilde = torch.einsum(\"ij,ik->ijk\", A2, B2).reshape(n, r * r)\n",
    "    \n",
    "    return M1_tilde, M2_tilde\n",
    "\n",
    "# utils__hadamard_lr = lambda (A1, A2, B1, B2): (jnp.einsum(\"ij,ik->ijk\", A1, B1).reshape(A1.shape[0], -1), jnp.einsum(\"ij,ik->ijk\", A2, B2).reshape(A2.shape[0], -1))\n",
    "\n",
    "def utils__LC_proj(X0, X1, Q, R):\n",
    "    \n",
    "    gQ = torch.sum(Q,axis=0)\n",
    "    Q_barycenters = torch.diag(1/gQ) @ Q.T @ X0\n",
    "    gR = torch.sum(R,axis=0)\n",
    "    R_barycenters = torch.diag(1/gR) @ R.T @ X1\n",
    "\n",
    "    return Q_barycenters, R_barycenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea693d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd__compute_grad_A(C, Q, R, Lambda, gamma, \\\n",
    "                   semiRelaxedLeft, semiRelaxedRight, device, \\\n",
    "                   Wasserstein=True, FGW=False, A=None, B=None, \\\n",
    "                   alpha=0.0, unbalanced=False, \\\n",
    "                   dtype=torch.float64, full_grad=True):\n",
    "    \n",
    "    '''\n",
    "    Code for computing the Wasserstein, Gromov-Wasserstein, and fused Gromov-Wasserstein gradients with respect to Q and R.\n",
    "    These depend on the marginal relaxation of the specific OT problem you want to solve, due to proportionality simplifications.\n",
    "    \n",
    "    ------Parameters------\n",
    "    C: torch.tensor (N1 x N2)\n",
    "        A matrix of pairwise feature distances in space X and space Y (inter-space).\n",
    "    Q: torch.tensor (N1 x r)\n",
    "        The left sub-coupling matrix.\n",
    "    R: torch.tensor (N2 x r)\n",
    "        The right sub-coupling matrix.\n",
    "    Lambda: torch.tensor (r x r)\n",
    "        The inner transition matrix.\n",
    "    gamma: float\n",
    "        The mirror-descent step-size.\n",
    "    semiRelaxedLeft: bool\n",
    "        True if relaxing the left marginal.\n",
    "    semiRelaxedRight: bool\n",
    "        True if relaxing the right marginal.\n",
    "    device: str\n",
    "        The device (i.e. 'cpu' or 'cuda')\n",
    "    Wasserstein: bool\n",
    "        True if using the Wasserstein loss <C, P>_F as the objective cost,\n",
    "        else runs GW if FGW false and FGW if GW true.\n",
    "    FGW: bool\n",
    "        True if running the Fused-Gromov Wasserstein problem, and otherwise false.\n",
    "    A: torch.tensor (N1 x N1)\n",
    "        Pairwise distance matrix in metric space X.\n",
    "    B: torch.tensor (N2 x N2)\n",
    "        Pairwise distance matrix in metric space Y.\n",
    "    alpha: float\n",
    "        A balance parameter between the Wasserstein term and\n",
    "        the Gromov-Wasserstein term of the objective.\n",
    "    unbalanced: bool\n",
    "        True if running the unbalanced problem;\n",
    "        if semiRelaxedLeft/Right and unbalanced False (default) then running the balanced problem.\n",
    "    '''\n",
    "    \n",
    "    r = Lambda.shape[0]\n",
    "    one_r = torch.ones((r), device=device, dtype=dtype)\n",
    "    One_rr = torch.outer(one_r, one_r).to(device)\n",
    "    \n",
    "    if Wasserstein:\n",
    "        gradQ, gradR = gd__Wasserstein_Grad(C, Q, R, Lambda, device, \\\n",
    "                   dtype=dtype, full_grad=full_grad)\n",
    "        \n",
    "    elif A is not None and B is not None:\n",
    "        if not semiRelaxedLeft and not semiRelaxedRight and not unbalanced:\n",
    "            # Balanced gradient (Q1_r = a AND R1_r = b)\n",
    "            gradQ = - 4 * (A@Q)@Lambda@(R.T@B@R)@Lambda.T\n",
    "            gradR = - 4 * (B@R@Lambda.T)@(Q.T@A@Q)@Lambda\n",
    "        elif semiRelaxedRight:\n",
    "            # Semi-relaxed right marginal gradient (Q1_r = a)\n",
    "            gradQ = - 4 * (A@Q)@Lambda@(R.T@B@R)@Lambda.T\n",
    "            gradR = 2*B**2 @ R @ One_rr - 4*(B@R@Lambda.T)@(Q.T@A@Q)@Lambda\n",
    "        elif semiRelaxedLeft:\n",
    "            # Semi-relaxed left marginal gradient (R1_r = b)\n",
    "            gradQ = 2*A**2 @ Q @ One_rr - 4 * (A@Q)@Lambda@(R.T@B@R)@Lambda.T\n",
    "            gradR = - 4 * (B@R@Lambda.T)@(Q.T@A@Q)@Lambda\n",
    "        elif unbalanced:\n",
    "            # Fully unbalanced with no marginal constraints\n",
    "            gradQ = 2*A**2 @ Q @ One_rr - 4 * (A@Q)@Lambda@(R.T@B@R)@Lambda.T\n",
    "            gradR = 2*B**2 @ R @ One_rr - 4 * (B@R@Lambda.T)@(Q.T@A@Q)@Lambda\n",
    "\n",
    "        if full_grad:\n",
    "            N1, N2 = Q.shape[0], R.shape[0]\n",
    "            one_N1, one_N2 = torch.ones((N1), device=device, dtype=dtype), torch.ones((N2), device=device, dtype=dtype)\n",
    "            gQ, gR = Q.T @ one_N1, R.T @ one_N2\n",
    "            F = (Q@Lambda@R.T)\n",
    "            MR = Lambda.T @ Q.T @ A @ F @ B @ R @ torch.diag(1/gR)\n",
    "            MQ = Lambda @ R.T @ B @ F.T @ A @ Q @ torch.diag(1/gQ)\n",
    "            gradQ += 4*torch.outer(one_N1, torch.diag(MQ))\n",
    "            gradR += 4*torch.outer(one_N2, torch.diag(MR))\n",
    "        \n",
    "        # Readjust cost for FGW problem\n",
    "        if FGW:\n",
    "            gradQW, gradRW = gd__Wasserstein_Grad(C, Q, R, Lambda, device, \\\n",
    "                   dtype=dtype, full_grad=full_grad)\n",
    "            gradQ = (1-alpha)*gradQW + alpha*gradQ\n",
    "            gradR = (1-alpha)*gradRW + alpha*gradR\n",
    "    else:\n",
    "        raise ValueError(\"---Input either Wasserstein=True or provide distance matrices A and B for GW problem---\")\n",
    "        \n",
    "    normalizer = torch.max(torch.tensor([torch.max(torch.abs(gradQ)) , torch.max(torch.abs(gradR))]))\n",
    "    gamma_k = gamma / normalizer\n",
    "    \n",
    "    return gradQ, gradR, gamma_k\n",
    "\n",
    "def gd__compute_grad_B(C, Q, R, Lambda, gQ, gR, gamma, device, Wasserstein=True, \\\n",
    "                   FGW=False, A=None, B=None, alpha=0.0, \\\n",
    "                   dtype=torch.float64):\n",
    "    '''\n",
    "    Code for computing the Wasserstein, Gromov-Wasserstein, and fused Gromov-Wasserstein gradients with respect to T.\n",
    "    \n",
    "    ------Parameters------\n",
    "    C: torch.tensor (N1 x N2)\n",
    "        A matrix of pairwise feature distances in space X and space Y (inter-space).\n",
    "    Q: torch.tensor (N1 x r)\n",
    "        The left sub-coupling matrix.\n",
    "    R: torch.tensor (N2 x r)\n",
    "        The right sub-coupling matrix.\n",
    "    Lambda: torch.tensor (r x r)\n",
    "        The inner transition matrix.\n",
    "    gQ: torch.tensor (r)\n",
    "        The inner marginal corresponding to the matrix Q.\n",
    "    gR: torch.tensor (r)\n",
    "        The inner marginal corresponding to the matrix R.\n",
    "    gamma: float\n",
    "        The mirror-descent step-size.\n",
    "    device: str\n",
    "        The device (i.e. 'cpu' or 'cuda')\n",
    "    Wasserstein: bool\n",
    "        True if using the Wasserstein loss <C, P>_F as the objective cost,\n",
    "        else runs GW if FGW false and FGW if GW true.\n",
    "    FGW: bool\n",
    "        True if running the Fused-Gromov Wasserstein problem, and otherwise false.\n",
    "    A: torch.tensor (N1 x N1)\n",
    "        Pairwise distance matrix in metric space X.\n",
    "    B: torch.tensor (N2 x N2)\n",
    "        Pairwise distance matrix in metric space Y.\n",
    "    alpha: float\n",
    "        A balance parameter between the Wasserstein term and\n",
    "        the Gromov-Wasserstein term of the objective.\n",
    "    '''\n",
    "    if Wasserstein:\n",
    "        gradLambda = Q.T @ C @ R\n",
    "    else:\n",
    "        gradLambda = -4 * Q.T @ A @ Q @ Lambda @ R.T @ B @ R\n",
    "        if FGW:\n",
    "            gradLambda = (1-alpha)*(Q.T @ C @ R) + alpha*gradLambda\n",
    "    gradT = torch.diag(1/gQ) @ gradLambda @ torch.diag(1/gR) # (mass-reweighted form)\n",
    "    gamma_T = gamma / torch.max(torch.abs(gradT))\n",
    "    return gradT, gamma_T\n",
    "\n",
    "def gd__Wasserstein_Grad(C, Q, R, Lambda, device, \\\n",
    "                   dtype=torch.float64, full_grad=True):\n",
    "    \n",
    "    gradQ = (C @ R) @ Lambda.T\n",
    "    if full_grad:\n",
    "        # rank-one perturbation\n",
    "        N1 = Q.shape[0]\n",
    "        one_N1 = torch.ones((N1), device=device, dtype=dtype)\n",
    "        gQ = Q.T @ one_N1\n",
    "        w1 = torch.diag( (gradQ.T @ Q) @ torch.diag(1/gQ) )\n",
    "        gradQ -= torch.outer(one_N1, w1)\n",
    "    \n",
    "    # linear term\n",
    "    gradR = (C.T @ Q) @ Lambda\n",
    "    if full_grad:\n",
    "        # rank-one perturbation\n",
    "        N2 = R.shape[0]\n",
    "        one_N2 = torch.ones((N2), device=device, dtype=dtype)\n",
    "        gR = R.T @ one_N2\n",
    "        w2 = torch.diag( torch.diag(1/gR) @ (R.T @ gradR) )\n",
    "        gradR -= torch.outer(one_N2, w2)\n",
    "    \n",
    "    return gradQ, gradR\n",
    "\n",
    "'''\n",
    "--------------\n",
    "Code for gradients assuming low-rank distance matrices C, A, B\n",
    "--------------\n",
    "'''\n",
    "\n",
    "def gd__compute_grad_A_LR(C_factors, A_factors, B_factors, Q, R, Lambda, gamma, device, \\\n",
    "                   alpha=0.0, dtype=torch.float64, full_grad=False):\n",
    "    \n",
    "    r = Lambda.shape[0]\n",
    "    one_r = torch.ones((r), device=device, dtype=dtype)\n",
    "    One_rr = torch.outer(one_r, one_r).to(device)\n",
    "    N1, N2 = C_factors[0].size(0), C_factors[1].size(1)\n",
    "    \n",
    "    if A_factors is not None and B_factors is not None:\n",
    "        A1, A2 = A_factors[0], A_factors[1]\n",
    "        B1, B2 = B_factors[0], B_factors[1]\n",
    "        \n",
    "        # A*2's low-rank factorization\n",
    "        A1_tild, A2_tild = utils__hadamard_square_lr(A1, A2.T, device=device)\n",
    "        \n",
    "        # GW gradients for balanced marginal cases\n",
    "        gradQ = - 4 * (A1 @ (A2 @ (Q @ Lambda@( (R.T@ B1) @ (B2 @R) )@Lambda.T)) )\n",
    "        gradR = - 4 * (B1 @ (B2 @ (R @ (Lambda.T@( (Q.T @ A1) @ ( A2 @ Q ))@Lambda)) ) )\n",
    "    \n",
    "        one_N1, one_N2 = torch.ones((N1), device=device, dtype=dtype), torch.ones((N2), device=device, dtype=dtype)\n",
    "        if full_grad:\n",
    "            # Rank-1 GW perturbation\n",
    "            N1, N2 = Q.shape[0], R.shape[0]\n",
    "            gQ, gR = Q.T @ one_N1, R.T @ one_N2\n",
    "            \n",
    "            MR = Lambda.T @ ( (Q.T @ A1) @ (A2 @ Q) ) @ Lambda @ ((R.T @ B1) @ (B2 @ R)) @ torch.diag(1/gR)\n",
    "            MQ = Lambda @ ( (R.T @ B1) @ (B2 @ R) ) @ Lambda.T @ ((Q.T @ A1) @ (A2 @ Q) ) @ torch.diag(1/gQ)\n",
    "            gradQ += 4*torch.outer(one_N1, torch.diag(MQ))\n",
    "            gradR += 4*torch.outer(one_N2, torch.diag(MR))\n",
    "    else:\n",
    "        gradQ = 0\n",
    "        gradR = 0\n",
    "    \n",
    "    # total gradients -- readjust cost for FGW problem by adding W gradients\n",
    "    gradQW, gradRW = gd__Wasserstein_Grad_LR(C_factors, Q, R, Lambda, device, \\\n",
    "                                                   dtype=dtype, full_grad=full_grad)\n",
    "    gradQ = (1-alpha)*gradQW + (alpha/2)*gradQ\n",
    "    gradR = (1-alpha)*gradRW + (alpha/2)*gradR\n",
    "    \n",
    "    normalizer = torch.max(torch.tensor([torch.max(torch.abs(gradQ)) , torch.max(torch.abs(gradR))]))\n",
    "    gamma_k = gamma / normalizer\n",
    "    \n",
    "    return gradQ, gradR, gamma_k\n",
    "\n",
    "def gd__compute_grad_B_LR(C_factors, A_factors, B_factors, Q, R, Lambda, gQ, gR, gamma, device, \\\n",
    "                   alpha=0.0, dtype=torch.float64):\n",
    "    \n",
    "    N1, N2 = C_factors[0].size(0), C_factors[1].size(1)\n",
    "\n",
    "    if A_factors is not None and B_factors is not None:\n",
    "        A1, A2 = A_factors[0], A_factors[1]\n",
    "        B1, B2 = B_factors[0], B_factors[1]\n",
    "        # GW grad\n",
    "        gradLambda = -4 * ( (Q.T @ A1) @ (A2 @ Q) ) @ Lambda @ ( (R.T @ B1) @ (B2 @ R) )\n",
    "        del A1,A2,B1,B2\n",
    "    else:\n",
    "        gradLambda = 0\n",
    "    \n",
    "    C1, C2 = C_factors[0], C_factors[1]\n",
    "    # total grad\n",
    "    gradLambda = (1-alpha)*( (Q.T @ C1) @ (C2 @ R) ) + (alpha/2)*gradLambda\n",
    "    gradT = torch.diag(1/gQ) @ gradLambda @ torch.diag(1/gR) # (mass-reweighted form)\n",
    "    gamma_T = gamma / torch.max(torch.abs(gradT))\n",
    "    return gradT, gamma_T\n",
    "\n",
    "def gd__Wasserstein_Grad_LR(C_factors, Q, R, Lambda, device, \\\n",
    "                   dtype=torch.float64, full_grad=True):\n",
    "\n",
    "    C1, C2 = C_factors[0], C_factors[1]\n",
    "    \n",
    "    gradQ = C1 @ ((C2 @ R) @ Lambda.T)\n",
    "    \n",
    "    if full_grad:\n",
    "        # rank-one perturbation\n",
    "        N1 = Q.shape[0]\n",
    "        one_N1 = torch.ones((N1), device=device, dtype=dtype)\n",
    "        gQ = Q.T @ one_N1\n",
    "        w1 = torch.diag( (gradQ.T @ Q) @ torch.diag(1/gQ) )\n",
    "        gradQ -= torch.outer(one_N1, w1)\n",
    "    \n",
    "    # linear term\n",
    "    gradR = C2.T @ ((C1.T @ Q) @ Lambda)\n",
    "    if full_grad:\n",
    "        # rank-one perturbation\n",
    "        N2 = R.shape[0]\n",
    "        one_N2 = torch.ones((N2), device=device, dtype=dtype)\n",
    "        gR = R.T @ one_N2\n",
    "        w2 = torch.diag( torch.diag(1/gR) @ (R.T @ gradR) )\n",
    "        gradR -= torch.outer(one_N2, w2)\n",
    "    \n",
    "    return gradQ, gradR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b262e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FRLC_opt(C, a=None, b=None, A=None, B=None, tau_in = 50, tau_out=50, \\\n",
    "             gamma=90, r = 10, r2=None, max_iter=200, device='cpu', dtype=torch.float64, \\\n",
    "                  semiRelaxedLeft=False, semiRelaxedRight=False, Wasserstein=True, \\\n",
    "                 printCost=True, returnFull=False, FGW=False, alpha=0.0, unbalanced=False, \\\n",
    "                  initialization='Full', init_args = None, full_grad=True, \\\n",
    "                   convergence_criterion=True, tol=1e-5, min_iter = 25, \\\n",
    "                   min_iterGW = 500, max_iterGW = 1000, \\\n",
    "                   max_inneriters_balanced= 300, max_inneriters_relaxed=50, \\\n",
    "                  diagonalize_return=False):\n",
    "    \n",
    "    '''\n",
    "    FRLC: Factor Relaxation with Latent Coupling\n",
    "    ------Parameters------\n",
    "    C: torch.tensor (N1 x N2)\n",
    "        A matrix of pairwise feature distances in space X and space Y (inter-space).\n",
    "    a: torch.tensor (N1)\n",
    "        A vector representing marginal one.\n",
    "    b: torch.tensor (N2)\n",
    "        A vector representing marginal two.\n",
    "    A: torch.tensor (N1 x N1)\n",
    "        A matrix of pairwise distances between points in metric space X.\n",
    "    B: torch.tensor (N2 x N2)\n",
    "        A matrix of pairwise distances between points in metric space Y.\n",
    "    tau_in: float (> 0)\n",
    "        A scalar which controls the regularity of the inner marginal update path.\n",
    "    tau_out: float (> 0)\n",
    "        A scalar which controls the regularization of the outer marginals a and b (e.g. for semi-relaxed or unbalanced OT)\n",
    "    gamma: float (> 0)\n",
    "        The mirror descent step-size, a scalar which controls the scaling of gradients\n",
    "        before being exponentiated into Sinkhorn kernels.\n",
    "    r: int (> 1)\n",
    "        A non-negative integer rank, controlling the rank of the FRLC learned OT coupling. \n",
    "    max_iter: int\n",
    "        The maximal number of iterations FRLC will run until convergence.\n",
    "    device: str\n",
    "        The device (i.e. 'cpu' or 'cuda') which FRLC runs on.\n",
    "    dtype: dtype\n",
    "        The datatype all tensors are stored on (naturally there is a space-accuracy\n",
    "        tradeoff for low-rank between 32 and 64 bit).\n",
    "    semiRelaxedLeft: bool\n",
    "        True if running the left-marginal relaxed low-rank algorithm.\n",
    "    semiRelaxedRight: bool\n",
    "        True if running the right-marginal relaxed low-rank algorithm.\n",
    "    Wasserstein: bool\n",
    "        True if using the Wasserstein loss <C, P>_F as the objective cost,\n",
    "        else runs GW if FGW false and FGW if GW true.\n",
    "    printCost: bool\n",
    "        True if printing the value of the objective cost at each iteration.\n",
    "        This can be expensive for large datasets if C is not factored.\n",
    "    returnFull: bool\n",
    "        True if returning P_r = Q Lambda R.T, else returns iterates (Q, R, T).\n",
    "    FGW: bool\n",
    "        True if running the Fused-Gromov Wasserstein problem, and otherwise false.\n",
    "    alpha: float\n",
    "        A balance parameter between the Wasserstein term and\n",
    "        the Gromov-Wasserstein term of the objective.\n",
    "    unbalanced: bool\n",
    "        True if running the unbalanced problem;\n",
    "        if semiRelaxedLeft/Right and unbalanced False (default) then running the balanced problem.\n",
    "    initialization: str, 'Full' or 'Rank-2'\n",
    "        'Full' if sub-couplings initialized to be full-rank, if 'Rank-2' set to a rank-2 initialization.\n",
    "        We advise setting this to be 'Full'.\n",
    "    init_args: tuple of 3-tensors\n",
    "        A tuple of (Q0, R0, T0) for tuple[i] of type tensor\n",
    "    full_grad: bool\n",
    "        If True, evaluates gradient with rank-1 perturbations.\n",
    "        Else if False, omits perturbation terms.\n",
    "    convergence_criterion: bool\n",
    "        If True, use the convergence criterion. Else if False, default to running up to max_iters.\n",
    "    tol: float\n",
    "        Tolerance used for established when convergence is reached.\n",
    "    min_iter: int\n",
    "        The minimum iterations for the algorithm to run for in the Wasserstein case.\n",
    "    min_iterGW: int\n",
    "        The minimum number of iterations to run for in the GW case.\n",
    "    max_iterGW: int\n",
    "        The maximum number of iterations to run for in the GW case.\n",
    "    max_inneriters_balanced: int\n",
    "        The maximum number of inner iterations for the Sinkhorn loop.\n",
    "    max_inneriters_relaxed: int\n",
    "        The maximum number of inner iterations for the relaxed and semi-relaxed loops.\n",
    "    diagonalize_return: bool\n",
    "        If True, diagonalize the LC-factorization to the form of Scetbon et al '21.\n",
    "        Else if False, return the LC-factorization.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    N1, N2 = C.size(dim=0), C.size(dim=1)\n",
    "    k = 0\n",
    "    stationarity_gap = torch.inf\n",
    "    \n",
    "    one_N1 = torch.ones((N1), device=device, dtype=dtype)\n",
    "    one_N2 = torch.ones((N2), device=device, dtype=dtype)\n",
    "    \n",
    "    if a is None:\n",
    "        a = one_N1 / N1\n",
    "    if b is None:\n",
    "        b = one_N2 / N2\n",
    "    if r2 is None:\n",
    "        r2 = r\n",
    "\n",
    "    one_r = torch.ones((r), device=device, dtype=dtype)\n",
    "    one_r2 = torch.ones((r2), device=device, dtype=dtype)\n",
    "    \n",
    "    # Initialize inner marginals to uniform; \n",
    "    # generalized to be of differing dimensions to account for non-square latent-coupling.\n",
    "    gQ = (1/r)*one_r\n",
    "    gR = (1/r2)*one_r2\n",
    "    \n",
    "    full_rank = True if initialization == 'Full' else False\n",
    "    \n",
    "    if initialization == 'Full':\n",
    "        full_rank = True\n",
    "    elif initialization == 'Rank-2':\n",
    "        full_rank = False\n",
    "    else:\n",
    "        full_rank = True\n",
    "        print('Initialization must be either \"Full\" or \"Rank-2\", defaulting to \"Full\".')\n",
    "        \n",
    "    if init_args is None:\n",
    "        Q, R, T, Lambda = utils__initialize_couplings(a, b, gQ, gR, \\\n",
    "                                                    gamma, full_rank=full_rank, \\\n",
    "                                                device=device, dtype=dtype, \\\n",
    "                                                    max_iter = max_inneriters_balanced)\n",
    "    else:\n",
    "        Q, R, T = init_args\n",
    "        Lambda = torch.diag(1/ (Q.T @ one_N1)) @ T @ torch.diag(1/ (R.T @ one_N2))\n",
    "\n",
    "    if Wasserstein is False:\n",
    "        min_iter = min_iterGW\n",
    "        max_iter = max_iterGW\n",
    "\n",
    "    '''\n",
    "    Preparing main loop.\n",
    "    '''\n",
    "    errs = []\n",
    "    grad = torch.inf\n",
    "    gamma_k = gamma\n",
    "    Q_prev, R_prev, T_prev = None, None, None\n",
    "    \n",
    "    while (k < max_iter and (not convergence_criterion or \\\n",
    "                       (k < min_iter or utils__Delta((Q, R, T), (Q_prev, R_prev, T_prev), gamma_k) > tol))):\n",
    "        \n",
    "        if convergence_criterion:\n",
    "            # Set previous iterates to evaluate convergence at the next round\n",
    "            Q_prev, R_prev, T_prev = Q, R, T\n",
    "        \n",
    "        if k % 25 == 0:\n",
    "            print(f'Iteration: {k}')\n",
    "        \n",
    "        gradQ, gradR, gamma_k = gd__compute_grad_A(C, Q, R, Lambda, gamma, semiRelaxedLeft, \\\n",
    "                                               semiRelaxedRight, device, Wasserstein=Wasserstein, \\\n",
    "                                               A=A, B=B, FGW=FGW, alpha=alpha, \\\n",
    "                                                  unbalanced=unbalanced, full_grad=full_grad, dtype=dtype)\n",
    "        if semiRelaxedLeft:\n",
    "            \n",
    "            R = utils__logSinkhorn(gradR - (gamma_k**-1)*torch.log(R), b, gR, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=False, tau=tau_in)\n",
    "            Q = utils__logSinkhorn(gradQ - (gamma_k**-1)*torch.log(Q), a, gQ, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=True, tau=tau_out, tau2=tau_in)\n",
    "            \n",
    "            gQ, gR = Q.T @ one_N1, R.T @ one_N2\n",
    "            gradT, gamma_T = gd__compute_grad_B(C, Q, R, Lambda, gQ, gR, \\\n",
    "                                               gamma, device, Wasserstein=Wasserstein, \\\n",
    "                                               A=A, B=B, FGW=FGW, alpha=alpha, dtype=dtype)\n",
    "            \n",
    "        elif semiRelaxedRight:\n",
    "            \n",
    "            Q = utils__logSinkhorn(gradQ - (gamma_k**-1)*torch.log(Q), a, gQ, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=False, tau=tau_in)\n",
    "            R = utils__logSinkhorn(gradR - (gamma_k**-1)*torch.log(R), b, gR, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=True, tau=tau_out, tau2=tau_in)\n",
    "            \n",
    "            gQ, gR = Q.T @ one_N1, R.T @ one_N2\n",
    "            gradT, gamma_T = gd__compute_grad_B(C, Q, R, Lambda, gQ, gR, \\\n",
    "                                               gamma, device, Wasserstein=Wasserstein, \\\n",
    "                                               A=A, B=B, FGW=FGW, alpha=alpha, dtype=dtype)\n",
    "            \n",
    "        elif unbalanced:\n",
    "            \n",
    "            Q = utils__logSinkhorn(gradQ - (gamma_k**-1)*torch.log(Q), a, gQ, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=True, tau=tau_out, tau2=tau_in)\n",
    "            R = utils__logSinkhorn(gradR - (gamma_k**-1)*torch.log(R), b, gR, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=True, tau=tau_out, tau2=tau_in)\n",
    "            \n",
    "            gQ, gR = Q.T @ one_N1, R.T @ one_N2\n",
    "            \n",
    "            gradT, gamma_T = gd__compute_grad_B(C, Q, R, Lambda, gQ, gR, gamma, \\\n",
    "                                               device, Wasserstein=Wasserstein, \\\n",
    "                                               A=A, B=B, FGW=FGW, alpha=alpha, dtype=dtype)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Balanced\n",
    "            Q = utils__logSinkhorn(gradQ - (gamma_k**-1)*torch.log(Q), a, gQ, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=False, tau=tau_in)\n",
    "            R = utils__logSinkhorn(gradR - (gamma_k**-1)*torch.log(R), b, gR, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=False, tau=tau_in)\n",
    "            \n",
    "            gQ, gR = Q.T @ one_N1, R.T @ one_N2\n",
    "            gradT, gamma_T = gd__compute_grad_B(C, Q, R, Lambda, gQ, gR, gamma, \\\n",
    "                                               device, Wasserstein=Wasserstein, \\\n",
    "                                               A=A, B=B, FGW=FGW, alpha=alpha, dtype=dtype)\n",
    "        \n",
    "        T = utils__logSinkhorn(gradT - (gamma_T**-1)*torch.log(T), gQ, gR, gamma_T, max_iter = max_inneriters_balanced, \\\n",
    "                         device=device, dtype=dtype, balanced=True, unbalanced=False)\n",
    "        \n",
    "        # Inner latent transition-inverse matrix\n",
    "        Lambda = torch.diag(1/gQ) @ T @ torch.diag(1/gR)\n",
    "        \n",
    "        if printCost:\n",
    "            if Wasserstein:\n",
    "                #P = Q @ Lambda @ R.T\n",
    "                cost = torch.trace(( (Q.T @ C) @ R) @ Lambda.T) #torch.sum(C * P)\n",
    "            else:\n",
    "                P = Q @ Lambda @ R.T\n",
    "                M1 = Q.T @ A**2 @ Q\n",
    "                M2 = R.T @ B**2 @ R\n",
    "                cost = one_r.T @ M1 @ one_r + one_r.T @ M2 @ one_r -2*torch.trace((A @ P @ B).T @ P)\n",
    "                #cost = one_N2.T @ P.T @ A**2 @ P @ one_N2 + one_N1.T @ P @ B**2 @ P.T @ one_N1 - 2*torch.trace((A @ P @ B).T @ P)\n",
    "                if FGW:\n",
    "                    cost = (1-alpha)*torch.sum(C * P) + alpha*cost\n",
    "            errs.append(cost.cpu())\n",
    "            \n",
    "        k+=1\n",
    "    if printCost:\n",
    "        ''' \n",
    "        Plotting OT objective value across iterations.\n",
    "        '''\n",
    "        plt.plot(range(len(errs)), errs)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('OT-Cost')\n",
    "        plt.show()\n",
    "        '''\n",
    "        Plotting latent coupling.\n",
    "        '''\n",
    "        plt.imshow(T.cpu())\n",
    "        plt.show()\n",
    "    \n",
    "    if returnFull:\n",
    "        P = Q @ Lambda @ R.T\n",
    "        return P, errs\n",
    "    else:\n",
    "        \n",
    "        if diagonalize_return:\n",
    "            '''\n",
    "            Diagonalize return to factorization of Scetbon '21\n",
    "            '''\n",
    "            Q = Q @ torch.diag(1 / gQ) @ T\n",
    "            gR = R.T @ one_N2\n",
    "            T = torch.diag(gR)\n",
    "            \n",
    "        return Q, R, T, errs\n",
    "\n",
    "'''\n",
    "----------\n",
    "Below:\n",
    "Optimization of FRLC supposing the distance matrices C, A, B have been factorized\n",
    "----------\n",
    "'''\n",
    "\n",
    "def FRLC_LR_opt(C_factors, A_factors, B_factors, a=None, b=None, tau_in = 50, tau_out=50, \\\n",
    "                  gamma=90, r = 10, r2=None, max_iter=200, device='cpu', dtype=torch.float64, \\\n",
    "                 printCost=True, returnFull=False, alpha=0.0, \\\n",
    "                  initialization='Full', init_args = None, full_grad=True, \\\n",
    "                   convergence_criterion=True, tol=5e-6, min_iter = 25, \\\n",
    "                   max_inneriters_balanced= 300, max_inneriters_relaxed=50, \\\n",
    "                  diagonalize_return=False):\n",
    "    '''\n",
    "    FRLC with a low-rank factorization of the distance matrices (C, A, B) assumed.\n",
    "    \n",
    "    *** Currently only implements balanced OT ***\n",
    "    \n",
    "    ------Parameters------\n",
    "    C_factors: tuple of torch.tensor (n x d, d x m)\n",
    "        A tuple of two tensors representing the factors of C (Wasserstein term).\n",
    "    A_factors: tuple of torch.tensor (n x d, d x n)\n",
    "        A tuple of the A factors (GW term).\n",
    "    B_factors: torch.tensor\n",
    "        A tuple of the B factors (GW term).\n",
    "    a: torch.tensor, optional (default=None)\n",
    "        A vector representing marginal one.\n",
    "    b: torch.tensor, optional (default=None)\n",
    "        A vector representing marginal two.\n",
    "    tau_in: float, optional (default=0.0001)\n",
    "        The inner marginal regularization parameter.\n",
    "    tau_out: float, optional (default=75)\n",
    "        The outer marginal regularization parameter.\n",
    "    gamma: float, optional (default=90)\n",
    "        Mirror descent step size.\n",
    "    r: int, optional (default=10)\n",
    "        A parameter representing a rank or dimension.\n",
    "    r2: int, optional (default=None)\n",
    "        A secondary rank parameter (if None, defaults to square latent coupling)\n",
    "    max_iter: int, optional (default=200)\n",
    "        The maximum number of iterations.\n",
    "    device: str, optional (default='cpu')\n",
    "        The device to run the computation on ('cpu' or 'cuda').\n",
    "    dtype: torch.dtype, optional (default=torch.float64)\n",
    "        The data type of the tensors.\n",
    "    printCost: bool, optional (default=True)\n",
    "        Whether to print and plot the cost during computation.\n",
    "    returnFull: bool, optional (default=False)\n",
    "        Whether to return the full coupling P. If False, returns (Q,R,T)\n",
    "    alpha: float, optional (default=0.2)\n",
    "        A parameter controlling weight to Wasserstein (alpha = 0.0) or GW (alpha = 1.0) terms\n",
    "    initialization: str, optional (default='Full')\n",
    "        'Full' if sub-couplings initialized to be full-rank, if 'Rank-2' set to a rank-2 initialization.\n",
    "        We advise setting this to be 'Full'.\n",
    "    init_args: dict, optional (default=None)\n",
    "        Arguments for the initialization method.\n",
    "    full_grad: bool, optional (default=True)\n",
    "        If True, evaluates gradient with rank-1 perturbations. Else if False, omits perturbation terms.\n",
    "    convergence_criterion: bool, optional (default=True)\n",
    "        If True, use the convergence criterion. Else if False, default to running up to max_iters.\n",
    "    tol: float, optional (default=5e-6)\n",
    "        The tolerance for convergence.\n",
    "    min_iter: int, optional (default=25)\n",
    "        The minimum number of iterations.\n",
    "    max_inneriters_balanced: int, optional (default=300)\n",
    "        The maximum number of inner iterations for balanced OT sub-routines.\n",
    "    max_inneriters_relaxed: int, optional (default=50)\n",
    "        The maximum number of inner iterations for relaxed OT sub-routines.\n",
    "    diagonalize_return: bool, optional (default=False)\n",
    "         If True, diagonalize the LC-factorization to the form of Scetbon et al '21.\n",
    "        Else if False, return the LC-factorization.\n",
    "    '''\n",
    "    \n",
    "    N1, N2 = C_factors[0].size(dim=0), C_factors[1].size(dim=1)\n",
    "    k = 0\n",
    "    stationarity_gap = torch.inf\n",
    "    \n",
    "    one_N1 = torch.ones((N1), device=device, dtype=dtype)\n",
    "    one_N2 = torch.ones((N2), device=device, dtype=dtype)\n",
    "    \n",
    "    if a is None:\n",
    "        a = one_N1 / N1\n",
    "    if b is None:\n",
    "        b = one_N2 / N2\n",
    "    if r2 is None:\n",
    "        r2 = r\n",
    "\n",
    "    one_r = torch.ones((r), device=device, dtype=dtype)\n",
    "    one_r2 = torch.ones((r2), device=device, dtype=dtype)\n",
    "    \n",
    "    # Initialize inner marginals to uniform; \n",
    "    # generalized to be of differing dimensions to account for non-square latent-coupling.\n",
    "    gQ = (1/r)*one_r\n",
    "    gR = (1/r2)*one_r2\n",
    "    \n",
    "    full_rank = True if initialization == 'Full' else False\n",
    "    \n",
    "    if initialization == 'Full':\n",
    "        full_rank = True\n",
    "    elif initialization == 'Rank-2':\n",
    "        full_rank = False\n",
    "    else:\n",
    "        full_rank = True\n",
    "        print('Initialization must be either \"Full\" or \"Rank-2\", defaulting to \"Full\".')\n",
    "        \n",
    "    if init_args is None:\n",
    "        Q, R, T, Lambda = utils__initialize_couplings(a, b, gQ, gR, \\\n",
    "                                                    gamma, full_rank=full_rank, \\\n",
    "                                                device=device, dtype=dtype, \\\n",
    "                                                    max_iter = max_inneriters_balanced)\n",
    "    else:\n",
    "        # Initialize to given factors\n",
    "        Q, R, T = init_args\n",
    "        if Q is not None:\n",
    "            gQ = Q.T @ one_N1\n",
    "        if R is not None:\n",
    "            gR =  R.T @ one_N2\n",
    "        if Q is None or R is None or T is None:\n",
    "            _Q, _R, _T, Lambda = utils__initialize_couplings(a, b, gQ, gR, \\\n",
    "                                                    gamma, full_rank=full_rank, \\\n",
    "                                                device=device, dtype=dtype, \\\n",
    "                                                    max_iter = max_inneriters_balanced)\n",
    "            if Q is None:\n",
    "                Q = _Q\n",
    "            if R is None:\n",
    "                R = _R\n",
    "            if T is None:\n",
    "                T = _T\n",
    "        \n",
    "        Lambda = torch.diag(1/ (Q.T @ one_N1)) @ T @ torch.diag(1/ (R.T @ one_N2))\n",
    "    \n",
    "    '''\n",
    "    Preparing main loop.\n",
    "    '''\n",
    "    errs = {'total_cost':[], 'W_cost':[], 'GW_cost': []}\n",
    "    grad = torch.inf\n",
    "    gamma_k = gamma\n",
    "    Q_prev, R_prev, T_prev = None, None, None\n",
    "    \n",
    "    while (k < max_iter and (not convergence_criterion or \\\n",
    "                       (k < min_iter or utils__Delta((Q, R, T), (Q_prev, R_prev, T_prev), gamma_k) > tol))):\n",
    "        \n",
    "        if convergence_criterion:\n",
    "            # Set previous iterates to evaluate convergence at the next round\n",
    "            Q_prev, R_prev, T_prev = Q, R, T\n",
    "        \n",
    "        if k % 25 == 0:\n",
    "            print(f'Iteration: {k}')\n",
    "\n",
    "        gradQ, gradR, gamma_k = gd__compute_grad_A_LR(C_factors, A_factors, B_factors, Q, R, Lambda, gamma, device, \\\n",
    "                                   alpha=alpha, dtype=dtype, full_grad=full_grad)\n",
    "        \n",
    "        ### Semi-relaxed updates ###\n",
    "        R = utils__logSinkhorn(gradR - (gamma_k**-1)*torch.log(R), b, gR, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=False, tau=tau_in)\n",
    "        \n",
    "        Q = utils__logSinkhorn(gradQ - (gamma_k**-1)*torch.log(Q), a, gQ, gamma_k, max_iter = max_inneriters_relaxed, \\\n",
    "                         device=device, dtype=dtype, balanced=False, unbalanced=False, tau=tau_in)\n",
    "        \n",
    "        gQ, gR = Q.T @ one_N1, R.T @ one_N2\n",
    "        \n",
    "        gradT, gamma_T = gd__compute_grad_B_LR(C_factors, A_factors, B_factors, Q, R, Lambda, gQ, gR, gamma, device, \\\n",
    "                                       alpha=alpha, dtype=dtype)\n",
    "        \n",
    "        \n",
    "        T = utils__logSinkhorn(gradT - (gamma_T**-1)*torch.log(T), gQ, gR, gamma_T, max_iter = max_inneriters_balanced, \\\n",
    "                         device=device, dtype=dtype, balanced=True, unbalanced=False)\n",
    "        \n",
    "        # Inner latent transition-inverse matrix\n",
    "        Lambda = torch.diag(1/gQ) @ T @ torch.diag(1/gR)\n",
    "        k+=1\n",
    "        \n",
    "        if printCost:\n",
    "            primal_cost = torch.trace(((Q.T @ C_factors[0]) @ (C_factors[1] @ R)) @ Lambda.T)\n",
    "            errs['W_cost'].append(primal_cost.cpu())\n",
    "            if A_factors is not None and B_factors is not None:\n",
    "                X = R @ ((Lambda.T @ ((Q.T @ A_factors[0]) @ (A_factors[1] @ Q)) @ Lambda) @ (R.T @ B_factors[0])) @ B_factors[1]\n",
    "                GW_cost = - 2 * torch.trace(X) # add these: one_r.T @ M1 @ one_r + one_r.T @ M2 @ one_r\n",
    "                del X\n",
    "                A1_tild, A2_tild = utils__hadamard_square_lr(A_factors[0], A_factors[1].T, device=device)\n",
    "                GW_cost += torch.inner(A1_tild.T @ (Q @ one_r), A2_tild.T @ (Q @ one_r))\n",
    "                del A1_tild, A2_tild\n",
    "                B1_tild, B2_tild = utils__hadamard_square_lr(B_factors[0], B_factors[1].T, device=device)\n",
    "                GW_cost += torch.inner(B1_tild.T @ (R @ one_r2), B2_tild.T @ (R @ one_r2))\n",
    "                del B1_tild, B2_tild\n",
    "                # Update costs\n",
    "                errs['GW_cost'].append((GW_cost).cpu())\n",
    "                errs['total_cost'].append(((1-alpha)*primal_cost + alpha*GW_cost).cpu())\n",
    "            else:\n",
    "                errs['GW_cost'].append(0)\n",
    "                errs['total_cost'].append(primal_cost.cpu())\n",
    "        \n",
    "    if printCost:\n",
    "        print(f\"Initial Wasserstein cost: {errs['W_cost'][0]}, GW-cost: {errs['GW_cost'][0]}, Total cost: {errs['total_cost'][0]}\")\n",
    "        print(f\"Final Wasserstein cost: {errs['W_cost'][-1]}, GW-cost: {errs['GW_cost'][-1]}, Total cost: {errs['total_cost'][-1]}\")\n",
    "        plt.plot(errs['total_cost'])\n",
    "        plt.show()\n",
    "    \n",
    "    if returnFull:\n",
    "        P = Q @ Lambda @ R.T\n",
    "        return P, errs\n",
    "    else:\n",
    "        if diagonalize_return:\n",
    "            '''\n",
    "            Diagonalize return to factorization of Scetbon '21\n",
    "            '''\n",
    "            Q = Q @ (torch.diag(1 / gQ) @ T)\n",
    "            gR = R.T @ one_N2\n",
    "            T = torch.diag(gR)\n",
    "        return Q, R, T, errs\n",
    "    \n",
    "def FRLC_compute_OT_cost(X, Y, C=None, Monge_clusters=None, sq_Euclidean=True):\n",
    "    \"\"\"\n",
    "    Compute the optimal transport cost in linear space and time (without coupling).\n",
    "    \"\"\"\n",
    "    cost = 0\n",
    "    for clus in Monge_clusters:\n",
    "        idx1, idx2 = clus\n",
    "        if C is not None:\n",
    "            # If C saved, index into general cost directly\n",
    "            cost += C[idx1, idx2]\n",
    "        else:\n",
    "            # In case point-cloud init used, must directly compute distances between point pairs in X, Y.\n",
    "            if sq_Euclidean:\n",
    "                # squared Euclidean case\n",
    "                cost += torch.norm(X[idx1,:] - Y[idx2,:])**2\n",
    "            else:\n",
    "                # normal Euclidean cost\n",
    "                cost += torch.norm(X[idx1,:] - Y[idx2,:])\n",
    "    # Appropriately normalize the cost\n",
    "    return cost / len(Monge_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c6457",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FRLC_opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHierarchicalRefinementOT\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    A class to perform Hierarchical OT refinement with optional (CPU) parallelization.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m        A list containing the Monge-map pairings\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     37\u001b[0m                 C: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     38\u001b[0m                  rank_schedule: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m                  num_processes: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     47\u001b[0m                 ):\n",
      "Cell \u001b[1;32mIn[1], line 39\u001b[0m, in \u001b[0;36mHierarchicalRefinementOT\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHierarchicalRefinementOT\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    A class to perform Hierarchical OT refinement with optional (CPU) parallelization.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m        A list containing the Monge-map pairings\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     37\u001b[0m                 C: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     38\u001b[0m                  rank_schedule: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m---> 39\u001b[0m                  solver: Callable \u001b[38;5;241m=\u001b[39m \u001b[43mFRLC_opt\u001b[49m,\n\u001b[0;32m     40\u001b[0m                  solver_params: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any] , \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m                  device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     42\u001b[0m                  base_rank: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     43\u001b[0m                  clustering_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     44\u001b[0m                  plot_clusterings: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m                  parallel: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m                  num_processes: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     47\u001b[0m                 ):\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank_schedule \u001b[38;5;241m=\u001b[39m rank_schedule\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FRLC_opt' is not defined"
     ]
    }
   ],
   "source": [
    "class HierarchicalRefinementOT:\n",
    "    \"\"\"\n",
    "    A class to perform Hierarchical OT refinement with optional (CPU) parallelization.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    C : torch.tensor\n",
    "        The cost matrix of shape (N, N), currently assumed square for hierarchical OT.\n",
    "        Can represent general user-defined costs.\n",
    "    rank_schedule : list\n",
    "        The list of ranks for each hierarchical level -- i.e. the rank-annealing schedule.\n",
    "    solver : callable\n",
    "        A low-rank OT solver that takes a cost submatrix and returns Q, R, diagG, errs.\n",
    "    solver_params: Dict[str, Any], optional\n",
    "        Additional parameters for the low-rank solver. If None, default values are used.\n",
    "    device : str\n",
    "        The device ('cpu' or 'cuda') to be used for computations.\n",
    "    base_rank : int\n",
    "        Base-case rank at which to stop subdividing clusters.\n",
    "    clustering_type : str\n",
    "        'soft' or 'hard'. Determines how cluster assignments are computed after each OT solve.\n",
    "    plot_clusterings : bool\n",
    "        Whether to plot the Q and R matrices at each step for debugging.\n",
    "    parallel : bool\n",
    "        Whether to execute each subproblem at a level in parallel.\n",
    "    num_processes : int or None\n",
    "        Number of worker processes to spawn (if `parallel=True`). Defaults to `None` which uses `mp.cpu_count()`.\n",
    "    X, Y : torch.tensor\n",
    "        The point-clouds for the first dataset (X) and the second dataset (Y)\n",
    "    N : int\n",
    "        The size of the dataset.\n",
    "    Monge_clusters : list (tuples of type torch.float)\n",
    "        A list containing the Monge-map pairings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                C: torch.Tensor,\n",
    "                 rank_schedule: List[int],\n",
    "                 solver: Callable = FRLC_opt,\n",
    "                 solver_params: Union[Dict[str, Any] , None] = None,\n",
    "                 device: str = 'cpu',\n",
    "                 base_rank: int = 1,\n",
    "                 clustering_type: str = 'soft',\n",
    "                 plot_clusterings: bool = False,\n",
    "                 parallel: bool = False,\n",
    "                 num_processes: Union[int, None] = None\n",
    "                ):\n",
    "    \n",
    "        self.C = C.to(device)\n",
    "        self.rank_schedule = rank_schedule\n",
    "        self.solver = solver\n",
    "        self.device = device\n",
    "        self.base_rank = base_rank\n",
    "        self.clustering_type = clustering_type\n",
    "        self.plot_clusterings =  plot_clusterings\n",
    "        self.parallel = parallel\n",
    "        self.num_processes = num_processes\n",
    "        \n",
    "        # Point clouds optional attributes\n",
    "        self.X, self.Y = None, None\n",
    "        self.N = C.shape[0]\n",
    "        self.Monge_clusters = None\n",
    "        # This is a dummy line -- this init doesn't compute C or its factorization\n",
    "        self.sq_Euclidean = False\n",
    "\n",
    "        # Setting parameters to use with the FRLC solver\n",
    "        default_solver_params = {\n",
    "            'gamma' : 30,\n",
    "            'max_iter' : 60,\n",
    "            'min_iter' : 25,\n",
    "            'max_inneriters_balanced' : 100,\n",
    "            'max_inneriters_relaxed' : 40,\n",
    "            'printCost' : False,\n",
    "            'tau_in' : 100000\n",
    "        }\n",
    "        if solver_params is not None:\n",
    "            default_solver_params.update(solver_params)\n",
    "        self.solver_params = default_solver_params\n",
    "        \n",
    "        assert C.shape[0] == C.shape[1], \"Currently assume square costs so that |X| = |Y| = N\"\n",
    "    \n",
    "    @classmethod\n",
    "    def init_from_point_clouds(cls,\n",
    "                            X: torch.Tensor,\n",
    "                            Y: torch.Tensor,\n",
    "                            rank_schedule: List[int],\n",
    "                            distance_rank_schedule: Union[List[int], None] = None,\n",
    "                            solver: Callable = FRLC_LR_opt,\n",
    "                            solver_params: Union[Dict[str, Any] , None] = None,\n",
    "                            device: str = 'cpu',\n",
    "                            base_rank: int = 1,\n",
    "                            clustering_type: str = 'soft',\n",
    "                            plot_clusterings: bool = False,\n",
    "                            parallel: bool = False,\n",
    "                            num_processes: Union[int, None] = None,\n",
    "                              sq_Euclidean = False):\n",
    "        r\"\"\"\n",
    "        Constructor for initializing from point clouds.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        X : torch.tensor\n",
    "            The point-cloud of shape N for measure \\mu\n",
    "        Y: torch.tensor\n",
    "            Point cloud of shape N for measure \\nu\n",
    "        distance_rank_schedule: List[int]\n",
    "            A separate rank-schedule for the low-rank distance matrix being factorized.\n",
    "        sq_Euclidean : bool\n",
    "            If True, assumes squared Euclidean cost. Otherwise, defaults to Euclidean.\n",
    "            Needed for the point-cloud variant, in order to define a distance metric\n",
    "            to use for the low-rank approximation of C.\n",
    "        \"\"\"\n",
    "        \n",
    "        obj = cls.__new__(cls)\n",
    "        \n",
    "        obj.X = X\n",
    "        obj.Y = Y\n",
    "        obj.rank_schedule = rank_schedule\n",
    "        \n",
    "        if distance_rank_schedule is None:\n",
    "            # Default: assume distance rank schedule is identical to rank schedule for coupling.\n",
    "            obj.distance_rank_schedule = rank_schedule\n",
    "        else:\n",
    "            obj.distance_rank_schedule = distance_rank_schedule\n",
    "        \n",
    "        obj.solver = solver\n",
    "        obj.device = device\n",
    "        obj.base_rank = base_rank\n",
    "        obj.clustering_type = clustering_type\n",
    "        obj.plot_clusterings =  plot_clusterings\n",
    "        obj.parallel = parallel\n",
    "        obj.num_processes = num_processes\n",
    "        obj.N = X.shape[0]\n",
    "        \n",
    "        # Cost-mat an optional attribute\n",
    "        obj.C = None\n",
    "        obj.Monge_clusters = None\n",
    "        obj.sq_Euclidean = sq_Euclidean\n",
    "\n",
    "        # Setting parameters to use with the FRLC solver\n",
    "        default_solver_params = {\n",
    "            'gamma' : 30,\n",
    "            'max_iter' : 60,\n",
    "            'min_iter' : 25,\n",
    "            'max_inneriters_balanced' : 100,\n",
    "            'max_inneriters_relaxed' : 40,\n",
    "            'printCost' : False,\n",
    "            'tau_in' : 100000\n",
    "        }\n",
    "        if solver_params is not None:\n",
    "            default_solver_params.update(solver_params)\n",
    "        obj.solver_params = default_solver_params\n",
    "        \n",
    "        assert X.shape[0] == Y.shape[0], \"Currently assume square costs so that |X| = |Y| = N\"\n",
    "        \n",
    "        return obj\n",
    "\n",
    "    def run(self, return_as_coupling: bool = False):\n",
    "        \"\"\"\n",
    "        Routine to run hierarchical refinement.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        return_as_coupling : bool\n",
    "            Whether to return a full coupling matrix (size NxN) \n",
    "            or a list of (idxX, idxY) co-clusters / assignments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of (idxX, idxY) pairs OR torch.tensor\n",
    "            If return_as_coupling=False: returns a list of tuples (idxX, idxY) for each co-cluster.\n",
    "            If return_as_coupling=True: returns a dense coupling matrix of shape (N, N).\n",
    "        \"\"\"\n",
    "        if self.parallel:\n",
    "            \"\"\" WARNING: not currently implemented in this class! See HR_OT_parallelized instead! \"\"\"\n",
    "            return self._hierarchical_refinement_parallelized(return_as_coupling = return_as_coupling)\n",
    "        else:\n",
    "            return self._hierarchical_refinement(return_as_coupling = return_as_coupling)\n",
    "\n",
    "    def _hierarchical_refinement(self, return_as_coupling: bool = False):\n",
    "        \"\"\"\n",
    "        Single-process (serial) Hierarchical Refinement\n",
    "        \"\"\"\n",
    "\n",
    "        # Define partitions\n",
    "        F_t = [(torch.arange( self.N , device=self.device), \n",
    "                torch.arange( self.N , device=self.device))]\n",
    "        \n",
    "        for i, rank_level in enumerate(self.rank_schedule):\n",
    "            # Iterate over ranks in the scheduler\n",
    "            F_tp1 = []\n",
    "\n",
    "            if i == len(self.rank_schedule)-1:\n",
    "                fin_iters = int(self.N / rank_level)\n",
    "                print(f'Last level, rank chunk-size {rank_level} with {fin_iters} iterations to completion.')\n",
    "                j = 0\n",
    "            \n",
    "            for (idxX, idxY) in F_t:\n",
    "\n",
    "                if i == len(self.rank_schedule)-1:\n",
    "                    print(f'{j}/{fin_iters} of final-level iterations to completion')\n",
    "                    j += 1\n",
    "                \n",
    "                if len(idxX) <=self.base_rank or len(idxY) <= self.base_rank:\n",
    "                    # Return tuple of base-rank sized index sets (e.g. (x,T(x)) for base_rank=1)\n",
    "                    F_tp1.append( ( idxX, idxY ) )\n",
    "                    continue\n",
    "                \n",
    "                if self.C is not None:\n",
    "                    Q,R = self._solve_prob( idxX, idxY, rank_level)\n",
    "                else:\n",
    "                    rank_D = self.distance_rank_schedule[i]\n",
    "                    Q,R = self._solve_LR_prob( idxX, idxY, rank_level, rank_D )\n",
    "                \n",
    "                if self.plot_clusterings:\n",
    "                    # If visualizing the Q - R clustering matrices.\n",
    "                    \n",
    "                    plt.figure(figsize=(12, 5))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.imshow(Q.detach().cpu().numpy(), aspect='auto', cmap='viridis')\n",
    "                    plt.title(f\"Q Clustering Level {i+1}\")\n",
    "                    plt.colorbar()\n",
    "    \n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.imshow(R.detach().cpu().numpy(), aspect='auto', cmap='viridis')\n",
    "                    plt.title(f\"R Clustering Level {i+1}\")\n",
    "                    plt.colorbar()\n",
    "                    plt.show()\n",
    "                \n",
    "                # Next level cluster capacity\n",
    "                capacity = int(self.N) / int(torch.prod(torch.Tensor(self.rank_schedule[0:i+1])))\n",
    "                capacity = int(capacity)\n",
    "                \n",
    "                idx_seenX, idx_seenY = torch.arange(Q.shape[0], device=self.device), \\\n",
    "                                                    torch.arange(R.shape[0], device=self.device)\n",
    "                \n",
    "                # Split by hard or soft-clustering\n",
    "                if self.clustering_type == 'soft':\n",
    "                    # If using a solver which returns \"soft\" clusterings, must strictly fill partitions to capacities.\n",
    "                    \n",
    "                    for z in range(rank_level):\n",
    "                        \n",
    "                        topk_values, topk_indices_X = torch.topk( Q[idx_seenX][:,z], k=capacity )\n",
    "                        idxX_z = idxX[idx_seenX[topk_indices_X]]\n",
    "                        topk_values, topk_indices_Y = torch.topk( R[idx_seenY][:,z], k=capacity )\n",
    "                        idxY_z = idxY[idx_seenY[topk_indices_Y]]\n",
    "                        \n",
    "                        F_tp1.append(( idxX_z, idxY_z ))\n",
    "                        \n",
    "                        idx_seenX = idx_seenX[~torch.isin(idx_seenX, idx_seenX[topk_indices_X])]\n",
    "                        idx_seenY = idx_seenY[~torch.isin(idx_seenY, idx_seenY[topk_indices_Y])]\n",
    "                \n",
    "                elif self.clustering_type == 'hard':\n",
    "                    # If using a solver which returns \"hard\" clusterings, can exactly take argmax.\n",
    "                    \n",
    "                    zX = torch.argmax(Q, axis=1) # X-assignments\n",
    "                    zY = torch.argmax(R, axis=1) # Y-assignments\n",
    "                    \n",
    "                    for z in range(rank_level):\n",
    "                        \n",
    "                        idxX_z = idxX[zX == z]\n",
    "                        idxY_z = idxY[zY == z]\n",
    "    \n",
    "                        assert len(idxX_z) == len(idxY_z) == capacity, \\\n",
    "                                            \"Assertion failed! Not a hard-clustering function, or point sets of unequal size!\"\n",
    "                        \n",
    "                        F_tp1.append((idxX_z, idxY_z))\n",
    "                        \n",
    "            F_t = F_tp1\n",
    "        \n",
    "        self.Monge_clusters = F_t\n",
    "\n",
    "        \n",
    "        if return_as_coupling is False:\n",
    "            return self.Monge_clusters\n",
    "        else:\n",
    "            return self._compute_coupling_from_Ft()\n",
    "\n",
    "    def _solve_LR_prob(self, idxX, idxY, rank_level, rankD, eps=0.04):\n",
    "        \n",
    "        \"\"\"\n",
    "        Solve problem for low-rank coupling under a low-rank factorization of distance matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        _x0, _x1 = torch.index_select(self.X, 0, idxX), torch.index_select(self.Y, 0, idxY)\n",
    "        \n",
    "        if rankD < _x0.shape[0]:\n",
    "            \n",
    "            C_factors, A_factors, B_factors = self.get_dist_mats(_x0, _x1, \n",
    "                                                                 rankD, eps, \n",
    "                                                                 self.sq_Euclidean )\n",
    "            \n",
    "            # Solve a low-rank OT sub-problem with black-box solver\n",
    "            Q, R, diagG, errs = self.solver(C_factors, A_factors, B_factors,\n",
    "                                       gamma = self.solver_params['gamma'],\n",
    "                                       r = rank_level,\n",
    "                                       max_iter = self.solver_params['max_iter'],\n",
    "                                       device=self.device,\n",
    "                                       min_iter = self.solver_params['min_iter'],\n",
    "                                       max_inneriters_balanced = self.solver_params['max_inneriters_balanced'],\n",
    "                                       max_inneriters_relaxed = self.solver_params['max_inneriters_relaxed'],\n",
    "                                       diagonalize_return = True,\n",
    "                                       printCost = False, tau_in = self.solver_params['tau_in'],\n",
    "                                        dtype = _x0.dtype)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Final base instance -- cost within-cluster costs explicitly\n",
    "            if self.sq_Euclidean:\n",
    "                C_XY = torch.cdist(_x0, _x1)**2\n",
    "                \n",
    "            else:\n",
    "                # normal Euclidean distance otherwise\n",
    "                C_XY = torch.cdist(_x0, _x1)\n",
    "            \n",
    "            Q, R, diagG, errs = FRLC_opt(C_XY,\n",
    "                                   gamma = self.solver_params['gamma'],\n",
    "                                   r = rank_level,\n",
    "                                   max_iter = self.solver_params['max_iter'],\n",
    "                                   device = self.device,\n",
    "                                   min_iter = self.solver_params['min_iter'],\n",
    "                                   max_inneriters_balanced = self.solver_params['max_inneriters_balanced'],\n",
    "                                   max_inneriters_relaxed = self.solver_params['max_inneriters_relaxed'],\n",
    "                                   diagonalize_return=True,\n",
    "                                   printCost=False, tau_in = self.solver_params['tau_in'],\n",
    "                                       dtype = C_XY.dtype)\n",
    "            \n",
    "        return Q, R\n",
    "        \n",
    "    def _solve_prob(self, idxX, idxY, rank_level):\n",
    "        \"\"\"\n",
    "        Solve problem for low-rank coupling assuming cost sub-matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Index into sub-cost\n",
    "        submat = torch.index_select(self.C, 0, idxX)\n",
    "        C_XY = torch.index_select(submat, 1, idxY)\n",
    "        \n",
    "        # Solve a low-rank OT sub-problem with black-box solver\n",
    "        Q, R, diagG, errs = self.solver(C_XY,\n",
    "                                   gamma = self.solver_params['gamma'],\n",
    "                                   r = rank_level,\n",
    "                                   max_iter = self.solver_params['max_iter'],\n",
    "                                   device=self.device,\n",
    "                                   min_iter = self.solver_params['min_iter'],\n",
    "                                   max_inneriters_balanced = self.solver_params['max_inneriters_balanced'],\n",
    "                                   max_inneriters_relaxed = self.solver_params['max_inneriters_relaxed'],\n",
    "                                   diagonalize_return=True,\n",
    "                                   printCost=False, tau_in = self.solver_params['tau_in'],\n",
    "                                       dtype = C_XY.dtype)\n",
    "        return Q, R\n",
    "    \n",
    "    def _compute_coupling_from_Ft(self):\n",
    "        \"\"\"\n",
    "        Returns coupling as a full-rank matrix rather than as a set of (x, T(x)) pairs.\n",
    "        \"\"\"\n",
    "        size = (self.N, self.N)\n",
    "        P = torch.zeros(size)\n",
    "        # Fill sparse coupling with entries\n",
    "        for pair in self.Monge_clusters:\n",
    "            idx1, idx2 = pair\n",
    "            P[idx1, idx2] = 1\n",
    "        # Return, trivially normalized to satisfy standard OT constraints\n",
    "        return P / self.N\n",
    "    \n",
    "    def compute_OT_cost(self):\n",
    "        \"\"\"\n",
    "        Compute the optimal transport in linear space and time (w/o coupling).\n",
    "        \"\"\"\n",
    "        \n",
    "        cost = 0\n",
    "        for clus in self.Monge_clusters:\n",
    "            idx1, idx2 = clus\n",
    "            if self.C is not None:\n",
    "                # If C saved, index into general cost directly\n",
    "                cost += self.C[idx1, idx2]\n",
    "            else:\n",
    "                # In case point-cloud init used, must directly compute distances between point pairs in X, Y.\n",
    "                if self.sq_Euclidean:\n",
    "                    # squared Euclidean case\n",
    "                    cost += torch.norm(self.X[idx1,:] - self.Y[idx2,:])**2\n",
    "                else:\n",
    "                    # normal Euclidean cost\n",
    "                    cost += torch.norm(self.X[idx1,:] - self.Y[idx2,:])\n",
    "        # Appropriately normalize the cost\n",
    "        cost = cost / self.N\n",
    "        return cost\n",
    "\n",
    "    \n",
    "    def get_dist_mats(self, _x0, _x1, rankD, eps , sq_Euclidean ):\n",
    "        \n",
    "        # Wasserstein-only, setting A and B factors to be NoneType\n",
    "        A_factors = None\n",
    "        B_factors = None\n",
    "        \n",
    "        if sq_Euclidean:\n",
    "            # Sq Euclidean\n",
    "            C_factors = compute_lr_sqeuclidean_matrix(_x0, _x1, True)\n",
    "        else:\n",
    "            # Standard Euclidean dist\n",
    "            C_factors = self.ret_normalized_cost(_x0, _x1, rankD, eps)\n",
    "        \n",
    "        return C_factors, A_factors, B_factors\n",
    "    \n",
    "    def ret_normalized_cost(self, X, Y, rankD, eps):\n",
    "        \n",
    "        C1, C2 = utils__low_rank_distance_factorization(X,\n",
    "                                                      Y,\n",
    "                                                      r=rankD,\n",
    "                                                      eps=eps,\n",
    "                                                      device=self.device)\n",
    "        # Normalize appropriately\n",
    "        c = ( C1.max()**1/2 ) * ( C2.max()**1/2 )\n",
    "        C1, C2 = C1/c, C2/c\n",
    "        C_factors = (C1.to(X.dtype), C2.to(X.dtype))\n",
    "        \n",
    "        return C_factors\n",
    "\n",
    "\n",
    "\n",
    "def compute_lr_sqeuclidean_matrix(X_s,\n",
    "                                  X_t,\n",
    "                                  rescale_cost,\n",
    "                                  device=None,\n",
    "                                  dtype=None):\n",
    "    \"\"\"\n",
    "    Adapted from \"Section 3.5, proposition 1\" in Scetbon, M., Cuturi, M., & Peyré, G. (2021).\n",
    "    \n",
    "    A function for computing a low-rank factorization of a squared Euclidean distance matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    dtype, device = X_s.dtype, X_s.device\n",
    "    \n",
    "    ns, dim = X_s.shape\n",
    "    nt, _ = X_t.shape\n",
    "    \n",
    "    # First low rank decomposition of the cost matrix (M1)\n",
    "    # Compute sum of squares for each source sample\n",
    "    sum_Xs_sq = torch.sum(X_s ** 2, dim=1).reshape(ns, 1)  # Shape: (ns, 1)\n",
    "    ones_ns = torch.ones((ns, 1), device=device, dtype=dtype)  # Shape: (ns, 1)\n",
    "    neg_two_Xs = -2 * X_s  # Shape: (ns, dim)\n",
    "    M1 = torch.cat((sum_Xs_sq, ones_ns, neg_two_Xs), dim=1)  # Shape: (ns, dim + 2)\n",
    "    \n",
    "    # Second low rank decomposition of the cost matrix (M2)\n",
    "    ones_nt = torch.ones((nt, 1), device=device, dtype=dtype)  # Shape: (nt, 1)\n",
    "    sum_Xt_sq = torch.sum(X_t ** 2, dim=1).reshape(nt, 1)  # Shape: (nt, 1)\n",
    "    Xt = X_t  # Shape: (nt, dim)\n",
    "    M2 = torch.cat((ones_nt, sum_Xt_sq, Xt), dim=1)  # Shape: (nt, dim + 2)\n",
    "    \n",
    "    if rescale_cost:\n",
    "        # Compute the maximum value in M1 and M2 for rescaling\n",
    "        max_M1 = torch.max(M1)\n",
    "        max_M2 = torch.max(M2)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if max_M1 > 0:\n",
    "            M1 = M1 / torch.sqrt(max_M1)\n",
    "        if max_M2 > 0:\n",
    "            M2 = M2 / torch.sqrt(max_M2)\n",
    "    \n",
    "    return (M1, M2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76817def",
   "metadata": {},
   "source": [
    "# Fichier de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4183767f-2bc6-46db-afad-28df24da209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device: cpu\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def sinkhorn_loss(\n",
    "    x: jnp.ndarray, y: jnp.ndarray, epsilon: float = 0.001\n",
    ") -> float:\n",
    "    \"\"\"Computes transport between (x, a) and (y, b) via Sinkhorn algorithm.\"\"\"\n",
    "    a = jnp.ones(len(x)) / len(x)\n",
    "    b = jnp.ones(len(y)) / len(y)\n",
    "    \n",
    "    _, out = sinkhorn_divergence.sinkhorn_divergence(\n",
    "        pointcloud.PointCloud, x, y, epsilon=epsilon, a=a, b=b\n",
    "    )\n",
    "    \n",
    "    return out.divergence\n",
    "\n",
    "\n",
    "def run_progot(\n",
    "    x: jnp.ndarray, y: jnp.ndarray, cost_fn: costs.TICost, **kwargs: Any\n",
    ") -> progot.ProgOTOutput:\n",
    "    geom = pointcloud.PointCloud(x, y, cost_fn=cost_fn)\n",
    "    prob = linear_problem.LinearProblem(geom)\n",
    "    estim = progot.ProgOT(**kwargs)\n",
    "    out = estim(prob)\n",
    "    return out\n",
    "\n",
    "K = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'On device: {device}')\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0c6726-c9f4-468c-a814-7d2f3d9c4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 images from ImageNet!\n",
      "extracting embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 157/157 [04:26<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Liste tous les fichiers .jpg\n",
    "        self.image_paths = [os.path.join(root_dir, fname)\n",
    "                            for fname in os.listdir(root_dir)\n",
    "                            if fname.endswith('.jpg')]\n",
    "        # Si le nombre d'images est impair, enlever la dernière\n",
    "        if len(self.image_paths) % 2 != 0:\n",
    "            self.image_paths = self.image_paths[:-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Toujours convertir en RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # Pas d'étiquette ici, juste l'image\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images for CNN input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load ImageNet dataset from extracted path # /ILSVRC/Data/CLS-LOC/test\n",
    "imagenet_dataset = CustomImageDataset(root_dir='images', transform=transform)\n",
    "\n",
    "\n",
    "# Create DataLoader for batching\n",
    "imagenet_loader = DataLoader(imagenet_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Loaded {len(imagenet_dataset)} images from ImageNet!\")\n",
    "\n",
    "\n",
    "model_path = os.path.expanduser(\"resnet50-0676ba61.pth\")\n",
    "\n",
    "# Load pretrained ResNet model\n",
    "model = models.resnet50()\n",
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "model.fc = torch.nn.Identity()  # Remove classification layer to extract features\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Compute embeddings\n",
    "def extract_features(dataloader, model):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for idx, images in tqdm(enumerate(dataloader), desc=\"Extracting features\", total=len(dataloader)):\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            embeddings.append(features.cpu().numpy())\n",
    "    return np.vstack(embeddings)  # Stack all embeddings\n",
    "\n",
    "print('extracting embeddings!')\n",
    "embeddings = extract_features(imagenet_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eca2fc9-954e-4a85-8d97-b5b07391f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully to embeddings\\embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = \"embeddings\"\n",
    "# Create directory if it doesn't exist\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"embeddings.pkl\")\n",
    "\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(f\"Embeddings saved successfully to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc52a1a1-fd78-4c8a-a084-5b97d2fe468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully! Shape: (5000, 2048)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "emb_dir = 'embeddings/embeddings.pkl'\n",
    "\n",
    "# Load embeddings from the pickle file\n",
    "with open(emb_dir, \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(f\"Embeddings loaded successfully! Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96da85a1-2329-417e-a7ca-1cbce5a4cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num embeddings: 5000\n",
      "2500\n",
      "Optimized rank-annealing schedule: [2, 1250]\n",
      "[2, 1250]\n"
     ]
    }
   ],
   "source": [
    "# Get to a close even number which when divided is non-prime\n",
    "print(f'num embeddings: {embeddings.shape[0]}')\n",
    "N = embeddings.shape[0] // 2\n",
    "q = 640500\n",
    "k = (N-q)*2\n",
    "\n",
    "embed_sliced = embeddings[:-k]\n",
    "n = embed_sliced.shape[0] // 2\n",
    "print(n)\n",
    "\n",
    "rank_schedule = rank_annealing__optimal_rank_schedule( n , hierarchy_depth = 6, max_Q = int(2**11), max_rank = 64 )\n",
    "print(rank_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b5469a-3ca0-4cf8-86e2-977d2b6d2c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2500, 2048), Y shape: (2500, 2048)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_samples = embed_sliced.shape[0]\n",
    "\n",
    "# Shuffle indices\n",
    "indices = torch.randperm(num_samples)\n",
    "\n",
    "# Split into two tensors\n",
    "X = embeddings[indices[:n]]  # First 50%\n",
    "Y = embeddings[indices[n:]]  # Second 50%\n",
    "\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a938e94-841e-4f48-b341-3414e34335af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all-zero rows: 0\n"
     ]
    }
   ],
   "source": [
    "num_zero_rows = torch.sum(torch.sum(torch.tensor(embeddings), dim=1) == 0).item()\n",
    "print(f\"Number of all-zero rows: {num_zero_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e3693-3824-4b5c-990c-d5f1ddb8512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized rank-annealing schedule: [2, 1250]\n",
      "[2, 1250]\n"
     ]
    }
   ],
   "source": [
    "# Squared Euclidean cost p=2 or Euclidean if p=1\n",
    "p = 1\n",
    "K = 2\n",
    "\n",
    "#sample_sizes = [int(2**i) for i in range(5, 18, 1)]\n",
    "\n",
    "# Initialize dictionaries to store costs and sample sizes\n",
    "costs = {\n",
    "    'HROT_LR': {'samples': [], 'costs': []},\n",
    "    'Sinkhorn': {'samples': [], 'costs': []},\n",
    "    'ProgOT': {'samples': [], 'costs': []}\n",
    "}\n",
    "\n",
    "# Define pairwise Dist Mat\n",
    "_X = torch.tensor(np.array(X)).float().to(device)\n",
    "_Y = torch.tensor(np.array(Y)).float().to(device)\n",
    "\n",
    "# hierarchy_depth=6, max_Q=int(2**10), max_rank=16 up to 131072 points\n",
    "# hierarchy_depth=6, max_Q=int(2**10), max_rank=64 for 262144 + 524288 points\n",
    "# hierarchy_depth=6, max_Q=int(2**10), max_rank=64 for 262144 + 524288 points\n",
    "rank_schedule = rank_annealing__optimal_rank_schedule( X.shape[0] , hierarchy_depth = 6, max_Q = int(2**11), max_rank = 64 )\n",
    "print(rank_schedule)\n",
    "\n",
    "try:\n",
    "    hrot_lr = HierarchicalRefinementOT.init_from_point_clouds(_X, _Y, rank_schedule, base_rank=1, device=device)\n",
    "    del _X, _Y\n",
    "    F = hrot_lr.run(return_as_coupling=False)\n",
    "    cost_hrot_lr = hrot_lr.compute_OT_cost()\n",
    "    print(f'HR-OT cost: {cost_hrot_lr}')\n",
    "except Exception as e:\n",
    "    print(f'HROT-LR failed for sample size {n}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57d45171-518d-4627-b4ab-8cea117669cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\courn\\AppData\\Local\\Temp\\ipykernel_29472\\2244761235.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  mat[k, 0] = idx1.cpu().numpy()\n",
      "C:\\Users\\courn\\AppData\\Local\\Temp\\ipykernel_29472\\2244761235.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  mat[k, 1] = idx2.cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "n = len(F)\n",
    "mat = np.zeros((n, 2))\n",
    "\n",
    "for k, (idx1, idx2) in enumerate(F):\n",
    "    mat[k, 0] = idx1.cpu().numpy()\n",
    "    mat[k, 1] = idx2.cpu().numpy()\n",
    "\n",
    "np.save('embedding_alignment.npy', mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cdd2de5-5313-46c4-b4a3-951c6bcda4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 2)\n"
     ]
    }
   ],
   "source": [
    "mat = np.load('embedding_alignment.npy')\n",
    "print(mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3b186fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix shape: torch.Size([2500, 2500])\n",
      "Using rank 40 for FRLC\n",
      "Iteration: 0\n",
      "Iteration: 25\n",
      "Iteration: 50\n",
      "Iteration: 75\n",
      "FRLC cost: 23.262449264526367\n"
     ]
    }
   ],
   "source": [
    "# Squared Euclidean cost p=2 or Euclidean if p=1\n",
    "p = 1\n",
    "K = 2\n",
    "\n",
    "# Define pairwise Dist Mat\n",
    "_X = torch.tensor(np.array(X)).float().to(device)\n",
    "_Y = torch.tensor(np.array(Y)).float().to(device)\n",
    "\n",
    "# Compute distance matrix\n",
    "C = torch.cdist(_X, _Y)\n",
    "print(f\"Distance matrix shape: {C.shape}\")\n",
    "\n",
    "# Determine rank for FRLC\n",
    "r = 40\n",
    "print(f\"Using rank {r} for FRLC\")\n",
    "\n",
    "# Define the compute_OT_cost function\n",
    "try:\n",
    "    # Apply FRLC_opt for optimal transport\n",
    "    Q, R, T, errs = FRLC_opt(\n",
    "        C=C,\n",
    "        gamma=30,\n",
    "        r=r,\n",
    "        max_iter=100,\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "        printCost=False,\n",
    "        tau_in=100000\n",
    "    )\n",
    "    \n",
    "    # Compute the full coupling matrix (this can be memory intensive)\n",
    "    P = Q @ torch.diag(1/torch.sum(Q, dim=0)) @ T @ torch.diag(1/torch.sum(R, dim=0)) @ R.T\n",
    "    \n",
    "    # Now, let's define Monge clusters as pairs coming from the optimal transport solution\n",
    "    Monge_clusters = []\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(Y.shape[0]):\n",
    "            if P[i, j] > 0:  # A threshold for positive coupling\n",
    "                Monge_clusters.append((i, j))\n",
    "    \n",
    "    # Compute the OT cost using the new compute_OT_cost function\n",
    "    cost_frlc = FRLC_compute_OT_cost(_X, _Y, C=C, Monge_clusters=Monge_clusters, sq_Euclidean=(p == 2))\n",
    "    \n",
    "    print(f'FRLC cost: {cost_frlc}')\n",
    "    \n",
    "    # Extract matching pairs (as done in HierarchicalRefinement)\n",
    "    indices_X = torch.arange(X.shape[0])\n",
    "    P_approx = Q @ T @ R.T  # This is an approximation, might not fit in memory for large datasets\n",
    "    matches_Y = torch.argmax(P_approx, dim=1)\n",
    "    \n",
    "    # Create matching pairs similar to F in HierarchicalRefinement\n",
    "    F = []\n",
    "    for i in range(X.shape[0]):\n",
    "        F.append((torch.tensor([i]), torch.tensor([matches_Y[i].item()])))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'FRLC failed for sample size {X.shape[0]}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "426f953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\courn\\AppData\\Local\\Temp\\ipykernel_29472\\1143090218.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  mat[k, 0] = idx1.cpu().numpy()\n",
      "C:\\Users\\courn\\AppData\\Local\\Temp\\ipykernel_29472\\1143090218.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  mat[k, 1] = idx2.cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "n = len(F)\n",
    "mat = np.zeros((n, 2))\n",
    "\n",
    "for k, (idx1, idx2) in enumerate(F):\n",
    "    mat[k, 0] = idx1.cpu().numpy()\n",
    "    mat[k, 1] = idx2.cpu().numpy()\n",
    "\n",
    "np.save('embedding_alignment_FRLC.npy', mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "483129b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 2)\n"
     ]
    }
   ],
   "source": [
    "mat = np.load('embedding_alignment_FRLC.npy')\n",
    "print(mat.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
